[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Getting started with Python for MIKE+",
    "section": "",
    "text": "Introduction\nDHI offers a range of free, open-source Python libraries that enable automated and reproducible MIKE+ workflows, as well as unlock the potential for robust and flexible analyses. This course is designed for experienced MIKE+ modelers who are new to Python, providing a practical foundation to begin applying concepts to real projects. You’ll gain essential skills to read, run, and modify Python scripts relevant to MIKE+ modelling through focused, hand-tailored examples. The course will orient you to a new way of working, guiding you through the transition from a GUI to a script-based environment, helping you navigate common challenges, and giving you the confidence to continue exploring Python and seek out resources to further develop your skills.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#why-python-with-mike",
    "href": "index.html#why-python-with-mike",
    "title": "Getting started with Python for MIKE+",
    "section": "Why Python with MIKE+?",
    "text": "Why Python with MIKE+?\nUsing Python alongside MIKE+ provides the following advantages:\n\nEfficient handling of various file types, including dfs0, res1d, and xns11\nConversion of data between MIKE+ and third-party formats such as CSV and Excel\nFlexibility to modify MIKE+ databases, access tools, and run simulations\nAutomation of modelling tasks using a straightforward scripting syntax\nReproducible and documented workflows that enhance model quality assurance",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#intended-audience",
    "href": "index.html#intended-audience",
    "title": "Getting started with Python for MIKE+",
    "section": "Intended Audience",
    "text": "Intended Audience\nThis course is ideal for MIKE+ modelers who:\n\nAre eager to explore Python’s potential in MIKE+ modelling\nWant to enhance, automate, or document parts of their workflows with Python\nSeek more flexible and robust techniques for advanced modelling needs",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#course-structure",
    "href": "index.html#course-structure",
    "title": "Getting started with Python for MIKE+",
    "section": "Course Structure",
    "text": "Course Structure\nThe course focuses on practical applications of Python for common MIKE+ modelling tasks. Content generally consists of a combination of videos, live sessions, and hands-on exercises. We will cover Python libraries such as MIKE IO, MIKE IO 1D, and MIKE+Py.\n\nModule 1 | Foundations\n\nTopics: Python and Python Packages, Visual Studio Code, GitHub, Jupyter Notebooks, LLMs for coding, Pandas, Matplotlib, Documentation\n\nModule 2 | Time Series\n\nTopics: dfs0 files, plotting, statistics, selections, resampling, basic data validation\n\nModule 3 | Network Results\n\nTopics: network result files (e.g. res1d, res, res11), selecting data, extracting results, geospatial formats (e.g. shapefiles)\n\nModule 4 | Calibration Plots and Statistics\n\nTopics: basic statistics and plots relevant for model calibration\n\nModule 5 | MIKE+Py\n\nTopics: databases and SQL, modifying MIKE+ databases, accessing GUI tools, running simulations\n\nModule 6 | Putting Everything Together\n\nTopics: final project applying lessons of previous modules.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#course-objectives",
    "href": "index.html#course-objectives",
    "title": "Getting started with Python for MIKE+",
    "section": "Course Objectives",
    "text": "Course Objectives\nAfter completing this course, you should be able to:\n\nInstall Python and related packages for use with MIKE+\nApply Python to create reproducible and automated workflows\nExplore documentation and run example notebooks and scripts\nConnect with the open-source Python community and MIKE+ modelers",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "module1_foundations/index.html",
    "href": "module1_foundations/index.html",
    "title": "Welcome to Module One!",
    "section": "",
    "text": "In this module, you’ll gain the skills to confidently set up your coding environment, access course materials, and run Python code with ease. Our focus is on:\n\nMastering essential tools (e.g. GitHub, Visual Studio Code, uv).\nRunning and understanding Python scripts and Jupyter notebooks.\nLearning basic Python syntax and concepts.\nExploring core libraries (e.g. NumPy, Pandas, Matplotlib) for data analysis.\n\nDon’t worry if it feels fast-paced — you’ll practice these concepts in later modules. Let’s get you set up and ready to dive in!",
    "crumbs": [
      "Module 1 - Foundations",
      "Welcome to Module One!"
    ]
  },
  {
    "objectID": "module1_foundations/github.html",
    "href": "module1_foundations/github.html",
    "title": "1  GitHub",
    "section": "",
    "text": "1.1 DHI’s Python Ecosystem on GitHub\nGitHub is a website for storing, sharing, and collaborating on software development projects. It’s an especially popular platform for open-source software. DHI uses GitHub for hosting its entire open-source Python ecosystem, including documentation and examples.\nDHI’s Python ecosystem is organized into modular Python packages based on functionality. This is a common pattern in Python that empowers users to flexibly combine functionalities to meet specific project needs. An overview of Python packages useful for MIKE+ modelling is provided in the table below.\nFeel free to browse additional open-source packages on DHI’s GitHub profile.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>GitHub</span>"
    ]
  },
  {
    "objectID": "module1_foundations/github.html#dhis-python-ecosystem-on-github",
    "href": "module1_foundations/github.html#dhis-python-ecosystem-on-github",
    "title": "1  GitHub",
    "section": "",
    "text": "Package\nDescription\n\n\n\n\n\nRead, write and manipulate dfs0, dfs1, dfs2, dfs3, dfsu and mesh files.\n\n\n\nRead, manipulate, and analyze res1d, res, resx, out, and xns11 files.\n\n\n\nMIKE+Py is a python interface for MIKE+.\n\n\n\nCompare MIKE model results and observations.\n\n\n\nAnomaly Detection for time series data.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>GitHub</span>"
    ]
  },
  {
    "objectID": "module1_foundations/github.html#why-visit-a-python-packages-github-page",
    "href": "module1_foundations/github.html#why-visit-a-python-packages-github-page",
    "title": "1  GitHub",
    "section": "1.2 Why visit a Python package’s GitHub page?",
    "text": "1.2 Why visit a Python package’s GitHub page?\nYou’ll use GitHub for:\n\nAccessing documentation and examples.\nCreating ‘issues’ and/or ‘discussions’ when you need help.\nChecking out changes with new package versions.\nBrowsing source code and/or contributing code you think is generally useful.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>GitHub</span>"
    ]
  },
  {
    "objectID": "module1_foundations/github.html#typical-structure-of-a-python-package-on-github",
    "href": "module1_foundations/github.html#typical-structure-of-a-python-package-on-github",
    "title": "1  GitHub",
    "section": "1.3 Typical structure of a Python package on GitHub",
    "text": "1.3 Typical structure of a Python package on GitHub\nPlease watch the video below for a guided tour of how DHI organizes their Python packages on GitHub.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>GitHub</span>"
    ]
  },
  {
    "objectID": "module1_foundations/python_management.html",
    "href": "module1_foundations/python_management.html",
    "title": "2  Python Management",
    "section": "",
    "text": "2.1 Tools\nPython continuously releases new versions. Similarly, individual Python packages (hosted on PyPI) also continuously release new versions. Python scripts usually have dependencies on specific Python versions and packages, which highlights the need to carefully managing these. This is similar to different versions of MIKE+: you would not expect a MIKE+ 2025 model to run with MIKE+ 2023.\nThere are several tools for managing Python and packages together. Two common options are:\nThis course uses uv. Please install uv according to their official installation instructions. Use the “standalone installer” for Windows.\nConfirm you properly installed uv by opening a terminal and running:",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Python Management</span>"
    ]
  },
  {
    "objectID": "module1_foundations/python_management.html#tools",
    "href": "module1_foundations/python_management.html#tools",
    "title": "2  Python Management",
    "section": "",
    "text": "uv\nMiniforge\n\n\n\nuv --version\n\n\n\n\n\n\nLearn basics of terminals\n\n\n\nInstalling and using uv requires using a terminal. Being familiar with terminals is generally useful for Python. This course assumes basic knowledge. If you’ve never used a terminal before, then please refer to an introductory resource such as: Windows PowerShell.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Python Management</span>"
    ]
  },
  {
    "objectID": "module1_foundations/python_management.html#installing-python-with-uv",
    "href": "module1_foundations/python_management.html#installing-python-with-uv",
    "title": "2  Python Management",
    "section": "2.2 Installing Python with uv",
    "text": "2.2 Installing Python with uv\nYou can install Python with uv from the command line:\nuv python install\nBy default, this installs the latest version of Python (3.13.2 at the time of writing).\nConfirm it installed correctly by running:\nuv run python --version",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Python Management</span>"
    ]
  },
  {
    "objectID": "module1_foundations/python_management.html#virtual-environments",
    "href": "module1_foundations/python_management.html#virtual-environments",
    "title": "2  Python Management",
    "section": "2.3 Virtual Environments",
    "text": "2.3 Virtual Environments\n\n\n\n\n\n\nNote\n\n\n\nVirtual environments are an advanced Python topic, however, they are fundamental to using uv. Therefore, they will not be covered in depth, but explained just enough to be useful.\n\n\nVirtual environments are useful for isolating dependencies between projects. For example, let’s say you work on two projects: Project A and Project B. If Project A requires a different version of Python than Project B, then you can handle that by creating virtual environments for each project. This avoids a common issue encountered when not using virtual environments. Conceptually, a virtual environment is a single Python version and set of Python packages.\nCreate a new folder, and make a virtual environment:\nuv venv\n\n\n\n\n\n\nTip\n\n\n\nUse the terminal cd command to change its current directory. Alternatively, install Windows Terminal to easily launch a terminal from a folder within File Explorer via the right-click context menu.\n\n\nNotice a folder called .venv was created. Explore that folder to see what it contains. Can you find the file Python.exeand the folder site-packages?\nIt’s good practice to create a single virtual environment in the root directory of each project. Therefore, the remainder of this course assumes you always run uv from within a folder containing a virtual environment.\nRefer to uv’s documentation for additional details.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Python Management</span>"
    ]
  },
  {
    "objectID": "module1_foundations/python_management.html#python-package-management",
    "href": "module1_foundations/python_management.html#python-package-management",
    "title": "2  Python Management",
    "section": "2.4 Python package management",
    "text": "2.4 Python package management\nuv provides two different approaches for Python package management. This course uses their pip interface. Common workflows are shown in the following sections. Refer to uv’s documentation for more details.\n\n2.4.1 Install packages\nInstall Python packages with uv as follows:\nuv pip install &lt;package-name&gt;\nFor example, install mikeio as follows:\nuv pip install mikeio\nLook at the site-packages folder again. Notice that it now includes mikeio and many other packages. When a package is installed, all of its dependencies are also installed automatically.\n\n\n2.4.2 List installed packages\nList all installed Python packages and their versions with:\nuv pip list\n\n\n2.4.3 Upgrade packages\nUpgrade an older package version to the latest version as follows:\nuv pip install --upgrade mikeio\n\n\n2.4.4 Install specific package versions\nOccasionally there’s a need to install an older version of a package, which can be done as follows:\nuv pip install mikeio==1.7.1",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Python Management</span>"
    ]
  },
  {
    "objectID": "module1_foundations/python_management.html#example-video",
    "href": "module1_foundations/python_management.html#example-video",
    "title": "2  Python Management",
    "section": "2.5 Example video",
    "text": "2.5 Example video",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Python Management</span>"
    ]
  },
  {
    "objectID": "module1_foundations/ide.html",
    "href": "module1_foundations/ide.html",
    "title": "3  Integrated Development Environments (IDEs)",
    "section": "",
    "text": "3.1 Why use an IDE?\nAn Integrated Development Environment (IDE) is a software that bundles together tools convenient for software development. This course uses Visual Studio Code as an IDE, which is a popular free and open-source software provided by Microsoft.\nThere are several benefits to using an IDE compared to using a text editor like Notepad:",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Integrated Development Environments (IDEs)</span>"
    ]
  },
  {
    "objectID": "module1_foundations/ide.html#why-use-an-ide",
    "href": "module1_foundations/ide.html#why-use-an-ide",
    "title": "3  Integrated Development Environments (IDEs)",
    "section": "",
    "text": "Designed for easy code writing, with several shortcuts\nSyntax highlighting for more readable code\nAutomatic code completion\nIntegrated terminal\nIntegrated LLM chat and code completion\nHighly customizable with extensions",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Integrated Development Environments (IDEs)</span>"
    ]
  },
  {
    "objectID": "module1_foundations/ide.html#install-visual-studio-code",
    "href": "module1_foundations/ide.html#install-visual-studio-code",
    "title": "3  Integrated Development Environments (IDEs)",
    "section": "3.2 Install Visual Studio Code",
    "text": "3.2 Install Visual Studio Code\nInstall Visual Studio Code (VSCode) according to their official instructions.\n\n\n\n\n\n\nCaution\n\n\n\nYou may stumble upon a software called Visual Studio, which is different than Visual Studio Code.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Integrated Development Environments (IDEs)</span>"
    ]
  },
  {
    "objectID": "module1_foundations/ide.html#getting-started",
    "href": "module1_foundations/ide.html#getting-started",
    "title": "3  Integrated Development Environments (IDEs)",
    "section": "3.3 Getting Started",
    "text": "3.3 Getting Started\nVS Code provides excellent documentation. Please refer to their getting started guide for a basic introduction.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Integrated Development Environments (IDEs)</span>"
    ]
  },
  {
    "objectID": "module1_foundations/ide.html#visual-studio-code-extensions",
    "href": "module1_foundations/ide.html#visual-studio-code-extensions",
    "title": "3  Integrated Development Environments (IDEs)",
    "section": "3.4 Visual Studio Code Extensions",
    "text": "3.4 Visual Studio Code Extensions\nThis course uses the Python extension for VS Code. Extensions can be installed from within VS Code. Refer to VS Code’s documentation for guidance.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Integrated Development Environments (IDEs)</span>"
    ]
  },
  {
    "objectID": "module1_foundations/ide.html#opening-projects",
    "href": "module1_foundations/ide.html#opening-projects",
    "title": "3  Integrated Development Environments (IDEs)",
    "section": "3.5 Opening Projects",
    "text": "3.5 Opening Projects\nVS Code can be used in different ways. This course uses a common workflow of opening VS Code from the root directory of a project folder. Alternatively, open a project folder via “Open Folder” from within VS Code.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Integrated Development Environments (IDEs)</span>"
    ]
  },
  {
    "objectID": "module1_foundations/ide.html#selecting-python-interpreters",
    "href": "module1_foundations/ide.html#selecting-python-interpreters",
    "title": "3  Integrated Development Environments (IDEs)",
    "section": "3.6 Selecting Python Interpreters",
    "text": "3.6 Selecting Python Interpreters\nVS Code should automatically detect virtual environments located in the root project directory.\nOtherwise, there’s an option of manually selecting which Python Interpreter VS Code uses. Access it via the Command Palette (CTRL + SHIFT + P) and typing “Python: Select Interpreter”.\nVS Code uses the selected interpreter for running scripts, as well as for other features like auto completion.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Integrated Development Environments (IDEs)</span>"
    ]
  },
  {
    "objectID": "module1_foundations/ide.html#example---setting-up-a-fresh-project",
    "href": "module1_foundations/ide.html#example---setting-up-a-fresh-project",
    "title": "3  Integrated Development Environments (IDEs)",
    "section": "3.7 Example - Setting up a fresh project",
    "text": "3.7 Example - Setting up a fresh project",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Integrated Development Environments (IDEs)</span>"
    ]
  },
  {
    "objectID": "module1_foundations/python_scripts.html",
    "href": "module1_foundations/python_scripts.html",
    "title": "4  Python Scripts",
    "section": "",
    "text": "4.1 Running Python Scripts\nA Python script is a file with the extension .py that contains Python code that’s executable via Python’s interpreter.\nPython is most powerful when scripts are reused. Therefore, it’s important to understand both how to run scripts others have sent you, as well as how to explain how others can use scripts you wrote.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Scripts</span>"
    ]
  },
  {
    "objectID": "module1_foundations/python_scripts.html#running-python-scripts",
    "href": "module1_foundations/python_scripts.html#running-python-scripts",
    "title": "4  Python Scripts",
    "section": "",
    "text": "4.1.1 Running in Terminal\nYou can run a script from the terminal by running:\nuv run python example_script.py\n\n\n4.1.2 Running in VS Code\nYou can run scripts from VS Code’s user interface. Under the hood, it executes the script in the terminal, so this is only a matter of preference. Refer to VS Code’s documentation on how to run Python code.\n\n\n\n\n\n\nTip\n\n\n\nRunning scripts in debug mode is more convenient via VS Code’s user interface. This lets you walk through code line by line as it executes, which is helpful when investigating unexpected outcomes.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Scripts</span>"
    ]
  },
  {
    "objectID": "module1_foundations/python_scripts.html#script-dependencies",
    "href": "module1_foundations/python_scripts.html#script-dependencies",
    "title": "4  Python Scripts",
    "section": "4.2 Script Dependencies",
    "text": "4.2 Script Dependencies\nAs previously mentioned, Python code includes dependencies on a set of Python packages (e.g. mikeio). If a script is run with a virtual environment that is missing these dependencies, there’ll be an error along the lines of: ModuleNotFoundError: No module named ‘mikeio’. The package listed in the error message (e.g. mikeio) needs to be installed before running the script.\n\n\n\n\n\n\nTip\n\n\n\nuv provides a way of defining dependencies within the script itself, such that they are automatically detected and installed when running the script with uv. Refer to uv’s documentation on script inline metadata for details.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Scripts</span>"
    ]
  },
  {
    "objectID": "module1_foundations/python_scripts.html#example---running-scripts",
    "href": "module1_foundations/python_scripts.html#example---running-scripts",
    "title": "4  Python Scripts",
    "section": "4.3 Example - running scripts",
    "text": "4.3 Example - running scripts",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Scripts</span>"
    ]
  },
  {
    "objectID": "module1_foundations/jupyter_notebooks.html",
    "href": "module1_foundations/jupyter_notebooks.html",
    "title": "5  Jupyter Notebooks",
    "section": "",
    "text": "5.1 Comparison with Python Scripts\nA Jupyter Notebook is a file with the extension .ipynb that combines code, its output, and markdown into an interactive notebook-like experience.\nA key difference is that notebooks are interactive, whereas scripts simply run from start to end. Generally, notebooks are more useful for exploratory or visual workflows (e.g. making plots, or analyzing data). It’s also a great tool for learning Python.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Jupyter Notebooks</span>"
    ]
  },
  {
    "objectID": "module1_foundations/jupyter_notebooks.html#terminology",
    "href": "module1_foundations/jupyter_notebooks.html#terminology",
    "title": "5  Jupyter Notebooks",
    "section": "5.2 Terminology",
    "text": "5.2 Terminology\nThe following are fundamental concepts of Jupyter Notebooks:\n\nCell\n\nA Jupyter Notebook is a collection of cells.\n\nCode Cell\n\nA cell containing Python code, whose output shows below after execution.\n\nCell Output\n\nThe output after executing a cell, which could be many things (e.g. a number, plot, or table)\n\nMarkdown Cell\n\nA cell containing markdown for nicely formatted text.\n\nKernel\n\nResponsible for executing cells. Same as Python virtual environment for the purposes of this course.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Jupyter Notebooks</span>"
    ]
  },
  {
    "objectID": "module1_foundations/jupyter_notebooks.html#running-a-jupyter-notebook",
    "href": "module1_foundations/jupyter_notebooks.html#running-a-jupyter-notebook",
    "title": "5  Jupyter Notebooks",
    "section": "5.3 Running a Jupyter Notebook",
    "text": "5.3 Running a Jupyter Notebook\nThe Python extension for VS Code allows opening jupyter notebook files (.ipynb).\nUpon opening a notebook, all cells are displayed along with any saved output of those cells.\nRunning a notebook first requires selecting the kernel (i.e. the Python virtual environment). If the virtual environment has not installed the package ipykernel, then VS Code will ask to do that. Alternatively, manually install it via:\nuv pip install ipykernel\nNext, “Run All” to run all cells from top to bottom. It’s also possible to run (or re-run) cells individually in any order.\n\n\n\n\n\n\nTip\n\n\n\nIt’s good practice to organize notebooks such that they run from top to bottom.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Jupyter Notebooks</span>"
    ]
  },
  {
    "objectID": "module1_foundations/jupyter_notebooks.html#creating-a-jupyter-notebook",
    "href": "module1_foundations/jupyter_notebooks.html#creating-a-jupyter-notebook",
    "title": "5  Jupyter Notebooks",
    "section": "5.4 Creating a Jupyter Notebook",
    "text": "5.4 Creating a Jupyter Notebook\nCreate a Jupyter Notebook from within VS Code by opening the Command Palette (CTRL + SHIFT + P) and typing “Create: New Jupyter Notebook”.\nSave the notebook in a project folder to help VS Code automatically find the project’s virtual environment. Then, start adding and running cells.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Jupyter Notebooks</span>"
    ]
  },
  {
    "objectID": "module1_foundations/jupyter_notebooks.html#useful-keyboard-shortcuts",
    "href": "module1_foundations/jupyter_notebooks.html#useful-keyboard-shortcuts",
    "title": "5  Jupyter Notebooks",
    "section": "5.5 Useful Keyboard Shortcuts",
    "text": "5.5 Useful Keyboard Shortcuts\nThere’s a few useful keyboard shortcuts when working with notebooks:\n\nShift + Enter: Run the current cell and move to the next.\nCtrl + Enter: Run the current cell.\nA: Insert a new cell above.\nB: Insert a new cell below.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Jupyter Notebooks</span>"
    ]
  },
  {
    "objectID": "module1_foundations/jupyter_notebooks.html#additional-resources",
    "href": "module1_foundations/jupyter_notebooks.html#additional-resources",
    "title": "5  Jupyter Notebooks",
    "section": "5.6 Additional resources",
    "text": "5.6 Additional resources\nFor additional information, refer to VS Code’s documentation on jupyter notebooks.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Jupyter Notebooks</span>"
    ]
  },
  {
    "objectID": "module1_foundations/jupyter_notebooks.html#example---using-jupyter-notebooks",
    "href": "module1_foundations/jupyter_notebooks.html#example---using-jupyter-notebooks",
    "title": "5  Jupyter Notebooks",
    "section": "5.7 Example - Using Jupyter Notebooks",
    "text": "5.7 Example - Using Jupyter Notebooks",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Jupyter Notebooks</span>"
    ]
  },
  {
    "objectID": "module1_foundations/python_basics.html",
    "href": "module1_foundations/python_basics.html",
    "title": "6  Python Basics",
    "section": "",
    "text": "6.1 Using libraries\nThis section provides a crash course on basic Python concepts used throughout the course. It is purposefully brief, with additional resources provided at the end.\nMost functionality useful for MIKE+ modelling exists in Python packages (e.g. mikeio). Therefore, it’s important to understand how to access functionality in a Python package.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Python Basics</span>"
    ]
  },
  {
    "objectID": "module1_foundations/python_basics.html#using-libraries",
    "href": "module1_foundations/python_basics.html#using-libraries",
    "title": "6  Python Basics",
    "section": "",
    "text": "Note\n\n\n\nThe terms package, library, and module are used interchangeably throughout this course.\n\n\n\n6.1.1 Import libraries\nImport libraries using the import statement:\n\nimport math\n\nOr import specific functionality from a library:\n\nfrom math import sqrt\n\n\n\n6.1.2 Objects\nAll imports are objects containing some functionality. Objects have members accessible via the dot notation:\n\nmath.pi\n\n3.141592653589793\n\n\nDot accessors can be chained together, since all members are also objects.\n\nmath.pi.is_integer()\n\nFalse\n\n\nThere are a few common types of objects to be aware of:\n\nModules: reusable code you can import into your program.\nClasses: templates for creating objects with specific properties and behaviors.\nFunctions / Methods: blocks of code that return a result.\nData: any stored information (e.g. numbers, text).\n\nSee the type of an object with:\n\ntype(math)\n\nmodule\n\n\nSee the members of an object with:\n\ndir(math)\n\nGet help for an object:\n\nhelp(math)\n\nGood libraries have documentation. For example, see the documentation for math.\n\n\n6.1.3 Using Functions / Methods\n\n\n\n\n\n\nNote\n\n\n\nThis course will use the terms ‘function’ and ‘method’ interchangeably.\n\n\nUse a function by invoking it with round brackets:\n\nsqrt(25)\n\n5.0\n\n\nBetween the brackets is the function arguments. There’s different ways of specifying arguments. For example, there could be a list of arguments:\n\nmath.pow(2, 3)\n\n8.0\n\n\n\n\n6.1.4 Using Classes\nSome library functionality is provided via a class that needs to be instantiated before using it.\nBelow, the Random class is instantiated and assigned to the identifier my_random for reference later on.\n\nfrom random import Random\nmy_random = Random()\n\nAn instantiation of a class is called an instance, and is also an object whose functionality is accessible with the dot notation:\n\nmy_random.random() # returns a random number\n\n0.09464567445753802",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Python Basics</span>"
    ]
  },
  {
    "objectID": "module1_foundations/python_basics.html#variables",
    "href": "module1_foundations/python_basics.html#variables",
    "title": "6  Python Basics",
    "section": "6.2 Variables",
    "text": "6.2 Variables\nStore data/objects in named variables by using the assignment operator =.\n\nresult = 1 + 1\nresult\n\n2\n\n\n\n\n\n\n\n\nNote\n\n\n\nA valid name must be used. In general, this means it must start with a letter or underscore.\n\n\nVariable names can be referenced anywhere after their definition.\n\nresult = result * 2\nresult\n\n4",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Python Basics</span>"
    ]
  },
  {
    "objectID": "module1_foundations/python_basics.html#collections",
    "href": "module1_foundations/python_basics.html#collections",
    "title": "6  Python Basics",
    "section": "6.3 Collections",
    "text": "6.3 Collections\nA common need is to have a collection of related data. Perhaps the most common type of collection is a list, which is briefly introduced below.\nCreate a list with square brackets. Optionally include comma separated elements, otherwise an empty list is created.\n\nmy_numbers = [1, 2, 3]\nmy_numbers\n\n[1, 2, 3]\n\n\nAppend elements to an existing list.\n\nmy_numbers.append(4)\n\nAccess a specific element by indexing the list with the zero-based index. Zero refers to the first element, one the second, and so on.\n\nmy_numbers[0]\n\n1\n\n\nAccess a subset of a list by slicing it. The example below accesses elements with index 0 up to, but excluding, 2.\n\nmy_numbers[0:2]\n\n[1, 2]",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Python Basics</span>"
    ]
  },
  {
    "objectID": "module1_foundations/python_basics.html#control-logic",
    "href": "module1_foundations/python_basics.html#control-logic",
    "title": "6  Python Basics",
    "section": "6.4 Control Logic",
    "text": "6.4 Control Logic\nControl logic allows the flow of a program to be controlled via boolean conditions.\n\n6.4.1 Conditional statements\nUse if statements to execute code only if the specified condition is true.\n\nif 100 &gt; 10:\n    print(\"100 is greater than 10\")\n\n100 is greater than 10\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe code that the if statement applies to is called a block, which must be indented.\n\n\nUse else statements after an if statement to execute code only if the condition is untrue.\n\nif 100 &lt; 10:\n    print(\"100 is less than 10\")\nelse:\n    print(\"of course, 100 is not less than 10\")\n\nof course, 100 is not less than 10\n\n\n\n\n6.4.2 Loops\nA while loop continuously executes a block of code while the specified condition is true.\n\ni = 0\nwhile i &lt; 3:\n    print(i)\n    i = i + 1\n\n0\n1\n2\n\n\nA for loop executes a block of code per element in a specified collection.\n\nfor fruit in [\"Apple\", \"Banana\", \"Orange\"]:\n    print(fruit)\n\nApple\nBanana\nOrange",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Python Basics</span>"
    ]
  },
  {
    "objectID": "module1_foundations/python_basics.html#additional-resources",
    "href": "module1_foundations/python_basics.html#additional-resources",
    "title": "6  Python Basics",
    "section": "6.5 Additional resources",
    "text": "6.5 Additional resources\nLearning Python should be a continuous endeavor through practice. Luckily there’s an abundance of high quality resources online. Here’s a few examples:\n\nOfficial Python Documentation\nLearn X in Y minutes\nFreeCodeCamp: Scientific Computing with Python\n\n\n\n\n\n\n\nTip\n\n\n\nPython is used by a wide variety of domains (e.g. web development). Try to use resources specific for engineering/science applications for a more efficient learning path.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Python Basics</span>"
    ]
  },
  {
    "objectID": "module1_foundations/python_basics.html#example---using-pythons-interpreter",
    "href": "module1_foundations/python_basics.html#example---using-pythons-interpreter",
    "title": "6  Python Basics",
    "section": "6.6 Example - Using Python’s Interpreter",
    "text": "6.6 Example - Using Python’s Interpreter",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Python Basics</span>"
    ]
  },
  {
    "objectID": "module1_foundations/llm_coding.html",
    "href": "module1_foundations/llm_coding.html",
    "title": "7  LLMs for Coding",
    "section": "",
    "text": "7.1 Ways of using LLMs\nLarge Language Models (LLMs) can significantly enhance coding efficiency. They’re also a great tool for explaining code, which is helpful for learning Python.\nLLMs for coding is an area under rapid development. Here are a few ways of using LLMs for coding, roughly in the order in which they became available for use:\nUsing LLMs is completely optional for the course. However, since GitHub Copilot is free and integrated with VS Code, our suggestion is to try it out as a learning assistant.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>LLMs for Coding</span>"
    ]
  },
  {
    "objectID": "module1_foundations/llm_coding.html#ways-of-using-llms",
    "href": "module1_foundations/llm_coding.html#ways-of-using-llms",
    "title": "7  LLMs for Coding",
    "section": "",
    "text": "Chat interfaces via web (e.g. ChatGPT, Mistral AI)\nChat interfaces via an IDE (e.g. GitHub Copilot Chat)\nInline chat and autocompletion in IDE (e.g. GitHub Copilot)\nAgentic coding with specialized IDEs (e.g. Windsurf)",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>LLMs for Coding</span>"
    ]
  },
  {
    "objectID": "module1_foundations/llm_coding.html#ideas-of-how-to-use-llms-in-coding",
    "href": "module1_foundations/llm_coding.html#ideas-of-how-to-use-llms-in-coding",
    "title": "7  LLMs for Coding",
    "section": "7.2 Ideas of how to use LLMs in coding",
    "text": "7.2 Ideas of how to use LLMs in coding\nA few ideas of how to use LLMs in coding:\n\nWrite scripts from scratch based on a description of what’s needed\nExplain a given script line by line to enhance understanding\nUnderstand cryptic error messages, and get potential solutions\nReview the quality of your code to see if it could be improved",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>LLMs for Coding</span>"
    ]
  },
  {
    "objectID": "module1_foundations/llm_coding.html#example---andrej-karpathy-using-llms-for-coding",
    "href": "module1_foundations/llm_coding.html#example---andrej-karpathy-using-llms-for-coding",
    "title": "7  LLMs for Coding",
    "section": "7.3 Example - Andrej Karpathy using LLMs for coding",
    "text": "7.3 Example - Andrej Karpathy using LLMs for coding",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>LLMs for Coding</span>"
    ]
  },
  {
    "objectID": "module1_foundations/scientific_python.html",
    "href": "module1_foundations/scientific_python.html",
    "title": "8  Scientific Python",
    "section": "",
    "text": "8.1 Package ecosystem for scientific Python\nPython is a general purpose programming language that’s used by a broad range of domains. MIKE+ modelling workflows most closely align with the scientific python community.\nThere are several useful packages for engineering and science. This course will use the following packages:\nCheck out packages sponsored by NumFOCUS for an overview of useful libraries.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Scientific Python</span>"
    ]
  },
  {
    "objectID": "module1_foundations/scientific_python.html#package-ecosystem-for-scientific-python",
    "href": "module1_foundations/scientific_python.html#package-ecosystem-for-scientific-python",
    "title": "8  Scientific Python",
    "section": "",
    "text": "NumPy\nMatplotlib\npandas\n\n\n\n\n\n\n\n\nTip\n\n\n\nDHI builds their Python ecosystem on top of these packages, to enable better integration between them and allow scientists/engineers the flexibility that’s often required.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Scientific Python</span>"
    ]
  },
  {
    "objectID": "module1_foundations/scientific_python.html#numpy",
    "href": "module1_foundations/scientific_python.html#numpy",
    "title": "8  Scientific Python",
    "section": "8.2 NumPy",
    "text": "8.2 NumPy\nNumPy is a package that essentially enables faster numerical computing on large arrays than would otherwise be possible via Python collections. It is foundational to many other packages.\nNumPy is imported as np by convention:\n\nimport numpy as np\n\n\n\n\n\n\n\nNote\n\n\n\nImport as ‘np’ simply imports numpy and creates an alias for it as ‘np’.\n\n\nCreate a NumPy array from a Python collection:\n\nmy_array = np.array([1, 2, 3])\nmy_array\n\narray([1, 2, 3])\n\n\nUse vectorized operations on arrays. For example, multiply all elements of the previous array by 2:\n\nmy_array * 2\n\narray([2, 4, 6])\n\n\nIndex and slice arrays the same way as Python collections:\n\nmy_array[0]\n\nnp.int64(1)\n\n\nPerform aggregation functions on an array (e.g. sum, mean, max):\n\nmy_array.sum()\n\nnp.int64(6)\n\n\nRefer to NumPy’s official documentation for additional information.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Scientific Python</span>"
    ]
  },
  {
    "objectID": "module1_foundations/scientific_python.html#pandas",
    "href": "module1_foundations/scientific_python.html#pandas",
    "title": "8  Scientific Python",
    "section": "8.3 Pandas",
    "text": "8.3 Pandas\nPandas builds upon NumPy with a special focus on tabular data (like spreadsheets, or csv files).\nPandas is imported as ‘pd’ by convention:\n\nimport pandas as pd\n\nCreate a DataFrame, which is like a 2D labeled array (rows + columns):\n\nimport pandas as pd\ndata = [['Alice', 25], ['Bob', 30]]\ndf = pd.DataFrame(data, columns=['name', 'age'])\ndf\n\n\n\n\n\n\n\n\nname\nage\n\n\n\n\n0\nAlice\n25\n\n\n1\nBob\n30\n\n\n\n\n\n\n\nSelect a single column by name:\n\ndf['age']\n\n0    25\n1    30\nName: age, dtype: int64\n\n\nPerform aggregation operations just like as with NumPy:\n\ndf['age'].mean()\n\nnp.float64(27.5)\n\n\nImport data from a csv file into a pandas DataFrame:\n\nrainfall = pd.read_csv('data/fake_daily_rainfall.csv', index_col='date')\nrainfall.head()\n\n\n\n\n\n\n\n\nrainfall_mm\n\n\ndate\n\n\n\n\n\n2025-06-01\n17.450712\n\n\n2025-06-02\n7.926035\n\n\n2025-06-03\n19.715328\n\n\n2025-06-04\n32.845448\n\n\n2025-06-05\n6.487699\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nUse the head method of a DataFrame to view the first five rows of very long DataFrames.\n\n\nCreate plots from a DataFrame:\n\nrainfall.plot(kind='bar')\n\n\n\n\n\n\n\n\nExport a DataFrame to csv, excel, or other formats:\n\nrainfall.to_csv(\"temp.csv\")\nrainfall.to_excel(\"temp.xlsx\")\n\nRefer to pandas’s official documentation for additional information.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Scientific Python</span>"
    ]
  },
  {
    "objectID": "module1_foundations/scientific_python.html#matplotlib",
    "href": "module1_foundations/scientific_python.html#matplotlib",
    "title": "8  Scientific Python",
    "section": "8.4 Matplotlib",
    "text": "8.4 Matplotlib\nMatplotlib is a library for creating plots and is commonly used by other libraries.\nMatplotlib is imported as ‘plt’ by convention:\n\nimport matplotlib.pyplot as plt\n\nCreate a simple line plot:\n\n# Create some data\nx = np.array([1, 2, 3, 4, 5])\ny = x ** 2\n\n# Make the plot\nplt.plot(x, y)              # Plots x vs y\nplt.title(\"My plot\")        # Gives a title to the plot\nplt.xlabel(\"X Axis\")        # Labels the x-axis\nplt.ylabel(\"Y Axis\")        # Labels the y-axis\nplt.grid()                  # Turns on grid lines\n\n\n\n\n\n\n\n\nRefer to Matplotlib’s official documentation for additional information.\nAlso, feel free to check out their example gallery for a sense of what’s possible.",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Scientific Python</span>"
    ]
  },
  {
    "objectID": "module1_foundations/scientific_python.html#example---importing-and-plotting-a-time-series-csv-file",
    "href": "module1_foundations/scientific_python.html#example---importing-and-plotting-a-time-series-csv-file",
    "title": "8  Scientific Python",
    "section": "8.5 Example - Importing and Plotting a Time Series CSV File",
    "text": "8.5 Example - Importing and Plotting a Time Series CSV File",
    "crumbs": [
      "Module 1 - Foundations",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Scientific Python</span>"
    ]
  },
  {
    "objectID": "module1_foundations/homework.html",
    "href": "module1_foundations/homework.html",
    "title": "Homework",
    "section": "",
    "text": "Exercise 1\n\nCreate a GitHub account.\nFind and explore the mikeio1d repository. Can you find its documentation?\nWhat’s the current version of mikeio1d?\nSearch around GitHub and star some repositories you think are cool.\n\nExercise 2\n\nMake a new folder somewhere on your PC.\nOpen the folder in Visual Studio Code.\nCreate a virtual environment in that folder using uv from VS Code’s terminal.\nInstall mikeio1d in the virtual environment using uv.\nList all the packages in the virtual environment. Do you recognize any?\nSelect the Python Interpreter in VS Code to be the virtual environment you created.\n\nExercise 3\n\nFrom VS Code, create a new .py file under the project folder created in exercise two.\nCopy the following code into the script:\n\n\nimport mikeio1d\n\nprint(\"I'm a script that uses mikeio1d version \" + mikeio1d.__version__)\n\n\nRun the script from VS Code’s terminal using uv.\nRun the script from VS Code’s user interface (i.e. via the ‘Run’ menu).\nDo you get the same output for steps 3 and 4?\n\nExercise 4\n\nInstall ipykernel into the same virtual environment of the previous exercises.\nCreate a new Jupyter Notebook from within VS Code.\nMake sure the kernel matches your virtual environment, otherwise update it.\nPaste the code from exercise three into a code cell.\nRun the cell created in the previous step. Does the output match that of exercise three?\n\nExercise 5\n\nInstall the package cowsay into your virtual environment.\nCreate a new script, and import the function cow from cowsay.\nMake a list containing the names of three countries you want to visit.\nLoop over the list, and invoke the function cow by passing the current element of the list.\nRun the script. What do you see?\nTry to get the same output in a jupyter notebook by using two code cells.\n\nExercise 6\n\nDownload this time series csv file into your project folder.\nInstall pandas and matplotlib into your virtual environment.\nCreate a new Jupyter Notebook and import pandas\nLoad the downloaded csv file into a DataFrame using pandas.\nCalculate the minimum, mean, and maximum values.\nPlot the DataFrame. Do the values calculated from the previous step make sense?\n\nPractice Exercises (optional)\n\nJupyter Notebook covering Python basics\nJupyter Notebook covering NumPy\nJupyter Notebook covering Pandas\nJupyter Notebook covering Matplotlib",
    "crumbs": [
      "Module 1 - Foundations",
      "Homework"
    ]
  },
  {
    "objectID": "module2_time_series/index.html",
    "href": "module2_time_series/index.html",
    "title": "Welcome to Module 2!",
    "section": "",
    "text": "This module launches you into the practical world of time series data, a fundamental component of nearly all MIKE+ modelling projects. Our focus is on empowering you to efficiently handle, analyze, and prepare time series for your MIKE+ workflows:\n\nConvert dfs0 to Pandas DataFrame\nSelect, resample, and clean data\nVisualize time series data\nConvert Pandas DataFrame to dfs0\n\nYou’ll build on your knowledge of Pandas, and be introduced to a new library: MIKE IO. Let’s go!\n\n\n\n\n\n\nWhere can I download sample data to follow along?\n\n\n\n\n\nAll of the sample data used in this module is available for download:\n\ndischarge.dfs0\nrain_events_2021_july.csv\nrain_events_2021_june.csv\nsingle_water_level.dfs0\nsirius_idf_rainfall.dfs0",
    "crumbs": [
      "Module 2 - Time Series",
      "Welcome to Module 2!"
    ]
  },
  {
    "objectID": "module2_time_series/01_mikeio.html",
    "href": "module2_time_series/01_mikeio.html",
    "title": "9  MIKE IO",
    "section": "",
    "text": "9.1 What is MIKE IO?\nThis section introduces MIKE IO, a fundamental DHI Python package. You’ll learn what MIKE IO is, the scope of its usage in this course, how to install it, and grasp its basic concepts. We provide a brief overview here; the next section delves into more detail.\nMIKE IO is an open-source Python package developed by DHI, which you might recall from Module 1. It empowers modelers with full flexibility by bridging the gap between various MIKE file formats and Scientific Python’s rich and powerful package ecosystem.",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>MIKE IO</span>"
    ]
  },
  {
    "objectID": "module2_time_series/01_mikeio.html#usage-in-course",
    "href": "module2_time_series/01_mikeio.html#usage-in-course",
    "title": "9  MIKE IO",
    "section": "9.2 Usage in course",
    "text": "9.2 Usage in course\nFor MIKE+ modelers, a key file format is dfs0, DHI’s standard for tabular time series data. You’re likely familiar with dfs0 files for storing data such as rainfall, water levels, or discharge.\nThis course primarily focuses on the dfs0 functionality of MIKE IO, since it’s most relevant for handling time series data in MIKE+. While MIKE IO also supports other formats like dfs2, dfsu, and mesh files, they are intermediate topics beyond the scope of this introductory course.\n\n\n\n\n\n\nAlternative ways of using MIKE IO\n\n\n\n\n\nIt’s important to note that this course primarily focuses on using MIKE IO to get time series data into Pandas DataFrames. This approach is chosen to:\n\nReduce the initial learning curve for Python beginners by leveraging Pandas skills.\nProvide a method that is sufficiently powerful for most common time series tasks in MIKE+ modelling.\n\nMIKE IO itself has a lot of other useful functionalities, especially for working directly with Dataset and DataArray objects, and for handling multidimensional data (like dfs2 or dfsu files). We encourage you to explore the official MIKE IO documentation after mastering the basics of Pandas.",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>MIKE IO</span>"
    ]
  },
  {
    "objectID": "module2_time_series/01_mikeio.html#installation",
    "href": "module2_time_series/01_mikeio.html#installation",
    "title": "9  MIKE IO",
    "section": "9.3 Installation",
    "text": "9.3 Installation\nInstall MIKE IO with:\nuv pip install mikeio\n\n\n\n\n\n\nTip\n\n\n\nAlways check the official MIKE IO’s documentation for the most up-to-date installation instructions and information on the latest versions.",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>MIKE IO</span>"
    ]
  },
  {
    "objectID": "module2_time_series/01_mikeio.html#quick-glance",
    "href": "module2_time_series/01_mikeio.html#quick-glance",
    "title": "9  MIKE IO",
    "section": "9.4 Quick glance",
    "text": "9.4 Quick glance\nLet’s take a quick look at some core MIKE IO objects and how to access them. When you read a dfs0 file, MIKE IO typically returns a Dataset object.\n\nimport mikeio\n\nds = mikeio.read(\"data/sirius_idf_rainfall.dfs0\")\n\n\n\n\n\n\n\nNote\n\n\n\nExample dfs0 is from MIKE+ Example Project: Sirius.\n\n\nThis Dataset object, ds, holds the data and metadata. You can easily access its contents, such as the items:\n\nds.items\n\nThe time axis is a Pandas DatetimeIndex, shared between all items:\n\nds.time\n\nTo access data for a specific item, you can select it from the Dataset, which returns a DataArray object:\n\nda = ds[0] # Access the first item from ds.items",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>MIKE IO</span>"
    ]
  },
  {
    "objectID": "module2_time_series/01_mikeio.html#key-concepts",
    "href": "module2_time_series/01_mikeio.html#key-concepts",
    "title": "9  MIKE IO",
    "section": "9.5 Key Concepts",
    "text": "9.5 Key Concepts\nUnderstanding a few key concepts in MIKE IO will be helpful as you progress through this course:\n\n\nDataset\n\nA Dataset is a collection of one or more DataArray objects that share the same time axis. Think of it as the entire content of a dfs0 file.\n\nDataArray\n\nA DataArray holds the data for a single item, including its time series values and associated metadata. This is comparable to a single column in a dfs0 file when viewed in a tabular format.\n\nItems\n\nEach DataArray within a Dataset represents an “item.” An item is characterized by its name, type (e.g., water level, discharge), unit (e.g., meters, m\\(^3\\)/s), and value type (e.g., instantaneous, accumulated), which are crucial for correct data interpretation in MIKE software.",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>MIKE IO</span>"
    ]
  },
  {
    "objectID": "module2_time_series/01_mikeio.html#additional-reading-optional",
    "href": "module2_time_series/01_mikeio.html#additional-reading-optional",
    "title": "9  MIKE IO",
    "section": "9.6 Additional reading (optional)",
    "text": "9.6 Additional reading (optional)\nThe following sections of MIKE IO’s documentation are particularly relevant for this course:\n\nGetting started\nData Structures\nDataArray\nDataset\nDfs0\nEUM (Units and Types)",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>MIKE IO</span>"
    ]
  },
  {
    "objectID": "module2_time_series/02_reading_dfs0.html",
    "href": "module2_time_series/02_reading_dfs0.html",
    "title": "10  Reading dfs0",
    "section": "",
    "text": "10.1 Workflow\nThis section guides you through loading time series data from dfs0 files into Pandas DataFrames. This approach allows you to leverage your existing Pandas skills, learned in Module 1, for powerful time series analysis and manipulation.\nThe general workflow for working with dfs0 data is as follows:",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Reading dfs0</span>"
    ]
  },
  {
    "objectID": "module2_time_series/02_reading_dfs0.html#workflow",
    "href": "module2_time_series/02_reading_dfs0.html#workflow",
    "title": "10  Reading dfs0",
    "section": "",
    "text": "Read dfs0 file into Dataset\nSubset Dataset for specific items and times (optional)\nConvert Dataset (or DataArray) to DataFrame\nPerform some additional analysis via the DataFrame",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Reading dfs0</span>"
    ]
  },
  {
    "objectID": "module2_time_series/02_reading_dfs0.html#datasets",
    "href": "module2_time_series/02_reading_dfs0.html#datasets",
    "title": "10  Reading dfs0",
    "section": "10.2 Datasets",
    "text": "10.2 Datasets\nThe primary function for reading MIKE IO files is mikeio.read(). It returns a Dataset object, which is a container for one or more DataArray objects (e.g. a specific time series).\nReading a dfs0 file into a Dataset is as simple as calling the read() method with the dfs0 file path as the argument:\n\nds = mikeio.read(\"data/sirius_idf_rainfall.dfs0\")\nds\n\n&lt;mikeio.Dataset&gt;\ndims: (time:22)\ntime: 2019-01-01 00:00:00 - 2019-01-02 00:00:00 (22 non-equidistant records)\ngeometry: GeometryUndefined()\nitems:\n  0:   F=20 &lt;Rainfall Intensity&gt; (mm per hour) - 3\n  1:   F=10 &lt;Rainfall Intensity&gt; (mm per hour) - 3\n  2:   F=5 &lt;Rainfall Intensity&gt; (mm per hour) - 3\n  3:   F=2 &lt;Rainfall Intensity&gt; (mm per hour) - 3\n  4:   F=1 &lt;Rainfall Intensity&gt; (mm per hour) - 3\n  5:   F=0.5 &lt;Rainfall Intensity&gt; (mm per hour) - 3\n  6:   F=0.2 &lt;Rainfall Intensity&gt; (mm per hour) - 3\n  7:   F=0.1 &lt;Rainfall Intensity&gt; (mm per hour) - 3\n  8:   F=0.05 &lt;Rainfall Intensity&gt; (mm per hour) - 3\n\n\nNotice the representation of the Dataset object shows information about:\n\nTotal number of time steps\nTimestamps for first and last time step\nAll the items (i.e. DataArrays) available\n\n\n\n\n\n\n\nRead() loads entire dfs0 into memory by default\n\n\n\n\n\nBy default, mikeio.read() loads the entire dfs0 file into memory. This is fine for smaller files, but for very large dfs0 files, you might want to load only specific items or a particular time range to conserve memory and improve performance. You can do this directly with the items or time arguments in the read() function. For example, to read only the first item (index 0):\n\nds = mikeio.read(\"data/sirius_idf_rainfall.dfs0\", items=0)\nds\n\n&lt;mikeio.Dataset&gt;\ndims: (time:22)\ntime: 2019-01-01 00:00:00 - 2019-01-02 00:00:00 (22 non-equidistant records)\ngeometry: GeometryUndefined()\nitems:\n  0:   F=20 &lt;Rainfall Intensity&gt; (mm per hour) - 3\n\n\nSimilarly, to read only the data for the first time step (index 0):\n\nds = mikeio.read(\"data/sirius_idf_rainfall.dfs0\", time=0)\nds\n\n&lt;mikeio.Dataset&gt;\ndims: ()\ntime: 2019-01-01 00:00:00 (time-invariant)\ngeometry: GeometryUndefined()\nitems:\n  0:   F=20 &lt;Rainfall Intensity&gt; (mm per hour) - 3\n  1:   F=10 &lt;Rainfall Intensity&gt; (mm per hour) - 3\n  2:   F=5 &lt;Rainfall Intensity&gt; (mm per hour) - 3\n  3:   F=2 &lt;Rainfall Intensity&gt; (mm per hour) - 3\n  4:   F=1 &lt;Rainfall Intensity&gt; (mm per hour) - 3\n  5:   F=0.5 &lt;Rainfall Intensity&gt; (mm per hour) - 3\n  6:   F=0.2 &lt;Rainfall Intensity&gt; (mm per hour) - 3\n  7:   F=0.1 &lt;Rainfall Intensity&gt; (mm per hour) - 3\n  8:   F=0.05 &lt;Rainfall Intensity&gt; (mm per hour) - 3",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Reading dfs0</span>"
    ]
  },
  {
    "objectID": "module2_time_series/02_reading_dfs0.html#dataarrays",
    "href": "module2_time_series/02_reading_dfs0.html#dataarrays",
    "title": "10  Reading dfs0",
    "section": "10.3 DataArrays",
    "text": "10.3 DataArrays\nDataArray objects are accessed via the Dataset object after reading.\nSelect a specific DataArray from the Dataset by its index or its name. For example, to select the first DataArray by its index:\n\nda = ds[0]\nda\n\n&lt;mikeio.DataArray&gt;\nname:  F=20\ndims: ()\ntime: 2019-01-01 00:00:00 (time-invariant)\ngeometry: GeometryUndefined()\nvalues: 0.0\n\n\nAlternatively, select a DataArray by its name using square brackets:\n\nda = ds[\" F=20\"]\nda\n\n&lt;mikeio.DataArray&gt;\nname:  F=20\ndims: ()\ntime: 2019-01-01 00:00:00 (time-invariant)\ngeometry: GeometryUndefined()\nvalues: 0.0\n\n\nNotice the representation of the DataArray object is also informative, just like the Dataset object.",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Reading dfs0</span>"
    ]
  },
  {
    "objectID": "module2_time_series/02_reading_dfs0.html#convert-to-pandas",
    "href": "module2_time_series/02_reading_dfs0.html#convert-to-pandas",
    "title": "10  Reading dfs0",
    "section": "10.4 Convert to Pandas",
    "text": "10.4 Convert to Pandas\nYou can convert an entire Dataset (which might contain multiple time series) into a Pandas DataFrame. Each item in the Dataset will become a column in the DataFrame.\n\nimport pandas as pd\n\nds = mikeio.read(\"data/sirius_idf_rainfall.dfs0\")\ndf = ds.to_dataframe()\ndf.head()\n\n\n\n\n\n\n\n\nF=20\nF=10\nF=5\nF=2\nF=1\nF=0.5\nF=0.2\nF=0.1\nF=0.05\n\n\n\n\n2019-01-01 00:00:00\n0.00\n0.000000\n0.0\n0.0\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n\n\n2019-01-01 06:00:00\n0.15\n0.283333\n0.4\n0.4\n0.466667\n0.683333\n0.966667\n1.316667\n1.4\n\n\n2019-01-01 07:00:00\n0.20\n0.400000\n0.6\n0.6\n0.800000\n1.100000\n1.600000\n2.200000\n2.3\n\n\n2019-01-01 08:00:00\n0.30\n0.600000\n0.7\n0.8\n0.900000\n1.400000\n2.000000\n2.800000\n3.1\n\n\n2019-01-01 09:00:00\n0.30\n0.600000\n0.9\n1.0\n1.200000\n1.800000\n2.600000\n3.600000\n4.0\n\n\n\n\n\n\n\nSimilarly, a single DataArray can be converted to a Pandas DataFrame (which will have one data column).\n\nda = ds[\" F=20\"]\ndf_T20 = da.to_dataframe()\ndf_T20.head()\n\n\n\n\n\n\n\n\nF=20\n\n\n\n\n2019-01-01 00:00:00\n0.00\n\n\n2019-01-01 06:00:00\n0.15\n\n\n2019-01-01 07:00:00\n0.20\n\n\n2019-01-01 08:00:00\n0.30\n\n\n2019-01-01 09:00:00\n0.30\n\n\n\n\n\n\n\nOnce your data is in a DataFrame, you can use all of Pandas’ powerful methods. For instance, you can easily plot a time series:\n\ndf_T20.plot(\n    title=\"Rainfall for Return Period F=20\",\n    ylabel=\"Rainfall (mm/hr)\"\n)\n\n\n\n\n\n\n\n\nOr get some descriptive statistics:\n\ndf.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nF=20\n22.0\n2.158333\n4.824954\n0.0\n0.300\n0.60\n1.80\n22.799999\n\n\nF=10\n22.0\n4.039394\n9.101874\n0.0\n0.525\n1.10\n3.00\n43.200001\n\n\nF=5\n22.0\n5.890909\n13.389296\n0.0\n0.725\n1.60\n4.65\n63.599998\n\n\nF=2\n22.0\n9.408333\n17.422466\n0.0\n0.825\n2.00\n8.40\n75.599998\n\n\nF=1\n22.0\n12.641667\n25.616199\n0.0\n0.950\n2.40\n9.75\n114.000000\n\n\nF=0.5\n22.0\n15.833333\n30.442241\n0.0\n1.425\n3.40\n13.50\n134.399994\n\n\nF=0.2\n22.0\n22.475000\n36.939988\n0.0\n2.075\n5.30\n23.40\n151.199997\n\n\nF=0.1\n22.0\n27.500758\n39.726610\n0.0\n2.900\n7.60\n34.05\n153.600006\n\n\nF=0.05\n22.0\n30.627272\n42.803784\n0.0\n3.200\n8.65\n39.75\n158.399994\n\n\n\n\n\n\n\n\n\n\n\n\n\nLearning Python’s scientific ecosystem pays off…\n\n\n\nNotice that using a common structure for data (e.g. DataFrame) unlocks familiar analyses independent of the original data source file format (e.g. dfs0, csv). This is an example of why converting data into a format compatible with the scientific Python ecosystem is useful.",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Reading dfs0</span>"
    ]
  },
  {
    "objectID": "module2_time_series/03_data_selection.html",
    "href": "module2_time_series/03_data_selection.html",
    "title": "11  Data Selection",
    "section": "",
    "text": "11.1 Why subset data?\nThis section explores how to select specific subsets of time series data.\nSelecting a subset of data is useful for:",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Selection</span>"
    ]
  },
  {
    "objectID": "module2_time_series/03_data_selection.html#why-subset-data",
    "href": "module2_time_series/03_data_selection.html#why-subset-data",
    "title": "11  Data Selection",
    "section": "",
    "text": "focusing analysis on data of interest (e.g. specific item or time range)\nreducing memory usage and computational overhead (helpful for large files)\ngenerating relevant illustrations (plots and table views)",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Selection</span>"
    ]
  },
  {
    "objectID": "module2_time_series/03_data_selection.html#alternative-methods",
    "href": "module2_time_series/03_data_selection.html#alternative-methods",
    "title": "11  Data Selection",
    "section": "11.2 Alternative Methods",
    "text": "11.2 Alternative Methods\nThere are various ways of selecting subsets of dfs0 data. This section covers two different approaches:\n\nUsing mikeio.read()\nUsing Pandas DataFrame\n\nAs mentioned, MIKE IO also provides additional functionality for selecting subsets, however this course focuses on Pandas for simplicity.\n\n\n\n\n\n\nMemory considerations\n\n\n\n\n\nSelecting data via the read() method is generally most performant, since it will avoid loading the entire file into memory. Selecting data via Dataset, DataArray, and DataFrame objects requires first loading the entire file into memory.\nA dfs0 file is a special case where the entire file is loaded into memory regardless, however that will not be the case for other dfs formats (e.g. dfs2, dfsu). Therefore, it’s a good practice to use the read() method when you know which data you want in advance.",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Selection</span>"
    ]
  },
  {
    "objectID": "module2_time_series/03_data_selection.html#selecting-items",
    "href": "module2_time_series/03_data_selection.html#selecting-items",
    "title": "11  Data Selection",
    "section": "11.3 Selecting Items",
    "text": "11.3 Selecting Items\nWhen reading data with mikeio.read(), the items argument lets you specify which items to load. You can do this by providing a list of item names.\n\nds = mikeio.read(\n    \"data/sirius_idf_rainfall.dfs0\", \n    items=[\" F=1\", \" F=2\"]\n)\nds\n\n&lt;mikeio.Dataset&gt;\ndims: (time:22)\ntime: 2019-01-01 00:00:00 - 2019-01-02 00:00:00 (22 non-equidistant records)\ngeometry: GeometryUndefined()\nitems:\n  0:   F=1 &lt;Rainfall Intensity&gt; (mm per hour) - 3\n  1:   F=2 &lt;Rainfall Intensity&gt; (mm per hour) - 3\n\n\nAlternatively, you can select items using their numerical indices (zero-based). For example, to load the first and third items:\n\nds = mikeio.read(\n    \"data/sirius_idf_rainfall.dfs0\",\n    items=[4, 3]\n)\nds\n\n&lt;mikeio.Dataset&gt;\ndims: (time:22)\ntime: 2019-01-01 00:00:00 - 2019-01-02 00:00:00 (22 non-equidistant records)\ngeometry: GeometryUndefined()\nitems:\n  0:   F=1 &lt;Rainfall Intensity&gt; (mm per hour) - 3\n  1:   F=2 &lt;Rainfall Intensity&gt; (mm per hour) - 3\n\n\n\n\n\n\n\n\nTip\n\n\n\nUsing item indices can be convenient, especially for quick explorations. However, specifying item names explicitly makes your code more readable and robust to changes in the dfs0 file structure, such as if items are reordered.\n\n\nFrom a Pandas DataFrame, you can select items using standard Pandas column selection techniques.\n\nds = mikeio.read(\"data/sirius_idf_rainfall.dfs0\")\ndf = ds.to_dataframe()\ndf.head()\n\n\n\n\n\n\n\n\nF=20\nF=10\nF=5\nF=2\nF=1\nF=0.5\nF=0.2\nF=0.1\nF=0.05\n\n\n\n\n2019-01-01 00:00:00\n0.00\n0.000000\n0.0\n0.0\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n\n\n2019-01-01 06:00:00\n0.15\n0.283333\n0.4\n0.4\n0.466667\n0.683333\n0.966667\n1.316667\n1.4\n\n\n2019-01-01 07:00:00\n0.20\n0.400000\n0.6\n0.6\n0.800000\n1.100000\n1.600000\n2.200000\n2.3\n\n\n2019-01-01 08:00:00\n0.30\n0.600000\n0.7\n0.8\n0.900000\n1.400000\n2.000000\n2.800000\n3.1\n\n\n2019-01-01 09:00:00\n0.30\n0.600000\n0.9\n1.0\n1.200000\n1.800000\n2.600000\n3.600000\n4.0\n\n\n\n\n\n\n\nTo select a single item:\n\ndf[[\" F=20\"]].head()\n\n\n\n\n\n\n\n\nF=20\n\n\n\n\n2019-01-01 00:00:00\n0.00\n\n\n2019-01-01 06:00:00\n0.15\n\n\n2019-01-01 07:00:00\n0.20\n\n\n2019-01-01 08:00:00\n0.30\n\n\n2019-01-01 09:00:00\n0.30\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIndexing with a list returns another DataFrame, whereas indexing with a single value returns a Series.\n\n\nFor multiple items, provide a list of column names:\n\ndf[[\" F=1\", \" F=2\"]].head()\n\n\n\n\n\n\n\n\nF=1\nF=2\n\n\n\n\n2019-01-01 00:00:00\n0.000000\n0.0\n\n\n2019-01-01 06:00:00\n0.466667\n0.4\n\n\n2019-01-01 07:00:00\n0.800000\n0.6\n\n\n2019-01-01 08:00:00\n0.900000\n0.8\n\n\n2019-01-01 09:00:00\n1.200000\n1.0",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Selection</span>"
    ]
  },
  {
    "objectID": "module2_time_series/03_data_selection.html#selecting-time-steps",
    "href": "module2_time_series/03_data_selection.html#selecting-time-steps",
    "title": "11  Data Selection",
    "section": "11.4 Selecting Time Steps",
    "text": "11.4 Selecting Time Steps\nWhen reading data with mikeio.read(), the time argument allows for various ways to specify the desired subset.\nYou can select by a single time step index (e.g., the first time step, index 0).\n\nds = mikeio.read(\n    \"data/single_water_level.dfs0\",\n    time=0\n)\nds\n\n&lt;mikeio.Dataset&gt;\ndims: ()\ntime: 1993-12-02 00:00:00 (time-invariant)\ngeometry: GeometryUndefined()\nitems:\n  0:  ST 2: WL (m) &lt;Water Level&gt; (meter)\n\n\nOr provide a list of indices for specific time steps (e.g., the first three time steps).\n\nds = mikeio.read(\n    \"data/single_water_level.dfs0\",\n    time=[0,1,2]\n)\nds\n\n&lt;mikeio.Dataset&gt;\ndims: (time:3)\ntime: 1993-12-02 00:00:00 - 1993-12-02 01:00:00 (3 records)\ngeometry: GeometryUndefined()\nitems:\n  0:  ST 2: WL (m) &lt;Water Level&gt; (meter)\n\n\nYou can also use timestamp strings.\n\nds = mikeio.read(\n    \"data/single_water_level.dfs0\",\n    time=\"1993-12-02 00:00:00\"\n)\nds\n\n&lt;mikeio.Dataset&gt;\ndims: ()\ntime: 1993-12-02 00:00:00 (time-invariant)\ngeometry: GeometryUndefined()\nitems:\n  0:  ST 2: WL (m) &lt;Water Level&gt; (meter)\n\n\nSelect multiple timestamps with a more general string, such as all times on a specific date.\n\nds = mikeio.read(\n    \"data/single_water_level.dfs0\",\n    time=\"1993-12-03\"\n)\nds\n\n&lt;mikeio.Dataset&gt;\ndims: (time:48)\ntime: 1993-12-03 00:00:00 - 1993-12-03 23:30:00 (48 records)\ngeometry: GeometryUndefined()\nitems:\n  0:  ST 2: WL (m) &lt;Water Level&gt; (meter)\n\n\nTo specify a time range, use Python’s slice() object with start and end timestamps:\n\nds = mikeio.read(\n    \"data/single_water_level.dfs0\",\n    time=slice(\"1993-12-02 12:00\", \"1993-12-02 16:00\")\n)\nds\n\n&lt;mikeio.Dataset&gt;\ndims: (time:9)\ntime: 1993-12-02 12:00:00 - 1993-12-02 16:00:00 (9 records)\ngeometry: GeometryUndefined()\nitems:\n  0:  ST 2: WL (m) &lt;Water Level&gt; (meter)\n\n\n\n\n\n\n\n\nTip\n\n\n\nPython’s slice() method is versatile for defining ranges. While list-like slicing notation (e.g., time_series[start:end]) is common with Pandas DataFrames, slice(start, end) is the explicit way to create a slice object, often used in functions like mikeio.read().\n\n\nFrom a Pandas DataFrame, standard indexing and slicing techniques of the DatetimeIndex may be used.\nTo select by time step index, use .iloc.\n\nds = mikeio.read(\"data/single_water_level.dfs0\")\ndf = ds.to_dataframe()\ndf.iloc[[0]] \n\n\n\n\n\n\n\n\nST 2: WL (m)\n\n\n\n\n1993-12-02\n-0.2689\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nProviding a list to iloc returns another DataFrame, whereas providing a single value returns a Series.\n\n\nFor the first three time steps:\n\ndf.iloc[0:3]\n\n\n\n\n\n\n\n\nST 2: WL (m)\n\n\n\n\n1993-12-02 00:00:00\n-0.2689\n\n\n1993-12-02 00:30:00\n-0.2847\n\n\n1993-12-02 01:00:00\n-0.3020\n\n\n\n\n\n\n\nFor selection by timestamp strings, use .loc.\n\ndf.loc[[\"1993-12-02 00:00:00\"]]\n\n\n\n\n\n\n\n\nST 2: WL (m)\n\n\n\n\n1993-12-02\n-0.2689\n\n\n\n\n\n\n\nTo select all data for a particular day:\n\ndf.loc[\"1993-12-03\"].head()\n\n\n\n\n\n\n\n\nST 2: WL (m)\n\n\n\n\n1993-12-03 00:00:00\n0.0879\n\n\n1993-12-03 00:30:00\n0.0951\n\n\n1993-12-03 01:00:00\n0.0988\n\n\n1993-12-03 01:30:00\n0.0836\n\n\n1993-12-03 02:00:00\n0.0634\n\n\n\n\n\n\n\nAnd for a range between start and end timestamps:\n\ndf.loc[\"1993-12-02 12:00\":\"1993-12-02 16:00\"]\n\n\n\n\n\n\n\n\nST 2: WL (m)\n\n\n\n\n1993-12-02 12:00:00\n-0.4590\n\n\n1993-12-02 12:30:00\n-0.4698\n\n\n1993-12-02 13:00:00\n-0.4812\n\n\n1993-12-02 13:30:00\n-0.4919\n\n\n1993-12-02 14:00:00\n-0.5012\n\n\n1993-12-02 14:30:00\n-0.4798\n\n\n1993-12-02 15:00:00\n-0.4486\n\n\n1993-12-02 15:30:00\n-0.4137\n\n\n1993-12-02 16:00:00\n-0.3772\n\n\n\n\n\n\n\nA key distinction in Pandas is between .iloc and .loc:\n\n.iloc is used for integer-location based indexing (by position, e.g., df.iloc[0] for the first row).\n.loc is used for label-based indexing (by index names or boolean arrays, e.g., df.loc['2023-01-01']).\n\nWhen working with time series data having a DatetimeIndex, .loc is particularly powerful as it allows you to use date/time strings for intuitive selections and slicing, as shown in the examples.",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Selection</span>"
    ]
  },
  {
    "objectID": "module2_time_series/03_data_selection.html#example",
    "href": "module2_time_series/03_data_selection.html#example",
    "title": "11  Data Selection",
    "section": "11.5 Example",
    "text": "11.5 Example\nLet’s tie these concepts together with an example of plotting a subset of a dfs0 file.\n1. Read a specific item of the dfs0 file into a Dataset\n\nds = mikeio.read(\"data/single_water_level.dfs0\", items=\"ST 2: WL (m)\")\nds\n\n&lt;mikeio.Dataset&gt;\ndims: (time:577)\ntime: 1993-12-02 00:00:00 - 1993-12-14 00:00:00 (577 records)\ngeometry: GeometryUndefined()\nitems:\n  0:  ST 2: WL (m) &lt;Water Level&gt; (meter)\n\n\n2. Convert to Pandas DataFrame:\n\ndf = ds.to_dataframe()\ndf\n\n\n\n\n\n\n\n\nST 2: WL (m)\n\n\n\n\n1993-12-02 00:00:00\n-0.2689\n\n\n1993-12-02 00:30:00\n-0.2847\n\n\n1993-12-02 01:00:00\n-0.3020\n\n\n1993-12-02 01:30:00\n-0.3223\n\n\n1993-12-02 02:00:00\n-0.3483\n\n\n...\n...\n\n\n1993-12-13 22:00:00\n-0.0462\n\n\n1993-12-13 22:30:00\n-0.0522\n\n\n1993-12-13 23:00:00\n-0.0619\n\n\n1993-12-13 23:30:00\n-0.0717\n\n\n1993-12-14 00:00:00\n-0.0814\n\n\n\n\n577 rows × 1 columns\n\n\n\n3. Filter the Pandas DataFrame for the time range of interest.\n\ndf = df.loc[\"1993-12-02 00:00\":\"1993-12-02 4:00\"]\ndf\n\n\n\n\n\n\n\n\nST 2: WL (m)\n\n\n\n\n1993-12-02 00:00:00\n-0.2689\n\n\n1993-12-02 00:30:00\n-0.2847\n\n\n1993-12-02 01:00:00\n-0.3020\n\n\n1993-12-02 01:30:00\n-0.3223\n\n\n1993-12-02 02:00:00\n-0.3483\n\n\n1993-12-02 02:30:00\n-0.3644\n\n\n1993-12-02 03:00:00\n-0.3778\n\n\n1993-12-02 03:30:00\n-0.3983\n\n\n1993-12-02 04:00:00\n-0.4192\n\n\n\n\n\n\n\n4. Plot\n\nax = df.plot()\nax.set_title(\"Water Level at Night\")\nax.set_ylabel(\"Water Level (m)\")\nax.grid(which=\"both\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nPlot methods often return a Matplotlib Axes object, conventionally called ax. Use it to customize the plot before it’s displayed a Jupyter Cell.",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data Selection</span>"
    ]
  },
  {
    "objectID": "module2_time_series/04_resampling.html",
    "href": "module2_time_series/04_resampling.html",
    "title": "12  Resampling",
    "section": "",
    "text": "12.1 What is Resampling?\nResampling is a powerful technique for changing the frequency of time series data, a common task when working with MIKE+ model inputs or outputs.\nAt its core, resampling involves adjusting the time steps in your data. There are two main types:\nA prerequisite for resampling in Pandas is that the DataFrame must have a DatetimeIndex. This will be the case if it was created via MIKE IO’s to_dataframe() method.",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Resampling</span>"
    ]
  },
  {
    "objectID": "module2_time_series/04_resampling.html#what-is-resampling",
    "href": "module2_time_series/04_resampling.html#what-is-resampling",
    "title": "12  Resampling",
    "section": "",
    "text": "Downsampling: reducing the frequency of data points (e.g., hourly to daily rainfall).\nUpsampling: increasing the frequency of data points (e.g., hourly to minutely discharge).\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf a DataFrame’s index is time-like but not already a DatetimeIndex, you can usually convert it with:\ndf.index = pd.to_datetime(df.index)",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Resampling</span>"
    ]
  },
  {
    "objectID": "module2_time_series/04_resampling.html#why-resample",
    "href": "module2_time_series/04_resampling.html#why-resample",
    "title": "12  Resampling",
    "section": "12.2 Why Resample?",
    "text": "12.2 Why Resample?\nTwo common motivations for resampling time series data in MIKE+ modelling are:\n\nAligning Series: comparing time series that were recorded at different frequencies (e.g., aligning 15-minute model results with hourly observations).\nSmoothing Data: reducing noise to highlight underlying trends by aggregating data over longer periods (e.g., hourly average flow from noisy instantaneous readings).",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Resampling</span>"
    ]
  },
  {
    "objectID": "module2_time_series/04_resampling.html#how-to-resample-with-pandas",
    "href": "module2_time_series/04_resampling.html#how-to-resample-with-pandas",
    "title": "12  Resampling",
    "section": "12.3 How to resample (with Pandas)",
    "text": "12.3 How to resample (with Pandas)\nPandas provides a straightforward resample() method for time series data.\nThe general syntax is: df.resample(&lt;rule&gt;).&lt;aggregation_or_fill_method&gt;().\n\nrule\n\nA string specifying the target frequency (e.g., ‘D’ for daily).\n\naggregation_or_fill_method\n\nA function to apply to the data within each new time bin. For downsampling, this is typically an aggregation like .mean() or .sum(). For upsampling, this is a fill method like .ffill() or .interpolate().\n\n\nA quick example for illustration with the following DataFrame:\n\ndf.head()\n\n\n\n\n\n\n\n\nST 2: WL (m)\n\n\n\n\n1993-12-02 00:00:00\n-0.2689\n\n\n1993-12-02 00:30:00\n-0.2847\n\n\n1993-12-02 01:00:00\n-0.3020\n\n\n1993-12-02 01:30:00\n-0.3223\n\n\n1993-12-02 02:00:00\n-0.3483\n\n\n\n\n\n\n\nTo resample this half-hourly data to daily mean values:\n\ndf_daily_mean = df.resample('D').mean()\ndf_daily_mean.head()\n\n\n\n\n\n\n\n\nST 2: WL (m)\n\n\n\n\n1993-12-02\n-0.302979\n\n\n1993-12-03\n0.041185\n\n\n1993-12-04\n0.014558\n\n\n1993-12-05\n0.265933\n\n\n1993-12-06\n-0.004035\n\n\n\n\n\n\n\n\n\nShow Plotting Code\nax = df.plot()\ndf_daily_mean.plot(ax=ax)\nax.legend([\"Original\", \"Downsampled\"])",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Resampling</span>"
    ]
  },
  {
    "objectID": "module2_time_series/04_resampling.html#common-frequency-rules",
    "href": "module2_time_series/04_resampling.html#common-frequency-rules",
    "title": "12  Resampling",
    "section": "12.4 Common Frequency Rules",
    "text": "12.4 Common Frequency Rules\nPandas offers many frequency aliases (rules). Some of the most common include:\n\n\"M\": Month-end frequency\n\"W\": Weekly frequency (defaults to Sunday)\n\"D\": Calendar day frequency\n\"H\": Hourly frequency\n\"15min\": 15-minute frequency\n\n\n\n\n\n\n\nNote\n\n\n\nFor a comprehensive list of frequency strings (offset aliases), refer to the Pandas documentation on Time Series / Date functionality.",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Resampling</span>"
    ]
  },
  {
    "objectID": "module2_time_series/04_resampling.html#aggregation-methods-downsampling",
    "href": "module2_time_series/04_resampling.html#aggregation-methods-downsampling",
    "title": "12  Resampling",
    "section": "12.5 Aggregation Methods (Downsampling)",
    "text": "12.5 Aggregation Methods (Downsampling)\nWhen downsampling, you are reducing the number of data points, so you need to decide how to aggregate the values within each new, larger time period. Common aggregation methods include:\n\n.mean(): calculate the average of the values.\n.sum(): calculate the sum of the values.\n.first(): select the first value in the period.\n.last(): select the last value in the period.\n.min(): find the minimum value.\n.max(): find the maximum value.\n\n\n\n\n\n\n\nTip\n\n\n\nThe choice of aggregation method depends on the nature of your data and what you want to represent. For instance, rainfall is often summed, while water levels or flows might be averaged.\n\n\nResample to daily values by choosing the maximum value on each day.\n\ndf_daily_max = df.resample('D').max()\ndf_daily_max.head()\n\n\n\n\n\n\n\n\nST 2: WL (m)\n\n\n\n\n1993-12-02\n0.0799\n\n\n1993-12-03\n0.1486\n\n\n1993-12-04\n0.1583\n\n\n1993-12-05\n0.5106\n\n\n1993-12-06\n0.1793\n\n\n\n\n\n\n\nOr, choose the minimum value on each day.\n\ndf_daily_min = df.resample('D').min()\ndf_daily_min.head()\n\n\n\n\n\n\n\n\nST 2: WL (m)\n\n\n\n\n1993-12-02\n-0.5012\n\n\n1993-12-03\n-0.0701\n\n\n1993-12-04\n-0.1112\n\n\n1993-12-05\n0.0524\n\n\n1993-12-06\n-0.1114\n\n\n\n\n\n\n\nCompare these two aggregation methods with a plot.\n\n\nShow Plotting Code\nax = df.plot(color='grey')\ndf_daily_mean.plot(ax=ax, linestyle=\"--\")\ndf_daily_min.plot(ax=ax, linestyle=\"--\")\ndf_daily_max.plot(ax=ax, linestyle=\"--\")\nax.legend([\"Original\", \"Downsample (mean)\", \"Downsample (min)\", \"Downsample (max)\"])\nax.grid(which=\"both\")",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Resampling</span>"
    ]
  },
  {
    "objectID": "module2_time_series/04_resampling.html#fill-methods-upsampling",
    "href": "module2_time_series/04_resampling.html#fill-methods-upsampling",
    "title": "12  Resampling",
    "section": "12.6 Fill Methods (Upsampling)",
    "text": "12.6 Fill Methods (Upsampling)\nWhen upsampling, you are increasing the number of data points, which means you’ll have new time steps with no existing data. You need to specify a method to fill these gaps. Common fill methods include:\n\n.ffill() (forward fill): propagate the last valid observation forward.\n.bfill() (backward fill): use the next valid observation to fill the gap.\n.interpolate(): fill nan values using an interpolation method (e.g., linear, spline).\n\n\n\n\n\n\n\nNote\n\n\n\nnan stands for ‘not a number’, which is a common way to represent missing values. See NumPy’s nan.\n\n\nRecall our original DataFrame had half-hourly time steps:\n\ndf.head(2) # show only first two rows\n\n\n\n\n\n\n\n\nST 2: WL (m)\n\n\n\n\n1993-12-02 00:00:00\n-0.2689\n\n\n1993-12-02 00:30:00\n-0.2847\n\n\n\n\n\n\n\nUpsample this to a resolution of one minute, comparing ffill and bfill:\n\ndf.resample(\"5min\").ffill().head()\n\n\n\n\n\n\n\n\nST 2: WL (m)\n\n\n\n\n1993-12-02 00:00:00\n-0.2689\n\n\n1993-12-02 00:05:00\n-0.2689\n\n\n1993-12-02 00:10:00\n-0.2689\n\n\n1993-12-02 00:15:00\n-0.2689\n\n\n1993-12-02 00:20:00\n-0.2689\n\n\n\n\n\n\n\n\ndf.resample(\"5min\").bfill().head()\n\n\n\n\n\n\n\n\nST 2: WL (m)\n\n\n\n\n1993-12-02 00:00:00\n-0.2689\n\n\n1993-12-02 00:05:00\n-0.2847\n\n\n1993-12-02 00:10:00\n-0.2847\n\n\n1993-12-02 00:15:00\n-0.2847\n\n\n1993-12-02 00:20:00\n-0.2847\n\n\n\n\n\n\n\nCompare the difference between these two. Find the new time stamps and how their values were chosen.\nDepending on use case, a more appropriate approach may be filling gaps with linear interpolation:\n\ndf_interpolated = df.resample('5min').interpolate(method='linear')\ndf_interpolated.head()\n\n\n\n\n\n\n\n\nST 2: WL (m)\n\n\n\n\n1993-12-02 00:00:00\n-0.268900\n\n\n1993-12-02 00:05:00\n-0.271533\n\n\n1993-12-02 00:10:00\n-0.274167\n\n\n1993-12-02 00:15:00\n-0.276800\n\n\n1993-12-02 00:20:00\n-0.279433\n\n\n\n\n\n\n\nCompare interpolation to original data for a zoomed-in time period:\n\n\nShow Plotting Code\nsubset = slice(\"1993-12-02 00:00:00\", \"1993-12-02 08:00:00\")\nax = df.loc[subset].plot(color=\"grey\", alpha=0.7, linewidth=8)\ndf_interpolated.loc[subset].plot(ax=ax, linestyle=\"--\")\nax.legend([\"Original\", \"Interpolated\"])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nUpsampling should be done with caution, as it involves making assumptions about the data between known points. The choice of fill method can significantly impact the resulting time series.",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Resampling</span>"
    ]
  },
  {
    "objectID": "module2_time_series/05_visualization.html",
    "href": "module2_time_series/05_visualization.html",
    "title": "13  Data Visualization",
    "section": "",
    "text": "13.1 Why Visualize Data?\nVisualizing time series data is a critical step in any MIKE+ modelling workflow. Effective plots can help understand data quality, model behavior, the agreement between simulations and observations, as well as communicating key findings.\nVisual inspection of data serves several key purposes in the modelling process:",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "module2_time_series/05_visualization.html#why-visualize-data",
    "href": "module2_time_series/05_visualization.html#why-visualize-data",
    "title": "13  Data Visualization",
    "section": "",
    "text": "Validate input data: Quickly identify anomalies, gaps, or questionable patterns in input time series like rainfall, flow, or water levels.\nGrasp system behavior: Understand underlying trends, seasonality, and extreme events within your datasets.\nCalibrate/validate: Graphically compare simulated results against observed data to assess model performance.\nDiagnose model errors: Pinpoint discrepancies in timing, magnitude, or overall patterns between model output and reality.\nCommunicate results: Create clear visuals to share modelling outcomes, impacts of different scenarios, or model performance metrics with stakeholders.",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "module2_time_series/05_visualization.html#simple-statistics",
    "href": "module2_time_series/05_visualization.html#simple-statistics",
    "title": "13  Data Visualization",
    "section": "13.2 Simple Statistics",
    "text": "13.2 Simple Statistics\nBefore diving into plots, it’s often useful to get a quick numerical summary of your data.\nAssuming you have a DataFrame df containing your time series with both observed and model data:\n\ndf.head()\n\n\n\n\n\n\n\n\nModel\nObserved\n\n\n\n\n1993-12-02 00:00:00\n-0.2689\n-0.219229\n\n\n1993-12-02 00:30:00\n-0.2847\n-0.298526\n\n\n1993-12-02 01:00:00\n-0.3020\n-0.237231\n\n\n1993-12-02 01:30:00\n-0.3223\n-0.169997\n\n\n1993-12-02 02:00:00\n-0.3483\n-0.371715\n\n\n\n\n\n\n\nThe describe() method provides useful statistics of each column in the DataFrame.\n\ndf.describe()\n\n\n\n\n\n\n\n\nModel\nObserved\n\n\n\n\ncount\n577.000000\n577.000000\n\n\nmean\n-0.005975\n-0.007724\n\n\nstd\n0.219331\n0.247389\n\n\nmin\n-0.501200\n-0.671723\n\n\n25%\n-0.136900\n-0.162345\n\n\n50%\n-0.000200\n-0.009162\n\n\n75%\n0.124900\n0.154040\n\n\nmax\n0.510600\n0.699579",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "module2_time_series/05_visualization.html#useful-time-series-plots",
    "href": "module2_time_series/05_visualization.html#useful-time-series-plots",
    "title": "13  Data Visualization",
    "section": "13.3 Useful Time Series Plots",
    "text": "13.3 Useful Time Series Plots\nThis section showcases a few useful plot types for time series data in MIKE+ modelling.\n\n13.3.1 Line Plots\nLine plots are essential for visualizing temporal patterns in hydraulic data like flows, water levels, or rainfall. They are also the primary way to compare simulated versus observed time series.\nYou can plot a single series directly from a DataFrame column:\n\ndf['Observed'].plot(\n    title='Observed Flow Over Time',\n    xlabel='Time',\n    ylabel='Flow (m$^3$/s)'\n)\n\n\n\n\n\n\n\n\nCompare two time series, such as observed and modelled flow:\n\ndf[['Observed', 'Model']].plot(\n    title='Flow Comparison: Observed vs. Model',\n    ylabel='Flow (m$^3$/s)'\n)\n\n\n\n\n\n\n\n\n\n\n13.3.2 Rolling Mean / Moving Average Plot\nThis plot helps smooth out noisy time series data, such as high-frequency sensor readings for flow or water level. This smoothing can make it easier to visualize underlying trends or long-term patterns.\n\ndf['Observed_Rolling_Mean'] = df['Observed'].rolling(window=6).mean()\n\ndf[['Observed', 'Observed_Rolling_Mean']].plot(\n    title='Observed 6-Hour Rolling Mean',\n    ylabel='Flow (m$^3$/s)'\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nAdjust the window size in the .rolling() method to control the amount of smoothing. Larger windows result in smoother trends but might obscure shorter-term variations.\n\n\n\n\n13.3.3 Scatter Plots\nScatter plots are particularly useful for model calibration. By plotting paired observed values against simulated values, you can assess point-by-point agreement.\n\nax = df.plot.scatter(\n    x='Observed',\n    y='Model',\n    alpha=0.5, # so we can see overlapping points better\n    title='Observed vs. Model'\n)\n\n# plot 1:1 line\nmax_val = max(df['Observed'].max(), df['Model'].max())\nmin_val = min(df['Observed'].min(), df['Model'].min())\nax.plot([min_val, max_val], [min_val, max_val], 'k--', label='1:1 Line')  # black dashed line\nax.legend()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nPoints clustering around the 1:1 line (the dashed line in the example) indicate good agreement.\n\n\n\n\n13.3.4 Box Plots\nTo understand seasonal variability in your data (e.g. diurnal or seasonal flow patterns), box plots can be effective.\n\ndf['Day'] = df.index.day_name()\nax = df.boxplot(column='Observed', by='Day')\nax.get_figure().suptitle(\"\") # remove figure title, just use axes title\nax.set_title(\"Flows by Day of Week\")\nax.set_ylabel(\"Water level (m)\")\n\nText(0, 0.5, 'Water level (m)')\n\n\n\n\n\n\n\n\n\n\n\n13.3.5 Cumulative Sum Plots\nCumulative sum plots are excellent for assessing overall water balance or comparing total accumulated volumes (e.g., rainfall, runoff) between observed and simulated data over a period.\n\ndf_discharge.cumsum().plot(ylabel=\"Cumulative Discharge (m$^3$/s)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCumulative sums for mass balance require calculating volume differentials. The example below is a simplified approach, recognizing they time series share the same time axis.\n\n\n\n\n13.3.6 Distribution Plots\nHistograms help you examine the frequency distribution of variables, such as water levels or flows. This can be useful for comparing the overall statistical profile of observed versus simulated data or understanding the prevalence of certain magnitudes.\n\ndf.plot.hist(bins=15, alpha=0.5)\n\n\n\n\n\n\n\n\nSimilarly, review frequency distribution with KDE plots:\n\ndf.plot.kde()",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "module2_time_series/05_visualization.html#saving-plots",
    "href": "module2_time_series/05_visualization.html#saving-plots",
    "title": "13  Data Visualization",
    "section": "13.4 Saving Plots",
    "text": "13.4 Saving Plots\nEasily save plots for inclusion in reports via plt.savefig().\n\nimport matplotlib.pyplot as plt\n\nax = df['Observed'].plot(title='Daily Average Flow', ylabel=\"Flow ($m^3/s$)\")\nplt.savefig(\"my_plot.png\")",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "module2_time_series/06_data_cleaning.html",
    "href": "module2_time_series/06_data_cleaning.html",
    "title": "14  Data Cleaning",
    "section": "",
    "text": "14.1 Missing Values\nData cleaning is an essential step in any MIKE+ modelling workflow to ensure your input data is complete. This section covers handling missing values (e.g. nan). Additionally, it introduces the topic of detecting anomalies in time series data.\nDHI’s modelling engines typically require complete datasets for calculations, and thus dfs0 files, which are often used as inputs, should not contain missing values. For example, a rainfall boundary condition cannot have the value nan.\nAssume we have a DataFrame with missing values on 1993-12-06:\nShow Plotting Code\nax = df.plot()\nax.axvspan(\n    xmin=\"1993-12-06 00:00\",\n    xmax=\"1993-12-07 00:00\",\n    color='grey',\n    alpha=0.3,\n    label=\"Missing Data\"\n)\nax.legend(loc=\"upper right\")\nCount the number of missing values (e.g. nan) for each time series by summing the result of isna().\ndf.isna().sum()\n\nST 2: WL (m)    48\ndtype: int64",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "module2_time_series/06_data_cleaning.html#missing-values",
    "href": "module2_time_series/06_data_cleaning.html#missing-values",
    "title": "14  Data Cleaning",
    "section": "",
    "text": "Note\n\n\n\nMissing numerical data is typically represented by nan. These arise from various sources, such as sensor malfunctions during data collection, gaps that occur during data transmission, or they might be the result of previous data processing or cleaning steps.",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "module2_time_series/06_data_cleaning.html#imputation",
    "href": "module2_time_series/06_data_cleaning.html#imputation",
    "title": "14  Data Cleaning",
    "section": "14.2 Imputation",
    "text": "14.2 Imputation\nThe process of filling missing values is known as imputation.\nFor missing values between valid data points (i.e. bounded), using the .interpolate() method is a common and effective approach.\n\ndf_interpolated = df.interpolate(method='time')\n\n\n\nShow Plotting Code\nax = df.plot()\nax.axvspan(\n    xmin=\"1993-12-06 00:00\",\n    xmax=\"1993-12-07 00:00\",\n    color='grey',\n    alpha=0.3,\n    label=\"Missing Data\"\n)\ndf_interpolated.columns = [\"Interpolation\"]\ndf_interpolated.loc[\"1993-12-06\"].plot(ax=ax)\nax.legend(loc=\"upper right\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe example above uses method='time', which is a linear interpolation that considers non-equidistant DatetimeIndex indices. Refer to Pandas’s documentation for additional interpolation methods, such as polynomial.\n\n\nFor missing values appearing at the very beginning or end of your dataset (i.e. unbounded), you can make use of:\n\n.fillna()\n.ffill()\n.bfill()\n\n\n\n\n\n\n\nTip\n\n\n\nRecall: these imputation methods were introduced in the section on resampling, where upsampling introduced nan values.\n\n\nSame example as above, but using ffill().\n\ndf_interpolated = df.ffill()\n\n\n\nShow Plotting Code\nax = df.plot()\nax.axvspan(\n    xmin=\"1993-12-06 00:00\",\n    xmax=\"1993-12-07 00:00\",\n    color='grey',\n    alpha=0.3,\n    label=\"Missing Data\"\n)\ndf_interpolated.columns = [\"Interpolation\"]\ndf_interpolated.loc[\"1993-12-06\"].plot(ax=ax)\nax.legend(loc=\"upper right\")\n\n\n\n\n\n\n\n\n\nSame example as above, but using bfill().\n\ndf_interpolated = df.bfill()\n\n\n\nShow Plotting Code\nax = df.plot()\nax.axvspan(\n    xmin=\"1993-12-06 00:00\",\n    xmax=\"1993-12-07 00:00\",\n    color='grey',\n    alpha=0.3,\n    label=\"Missing Data\"\n)\ndf_interpolated.columns = [\"Interpolation\"]\ndf_interpolated.loc[\"1993-12-06\"].plot(ax=ax)\nax.legend(loc=\"upper right\")\n\n\n\n\n\n\n\n\n\nSame example as above, but using fillna().\n\ndf_interpolated = df.fillna(0.1) # specify the value to fill with\n\n\n\nShow Plotting Code\nax = df.plot()\nax.axvspan(\n    xmin=\"1993-12-06 00:00\",\n    xmax=\"1993-12-07 00:00\",\n    color='grey',\n    alpha=0.3,\n    label=\"Missing Data\"\n)\ndf_interpolated.columns = [\"Interpolation\"]\ndf_interpolated.loc[\"1993-12-06\"].plot(ax=ax)\nax.legend(loc=\"upper right\")",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "module2_time_series/06_data_cleaning.html#anomaly-detection-rule-based",
    "href": "module2_time_series/06_data_cleaning.html#anomaly-detection-rule-based",
    "title": "14  Data Cleaning",
    "section": "14.3 Anomaly Detection (Rule-Based)",
    "text": "14.3 Anomaly Detection (Rule-Based)\n\n\n\n\n\n\nTip\n\n\n\nShort on time? This section provides an introduction to a useful package but can be considered optional for core module understanding.\n\n\nBeyond clearly missing values, time series data can also contain anomalies. Identifying and addressing these anomalies is crucial for building robust MIKE+ models.\nAnomaly detection is a broad and complex field. This section offers a basic introduction to rule-based anomaly detection using DHI’s tsod Python package.\n\n14.3.1 Install tsod\nuv pip install tsod\n\n\n14.3.2 The Detector Concept\ntsod operates using a concept called “detectors.” Each detector is designed to implement a specific rule or heuristic to identify anomalies. Example anomaly detectors:\n\nRangeDetector: Flags values outside a set range.\nConstantValueDetector: Detects unchanging values over time.\nDiffDetector: Catches large changes between points.\nRollingStdDetector: Finds points far from rolling standard deviation.\n\nThere’s also the CombinedDetector, which allows combining the rules of several detectors.\n\n\n14.3.3 Detecting Anomalies\nPlot the initial time series.\n\nts = df[\"ST 2: WL (m)\"]\nts.plot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\ntsod operates on Series. Select the subject Series from the DataFrame object as needed.\n\n\nSelect and instantiate a detector. If we know water levels must be in the range -0.4m to 0.4m, then a RangeDetector should be used.\n\nfrom tsod.detectors import RangeDetector\n\ndetector = RangeDetector(\n    min_value = -0.4,\n    max_value = 0.4\n)\ndetector\n\nRangeDetector(min: -4.0e-01, max: 4.0e-01)\n\n\nDetect anomalies for a given Series using the detect() method of the instantiated detector.\n\nanomaly_mask = detector.detect(ts)\nanomaly_mask.head()\n\n1993-12-02 00:00:00    False\n1993-12-02 00:30:00    False\n1993-12-02 01:00:00    False\n1993-12-02 01:30:00    False\n1993-12-02 02:00:00    False\nFreq: 30min, Name: ST 2: WL (m), dtype: bool\n\n\n\n\n\n\n\n\nNote\n\n\n\nA mask refers to a boolean indexer. In the example above, values are true for anomalies and false otherwise.\n\n\nPlot the detected anomalies.\n\nax = ts.plot()\nts[anomaly_mask].plot(\n    ax=ax,\n    style='ro',\n    label=\"Anomaly\",\n    alpha=0.5\n)\nax.legend()\n\n# horizontal lines to validate ranges\nax.axhline(0.4, color='grey', alpha=0.5)\nax.axhline(-0.4, color='grey', alpha=0.5)\n\n\n\n\n\n\n\n\nReplace anomalies with nan.\n\nimport numpy as np\n\nts_cleaned = ts.copy()\nts_cleaned[anomaly_mask] = np.nan\nts_cleaned.plot()\n\n\n\n\n\n\n\n\n\n\n14.3.4 Impute anomalies\nImpute anomalies by treating them just like missing values.\n\nts_cleaned = ts_cleaned.interpolate(method='time')\nts_cleaned.plot()",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "module2_time_series/07_writing_dfs0.html",
    "href": "module2_time_series/07_writing_dfs0.html",
    "title": "15  Writing dfs0",
    "section": "",
    "text": "15.1 Workflow\nCreating dfs0 files is a common need for MIKE+ modellers (e.g. rainfall from csv). This section focuses on how to create dfs0 files from a Pandas DataFrame.\nThe general workflow for creating dfs0 files from a DataFrame is as follows:",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Writing dfs0</span>"
    ]
  },
  {
    "objectID": "module2_time_series/07_writing_dfs0.html#workflow",
    "href": "module2_time_series/07_writing_dfs0.html#workflow",
    "title": "15  Writing dfs0",
    "section": "",
    "text": "Map ItemInfo objects to each column (optional)\nCreate a Dataset object from the DataFrame\nSave the Dataset object to a dfs0 file.",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Writing dfs0</span>"
    ]
  },
  {
    "objectID": "module2_time_series/07_writing_dfs0.html#example-source-data",
    "href": "module2_time_series/07_writing_dfs0.html#example-source-data",
    "title": "15  Writing dfs0",
    "section": "15.2 Example Source Data",
    "text": "15.2 Example Source Data\nThe following DataFrame will be used in this section as an example.\n\nimport pandas as pd\n\ndf = pd.read_csv(\n    \"data/rain_events_2021_july.csv\",\n    index_col=\"time\",\n    parse_dates=True\n)\ndf.head()\n\n\n\n\n\n\n\n\nrainfall\n\n\ntime\n\n\n\n\n\n2021-07-02 09:51:00\n0.000\n\n\n2021-07-02 09:52:00\n3.333\n\n\n2021-07-02 09:53:00\n0.333\n\n\n2021-07-02 09:54:00\n0.333\n\n\n2021-07-02 09:55:00\n0.333\n\n\n\n\n\n\n\nGet familiar with the data. Notice:\n\nThe time axis is non-equidistant\nValues represent rainfall depth since the last time step.\nRainfall events always start at values of zero.\n\n\ndf.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nrainfall\n1086.0\n1.120283\n3.219804\n0.0\n0.185\n0.37\n0.667\n36.667\n\n\n\n\n\n\n\n\ndf.plot()\n\n\n\n\n\n\n\n\n\ndf.plot.hist(bins=50)\n\n\n\n\n\n\n\n\nThis subset shows the division between two rainfall events:\n\ndf.loc[\"2021-07-25 17:32:00\":\"2021-07-25 20:30:00\"]\n\n\n\n\n\n\n\n\nrainfall\n\n\ntime\n\n\n\n\n\n2021-07-25 17:32:00\n0.476\n\n\n2021-07-25 17:33:00\n0.476\n\n\n2021-07-25 17:34:00\n0.476\n\n\n2021-07-25 20:28:00\n0.000\n\n\n2021-07-25 20:29:00\n3.333\n\n\n2021-07-25 20:30:00\n0.667",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Writing dfs0</span>"
    ]
  },
  {
    "objectID": "module2_time_series/07_writing_dfs0.html#dataframe-to-dataset",
    "href": "module2_time_series/07_writing_dfs0.html#dataframe-to-dataset",
    "title": "15  Writing dfs0",
    "section": "15.3 DataFrame to Dataset",
    "text": "15.3 DataFrame to Dataset\nConverting a DataFrame to a Dataset is straightforward using mikeio.from_pandas():\n\nds = mikeio.from_pandas(df)\nds\n\n&lt;mikeio.Dataset&gt;\ndims: (time:1086)\ntime: 2021-07-02 09:51:00 - 2021-07-31 13:16:00 (1086 non-equidistant records)\ngeometry: GeometryUndefined()\nitems:\n  0:  rainfall &lt;Undefined&gt; (undefined)\n\n\n\n\n\n\n\n\nTip\n\n\n\nEnsure your DataFrame’s index is a DatetimeIndex for time series dfs0 files. This is crucial for MIKE IO to correctly interpret the time information.\n\n\nNotice that the item type and unit are “undefined”. Let’s inspect the ItemInfo MIKE IO used by default:\n\nitem = ds[0].item\nprint(f\"Item Name: {item.name}\")\nprint(f\"Item Type: {item.type.name}\")\nprint(f\"Item Unit: {item.unit.name}\")\nprint(f\"Item Data Value Type: {item.data_value_type.name}\")\n\nItem Name: rainfall\nItem Type: Undefined\nItem Unit: undefined\nItem Data Value Type: Instantaneous\n\n\nThis highlights the need to almost always define item metadata before calling from_pandas().\n\n\n\n\n\n\nCaution\n\n\n\nProviding accurate ItemInfo is key for ensuring compatibility with MIKE software and correctly interpreting the meaning of your data within the MIKE ecosystem.",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Writing dfs0</span>"
    ]
  },
  {
    "objectID": "module2_time_series/07_writing_dfs0.html#item-metadata",
    "href": "module2_time_series/07_writing_dfs0.html#item-metadata",
    "title": "15  Writing dfs0",
    "section": "15.4 Item Metadata",
    "text": "15.4 Item Metadata\nPlease review MIKE IO’s user guide on EUM before proceeding.\nCreate an ItemInfo object for our example rainfall data:\n\nitem = mikeio.ItemInfo(\n    name = \"Rainfall\",\n    itemtype = mikeio.EUMType.Rainfall_Depth,\n    unit = mikeio.EUMUnit.millimeter,\n    data_value_type= \"StepAccumulated\",\n)\nitem\n\nRainfall &lt;Rainfall Depth&gt; (millimeter) - 2\n\n\n\n\n\n\n\n\nData Value Type\n\n\n\n\n\nThe Data Value Type specifies how data values relate to time steps. Common options include:\n\nInstantaneous: Value at a specific point in time.\nAccumulated: Value aggregated over the entire period up to the timestamp.\nStepAccumulated: Value aggregated over the preceding time interval.\nMeanStepBackward: Average value over the preceding time interval.\n\nRefer to MIKE+ documentation for explanation of these options.\n\n\n\nLet’s recreate the Dataset using the ItemInfo object for our rainfall.\n\nds = mikeio.from_pandas(df, items=[item])\nds\n\n&lt;mikeio.Dataset&gt;\ndims: (time:1086)\ntime: 2021-07-02 09:51:00 - 2021-07-31 13:16:00 (1086 non-equidistant records)\ngeometry: GeometryUndefined()\nitems:\n  0:  rainfall &lt;Rainfall Depth&gt; (millimeter) - 2\n\n\nNotice the item info is now correct on the Dataset.\n\n\n\n\n\n\nMapping items to column by name\n\n\n\n\n\nThe order of items matches the order of the DataFrame columns. You may prefer to explicitly name the columns:\n\nmikeio.from_pandas(df, items={\n    \"rainfall\" : item\n})\n\n&lt;mikeio.Dataset&gt;\ndims: (time:1086)\ntime: 2021-07-02 09:51:00 - 2021-07-31 13:16:00 (1086 non-equidistant records)\ngeometry: GeometryUndefined()\nitems:\n  0:  rainfall &lt;Rainfall Depth&gt; (millimeter) - 2",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Writing dfs0</span>"
    ]
  },
  {
    "objectID": "module2_time_series/07_writing_dfs0.html#dataset-to-dfs0",
    "href": "module2_time_series/07_writing_dfs0.html#dataset-to-dfs0",
    "title": "15  Writing dfs0",
    "section": "15.5 Dataset to dfs0",
    "text": "15.5 Dataset to dfs0\nThe final step is to save your carefully prepared Dataset object, now containing the correct data and metadata, to a dfs0 file. This is done using the .to_dfs() method of the Dataset object.\n\nds.to_dfs(\"rainfall.dfs0\")\n\nThis will create a file named rainfall.dfs0 ready to be used in MIKE+.\nConfirm it worked by reading the dfs0 file back into a Dataset (optional).\n\nds_validation = mikeio.read(\"rainfall.dfs0\")\nds_validation\n\n&lt;mikeio.Dataset&gt;\ndims: (time:1086)\ntime: 2021-07-02 09:51:00 - 2021-07-31 13:16:00 (1086 non-equidistant records)\ngeometry: GeometryUndefined()\nitems:\n  0:  rainfall &lt;Rainfall Depth&gt; (millimeter) - 2",
    "crumbs": [
      "Module 2 - Time Series",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Writing dfs0</span>"
    ]
  },
  {
    "objectID": "module2_time_series/homework.html",
    "href": "module2_time_series/homework.html",
    "title": "Homework",
    "section": "",
    "text": "Exercise 1\n\nDownload this dfs0 file into a new project folder.\nCreate an empty Jupyter Notebook and import mikeio.\nRead the dfs0 file into a Dataset object.\nConvert the Dataset object into a Pandas DataFrame object.\nCall the describe() method and review the statistics.\nIn the same notebook, select a subset of the items.\nExport the subset DataFrame to csv with Pandas (hint: to_csv())\n\nExercise 2\n\nDownload this dfs0 file into a new project folder.\nCreate an empty Jupyter Notebook and import mikeio.\nRead the dfs0 file into a Dataset, only including data between “1993-12-02 16:00” and “1993-12-02 20:00”.\nConvert the Dataset object into a Pandas DataFrame object.\nPlot the DataFrame using .plot().\nSelect the first 3 rows of the DataFrame in two different ways: using iloc and using loc.\n\nExercise 3\n\nRepeat steps 1-2 of the previous exercise.\nRead the dfs0 file into a Dataset object, then convert it to a DataFrame.\nResample the half-hourly data to minutely data (i.e. upsample) using time interpolation.\nResample the half-hourly data to hourly data (i.e downsample) using mean aggregation.\nTry 3-4 again, except choose a different fill/aggregation method. Compare the results.\n\nExercise 4\n\nDownload this dfs0 file into a new project folder.\nCreate an empty Jupyter Notebook and import mikeio.\nRead the dfs0 file into a Dataset object, then convert it to a DataFrame.\nCompare the observed and model values using a line plot, a scatter plot, and a histogram.\nSave the plots to a png file.\n\nExercise 5\n\nDownload this csv file into a new project folder.\nRead the csv file into a DataFrame using Pandas.\nCheck for nan values in the rainfall. How many missing values are there?\nFill the missing value(s) using an appropriate imputation method.\n\nExercise 6\n\nContinue from where you left off in the previous exercise.\nCreate an ItemInfo object for the rainfall data.\nCreate a Dataset object from the DataFrame. Ensure its item metadata is correct.\nSave the Dataset object to a dfs0 file.\nOpen the dfs0 file in MIKE+. Does it make sense?",
    "crumbs": [
      "Module 2 - Time Series",
      "Homework"
    ]
  },
  {
    "objectID": "module3_network_results/index.html",
    "href": "module3_network_results/index.html",
    "title": "Welcome to Module 3!",
    "section": "",
    "text": "This module dives into the specifics of handling 1D network model results, a common task for MIKE+ users working with collection systems, water distribution networks, or river models. Our focus is on equipping you to efficiently access, analyze, and prepare 1D simulation outputs using Python:\n\nOpen and read network result files (e.g., .res1d, .res, .resx).\nExplore and navigate the network structure (nodes, reaches, catchments, gridpoints).\nSelect and subset specific data by location, quantity, and time.\nConvert network data to Pandas DataFrames for further analysis and visualization.\nExport selected network data to common formats like dfs0 or csv.\n\nYou’ll be introduced to a powerful new library: MIKE IO 1D. Let’s begin!\n\n\n\n\n\n\nWhere can I download sample data to follow along?\n\n\n\n\n\nAll of the sample data used in this module is available for download:\n\nnetwork.res1d\ncatchments.res1d\nflow_meter_data.csv",
    "crumbs": [
      "Module 3 - Network Results",
      "Welcome to Module 3!"
    ]
  },
  {
    "objectID": "module3_network_results/01_overview_mikeio1d.html",
    "href": "module3_network_results/01_overview_mikeio1d.html",
    "title": "16  MIKE IO 1D",
    "section": "",
    "text": "16.1 What is MIKE IO 1D?\nThis section introduces MIKE IO 1D, a specialized Python package for working with 1D network result files from MIKE+ models.\nMIKE IO 1D is an open-source Python package developed by DHI, forming an integral part of DHI’s Python ecosystem. Similar to MIKE IO, it empowers modelers with full flexibility by bridging the gap between various MIKE 1D file formats and Scientific Python’s rich and powerful package ecosystem.",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>MIKE IO 1D</span>"
    ]
  },
  {
    "objectID": "module3_network_results/01_overview_mikeio1d.html#mike-io-vs-mike-io-1d",
    "href": "module3_network_results/01_overview_mikeio1d.html#mike-io-vs-mike-io-1d",
    "title": "16  MIKE IO 1D",
    "section": "16.2 MIKE IO vs MIKE IO 1D",
    "text": "16.2 MIKE IO vs MIKE IO 1D\nMIKE IO 1D is specifically tailored for interacting with MIKE 1D modelling files, such as network result files (e.g., .res1d) and cross-section files (e.g., .xns11). MIKE IO, on the other hand, primarily handles n-dimensional data files (e.g., .dfs0, .dfs2, .dfsu) for gridded or mesh-based data. MIKE+ modellers will often require both packages to support their workflows.\n\n\n\n\n\n\nHistory of MIKE IO 1D\n\n\n\n\n\nMIKE IO 1D was historically part of MIKE IO. It was split into a separate library due to fundamental differences in 1D functionality and the then-objectives of MIKE IO.",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>MIKE IO 1D</span>"
    ]
  },
  {
    "objectID": "module3_network_results/01_overview_mikeio1d.html#usage-in-course",
    "href": "module3_network_results/01_overview_mikeio1d.html#usage-in-course",
    "title": "16  MIKE IO 1D",
    "section": "16.3 Usage in course",
    "text": "16.3 Usage in course\nMost MIKE+ models (collection system, water distribution, rivers) require handling network result files (e.g. res1d, res, resx). We introduce a subset of MIKE IO 1D’s features that are useful for such workflows.\nWhile MIKE IO 1D includes other features, like cross-sectional data, this module excludes them to keep the content manageable for beginners. We encourage you to explore MIKE IO 1D’s documentation and examples of these features when you’re ready.",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>MIKE IO 1D</span>"
    ]
  },
  {
    "objectID": "module3_network_results/01_overview_mikeio1d.html#installation",
    "href": "module3_network_results/01_overview_mikeio1d.html#installation",
    "title": "16  MIKE IO 1D",
    "section": "16.4 Installation",
    "text": "16.4 Installation\nInstall MIKE IO 1D with:\nuv pip install mikeio1d\n\n\n\n\n\n\nTip\n\n\n\nAlways check the official MIKE IO 1D’s documentation for the latest installation instructions.",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>MIKE IO 1D</span>"
    ]
  },
  {
    "objectID": "module3_network_results/01_overview_mikeio1d.html#network-structure",
    "href": "module3_network_results/01_overview_mikeio1d.html#network-structure",
    "title": "16  MIKE IO 1D",
    "section": "16.5 Network Structure",
    "text": "16.5 Network Structure\nUnderstanding MIKE 1D’s network structure (e.g., nodes, reaches, catchments, gridpoints) is crucial when you work with MIKE IO 1D. You may already have a solid understanding of these concepts as a MIKE+ modeller. Please refer to MIKE IO 1D’s documentation for a brief refresher on the topic. For detailed information, refer to MIKE+’s documentation.",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>MIKE IO 1D</span>"
    ]
  },
  {
    "objectID": "module3_network_results/01_overview_mikeio1d.html#quick-glance",
    "href": "module3_network_results/01_overview_mikeio1d.html#quick-glance",
    "title": "16  MIKE IO 1D",
    "section": "16.6 Quick glance",
    "text": "16.6 Quick glance\nLet’s take a quick look at some core MIKE IO 1D objects and how to access them. When you open a network result file (e.g. res1d, res), MIKE IO 1D returns a Res1D object.\n\nimport mikeio1d\n\nres = mikeio1d.open(\"data/network.res1d\") \n\n\n\n\n\n\n\nAll network result file types (res1d, res, etc.) return a Res1D object on open()\n\n\n\n\n\nMIKE IO 1D opens all result file types with open(), returning a Res1D object regardless of the initial file extension. Refer to MIKE IO 1D’s documentation on this for details.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe Res1D object is the core container for network results, analogous to the Dataset object in MIKE IO.\n\n\nAll results within a Res1D object share a common time axis, much like data items in a .dfs0 file.\n\nres.time_index\n\nMIKE IO 1D provides access to standard result quantities (e.g., Water Level, Discharge).\n\nres.quantities\n\nAdditionally, it includes derived quantities (e.g., Water Depth) that are calculated on-the-fly, similar to MIKE+.\n\nres.derived_quantities\n\n\n\n\n\n\n\nNote\n\n\n\nDerived quantities are not stored in the result file, thus they always require calculation at runtime (even by MIKE+).\n\n\nYou can convert quantities at network locations into Pandas DataFrames.\n\nres.nodes[\"101\"].WaterLevel.to_dataframe()",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>MIKE IO 1D</span>"
    ]
  },
  {
    "objectID": "module3_network_results/01_overview_mikeio1d.html#key-concepts",
    "href": "module3_network_results/01_overview_mikeio1d.html#key-concepts",
    "title": "16  MIKE IO 1D",
    "section": "16.7 Key Concepts",
    "text": "16.7 Key Concepts\nUnderstanding these core concepts will help you navigate MIKE IO 1D:\n\nRes1D: This is the primary object representing the contents of a network result file (e.g., .res1d). It acts as the main entry point to all data.\nLocations: Results are associated with specific locations within the network model, such as nodes, reaches, or specific grid points along a reach.\nQuantities: Time series data with a concrete type, like ‘WaterLevel’ or ‘Discharge’, at a specific network location.\n\nImagine a hierarchy: a Res1D object contains various Locations (nodes, reaches, catchments). Each Location can have multiple Quantities available as time series data.\n\n\n\n“MIKE IO 1D Data Structure”\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis is a simplified diagram that excludes some details covered later in the module. For example, the hierarchical group of reaches could be further divided into gridpoints along each reach.",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>MIKE IO 1D</span>"
    ]
  },
  {
    "objectID": "module3_network_results/01_overview_mikeio1d.html#additional-reading-optional",
    "href": "module3_network_results/01_overview_mikeio1d.html#additional-reading-optional",
    "title": "16  MIKE IO 1D",
    "section": "16.8 Additional reading (optional)",
    "text": "16.8 Additional reading (optional)\nThe following are useful resources for learning MIKE IO 1D:\n\nMIKE IO 1D’s official documentation\nMIKE IO 1D Example Notebooks\nMIKE IO 1D’s GitHub repository",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>MIKE IO 1D</span>"
    ]
  },
  {
    "objectID": "module3_network_results/02_reading_network_files.html",
    "href": "module3_network_results/02_reading_network_files.html",
    "title": "17  Reading res1d",
    "section": "",
    "text": "17.1 Workflow\nThis section guides you through loading network results (e.g., from res1d files) into Pandas DataFrames. The approach is similar to that used with MIKE IO, allowing you to apply your DataFrame knowledge to network results.\nThe general workflow for network results in MIKE IO 1D is as follows:",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Reading res1d</span>"
    ]
  },
  {
    "objectID": "module3_network_results/02_reading_network_files.html#workflow",
    "href": "module3_network_results/02_reading_network_files.html#workflow",
    "title": "17  Reading res1d",
    "section": "",
    "text": "Open the network result file to obtain a Res1D object.\nOptionally, subset this Res1D object to select specific locations, quantities, or time steps.\nConvert the (subsetted) data to a DataFrame.\nPerform some additional analysis via the DataFrame.\n\n\n\n\n\n\n\nTip\n\n\n\nThis workflow is quite similar to the one you learned in Module 2 for handling dfs0 files with MIKE IO. The core idea of opening a file, accessing data, and then often converting to a DataFrame remains consistent.",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Reading res1d</span>"
    ]
  },
  {
    "objectID": "module3_network_results/02_reading_network_files.html#res1d",
    "href": "module3_network_results/02_reading_network_files.html#res1d",
    "title": "17  Reading res1d",
    "section": "17.2 Res1D",
    "text": "17.2 Res1D\nThe Res1D object is central to interacting with 1D network result files. It acts as the primary container for your simulation data, analogous to MIKE IO’s Dataset object for other DHI file types.\nTo start, you open your result file using mikeio1d.open():\n\nimport mikeio1d\n\nres = mikeio1d.open(\"data/network.res1d\")\nres\n\n&lt;mikeio1d.Res1D&gt;\n\n\n\n\n\n\n\n\nNote\n\n\n\nMIKE IO uses read() to create a Dataset, whereas MIKE IO 1D uses open() to create a Res1D object.\n\n\nGet an overview of key meta data with info().\n\nres.info()\n\nStart time: 1994-08-07 16:35:00\nEnd time: 1994-08-07 18:35:00\n# Timesteps: 110\n# Catchments: 0\n# Nodes: 119\n# Reaches: 118\n# Globals: 0\n0 - Water level (m)\n1 - Discharge (m^3/s)\n\n\nNotice the this produces similar information to that of a MIKE IO Dataset:\n\nTotal number of time steps\nTimestamps for first and last time step\nAll available quantities\n\nLike dfs0 files, all results within a Res1D object share a common time axis.",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Reading res1d</span>"
    ]
  },
  {
    "objectID": "module3_network_results/02_reading_network_files.html#convert-to-dataframe",
    "href": "module3_network_results/02_reading_network_files.html#convert-to-dataframe",
    "title": "17  Reading res1d",
    "section": "17.3 Convert to DataFrame",
    "text": "17.3 Convert to DataFrame\nThe read() method is the primary way to convert data from a Res1D object (or its subsets) into a Pandas DataFrame. This is a versatile method that can be called at various levels.\nYou can convert the entire content of the Res1D object into a single DataFrame. Each quantity at a specific location becomes a column in the DataFrame.\n\ndf = res.read()\ndf.head()\n\n\n\n\n\n\n\n\nWaterLevel:1\nWaterLevel:2\nWaterLevel:3\nWaterLevel:4\nWaterLevel:5\nWaterLevel:6\nWaterLevel:7\nWaterLevel:8\nWaterLevel:9\nWaterLevel:10\n...\nDischarge:99l1:22.2508\nWaterLevel:9l1:0\nWaterLevel:9l1:10\nDischarge:9l1:5\nWaterLevel:Weir:119w1:0\nWaterLevel:Weir:119w1:1\nDischarge:Weir:119w1:0.5\nWaterLevel:Pump:115p1:0\nWaterLevel:Pump:115p1:82.4281\nDischarge:Pump:115p1:41.214\n\n\n\n\n1994-08-07 16:35:00.000\n195.052994\n195.821503\n195.8815\n193.604996\n193.615005\n193.625000\n193.675003\n193.764999\n193.774994\n193.804993\n...\n0.000002\n193.774994\n193.764999\n0.000031\n193.550003\n188.479996\n0.0\n193.304993\n195.005005\n0.0\n\n\n1994-08-07 16:36:01.870\n195.052994\n195.821701\n195.8815\n193.604996\n193.615005\n193.625320\n193.675110\n193.765060\n193.775116\n193.804993\n...\n0.000002\n193.775070\n193.765060\n0.000031\n193.550003\n188.479996\n0.0\n193.306061\n195.005005\n0.0\n\n\n1994-08-07 16:37:07.560\n195.052994\n195.821640\n195.8815\n193.604996\n193.615005\n193.625671\n193.675369\n193.765106\n193.775513\n193.804993\n...\n0.000002\n193.775391\n193.765106\n0.000033\n193.550034\n188.479996\n0.0\n193.307144\n195.005005\n0.0\n\n\n1994-08-07 16:38:55.828\n195.052994\n195.821503\n195.8815\n193.604996\n193.615005\n193.626236\n193.675751\n193.765228\n193.776077\n193.804993\n...\n0.000002\n193.775894\n193.765228\n0.000037\n193.550079\n188.479996\n0.0\n193.308884\n195.005005\n0.0\n\n\n1994-08-07 16:39:55.828\n195.052994\n195.821503\n195.8815\n193.604996\n193.615005\n193.626556\n193.675949\n193.765335\n193.776352\n193.804993\n...\n0.000002\n193.776154\n193.765335\n0.000039\n193.550095\n188.479996\n0.0\n193.309860\n195.005005\n0.0\n\n\n\n\n5 rows × 495 columns\n\n\n\nThe DataFrame above includes all reaches and nodes, which results in many columns. To create more manageable DataFrames, call read() on data subsets (e.g., by location or quantity).\nFor example, all results for the location group reaches:\n\ndf_reaches = res.reaches.read()\ndf_reaches.head()\n\n\n\n\n\n\n\n\nWaterLevel:100l1:0\nWaterLevel:100l1:47.6827\nWaterLevel:101l1:0\nWaterLevel:101l1:66.4361\nWaterLevel:102l1:0\nWaterLevel:102l1:10.9366\nWaterLevel:103l1:0\nWaterLevel:103l1:26.0653\nWaterLevel:104l1:0\nWaterLevel:104l1:34.4131\n...\nDischarge:93l1:24.5832\nDischarge:94l1:21.2852\nDischarge:95l1:21.9487\nDischarge:96l1:14.9257\nDischarge:97l1:5.71207\nDischarge:98l1:8.00489\nDischarge:99l1:22.2508\nDischarge:9l1:5\nDischarge:Weir:119w1:0.5\nDischarge:Pump:115p1:41.214\n\n\n\n\n1994-08-07 16:35:00.000\n195.441498\n194.661499\n195.931503\n195.441498\n193.550003\n193.550003\n195.801498\n195.701508\n197.072006\n196.962006\n...\n0.000004\n0.000003\n0.000001\n0.000005\n0.000013\n0.000003\n0.000002\n0.000031\n0.0\n0.0\n\n\n1994-08-07 16:36:01.870\n195.441498\n194.661621\n195.931503\n195.441605\n193.550140\n193.550064\n195.801498\n195.703171\n197.072006\n196.962051\n...\n0.000004\n0.000003\n0.000001\n0.000005\n0.000010\n0.000003\n0.000002\n0.000031\n0.0\n0.0\n\n\n1994-08-07 16:37:07.560\n195.441498\n194.661728\n195.931503\n195.441620\n193.550232\n193.550156\n195.801498\n195.703400\n197.072006\n196.962082\n...\n0.000004\n0.000003\n0.000001\n0.000005\n0.000010\n0.000003\n0.000002\n0.000033\n0.0\n0.0\n\n\n1994-08-07 16:38:55.828\n195.441498\n194.661804\n195.931503\n195.441605\n193.550369\n193.550308\n195.801498\n195.703690\n197.072006\n196.962112\n...\n0.000004\n0.000003\n0.000001\n0.000005\n0.000009\n0.000003\n0.000002\n0.000037\n0.0\n0.0\n\n\n1994-08-07 16:39:55.828\n195.441498\n194.661972\n195.931503\n195.441605\n193.550430\n193.550369\n195.801498\n195.703827\n197.072006\n196.962128\n...\n0.000004\n0.000003\n0.000001\n0.000005\n0.000009\n0.000003\n0.000002\n0.000039\n0.0\n0.0\n\n\n\n\n5 rows × 376 columns\n\n\n\nOr results for a reach named 100l1:\n\ndf_reach_100l1 = res.reaches[\"100l1\"].read()\ndf_reach_100l1.head()\n\n\n\n\n\n\n\n\nWaterLevel:100l1:0\nWaterLevel:100l1:47.6827\nDischarge:100l1:23.8414\n\n\n\n\n1994-08-07 16:35:00.000\n195.441498\n194.661499\n0.000006\n\n\n1994-08-07 16:36:01.870\n195.441498\n194.661621\n0.000006\n\n\n1994-08-07 16:37:07.560\n195.441498\n194.661728\n0.000006\n\n\n1994-08-07 16:38:55.828\n195.441498\n194.661804\n0.000006\n\n\n1994-08-07 16:39:55.828\n195.441498\n194.661972\n0.000006\n\n\n\n\n\n\n\nOr just the Discharge quantity of the previous reach.\n\ndf_reach_100l1_q = res.reaches[\"100l1\"].Discharge.read()\ndf_reach_100l1_q.head()\n\n\n\n\n\n\n\n\nDischarge:100l1:23.8414\n\n\n\n\n1994-08-07 16:35:00.000\n0.000006\n\n\n1994-08-07 16:36:01.870\n0.000006\n\n\n1994-08-07 16:37:07.560\n0.000006\n\n\n1994-08-07 16:38:55.828\n0.000006\n\n\n1994-08-07 16:39:55.828\n0.000006\n\n\n\n\n\n\n\nIn the coming sections, we will cover how to explore the network structure of Res1D and select data. For now, just know that it’s possible to call read() from various sub-objects of Res1D to obtain a DataFrame.\n\n\n\n\n\n\nTip\n\n\n\nOnce you have your network data in a Pandas DataFrame using .read(), you can apply all the powerful analysis, manipulation, and visualization techniques you learned in Module 1 and Module 2.",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Reading res1d</span>"
    ]
  },
  {
    "objectID": "module3_network_results/02_reading_network_files.html#plotting-and-other-formats",
    "href": "module3_network_results/02_reading_network_files.html#plotting-and-other-formats",
    "title": "17  Reading res1d",
    "section": "17.4 Plotting and other formats",
    "text": "17.4 Plotting and other formats\nFor convenience, a plot() method is available anywhere that read() can be called. This allows for quick visualization of the data.\n\nres.reaches[\"100l1\"].Discharge.plot()\n\n\n\n\n\n\n\n\nSimilarly, wherever read() is available, you can export data to other common formats. For example:\n\nres.reaches[\"100l1\"].Discharge.to_dfs0(\"discharge_of_interest.dfs0\")\n\n\nres.reaches[\"100l1\"].Discharge.to_csv(\"discharge_of_interest.csv\")",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Reading res1d</span>"
    ]
  },
  {
    "objectID": "module3_network_results/02_reading_network_files.html#practical-example",
    "href": "module3_network_results/02_reading_network_files.html#practical-example",
    "title": "17  Reading res1d",
    "section": "17.5 Practical Example",
    "text": "17.5 Practical Example\nLet’s combine some of these concepts to accomplish the following objectives:\n\nGet descriptive statistics of all node water levels\nGet descriptive statistics of all reach discharges\n\nFirst, open the result file.\n\nimport mikeio1d\n\nres = mikeio1d.open(\"data/network.res1d\")\n\nThen, get our node water levels and reach discharges into DataFrames.\n\ndf_nodes_wl = res.nodes.WaterLevel.read()\ndf_reaches_q = res.reaches.Discharge.read()\n\nUse describe() on each DataFrame, just like from previous modules.\n\nwl_stats = df_nodes_wl.describe().T\nwl_stats.head()\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nWaterLevel:1\n110.0\n195.198898\n0.159523\n195.052994\n195.090881\n195.131989\n195.258179\n195.669006\n\n\nWaterLevel:2\n110.0\n195.822342\n0.000657\n195.821503\n195.821503\n195.822784\n195.822937\n195.822968\n\n\nWaterLevel:3\n110.0\n195.881470\n0.000000\n195.881500\n195.881500\n195.881500\n195.881500\n195.881500\n\n\nWaterLevel:4\n110.0\n193.947418\n0.348278\n193.604996\n193.614719\n193.891739\n194.173325\n194.661331\n\n\nWaterLevel:5\n110.0\n194.010544\n0.379473\n193.615005\n193.659035\n193.940742\n194.260757\n194.793060\n\n\n\n\n\n\n\n\nq_stats = df_reaches_q.describe().T\nq_stats.head()\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nDischarge:100l1:23.8414\n110.0\n0.014078\n0.026875\n0.000006\n0.000636\n0.001032\n0.005988\n0.099751\n\n\nDischarge:101l1:33.218\n110.0\n-0.000034\n0.005649\n-0.022655\n0.000004\n0.000005\n0.000022\n0.019202\n\n\nDischarge:102l1:5.46832\n110.0\n0.069058\n0.100331\n-0.011316\n0.003062\n0.017987\n0.096062\n0.326383\n\n\nDischarge:103l1:13.0327\n110.0\n-0.000011\n0.001084\n-0.006748\n0.000002\n0.000017\n0.000337\n0.001056\n\n\nDischarge:104l1:17.2065\n110.0\n0.000005\n0.000002\n0.000003\n0.000005\n0.000005\n0.000005\n0.000025\n\n\n\n\n\n\n\nFrom here we might want to use existing Pandas functionality to export our data for reporting purposes:\n\nwl_stats.to_excel(\"node_water_levels.xlsx\")\nq_stats.to_excel(\"reaches_discharges.xlsx\")\n\n\n\n\n\n\n\nTip\n\n\n\nYou need to install a Python package like openpyxl to use Pandas’ to_excel() method.",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Reading res1d</span>"
    ]
  },
  {
    "objectID": "module3_network_results/03_exploring_network_and_results.html",
    "href": "module3_network_results/03_exploring_network_and_results.html",
    "title": "18  Exploring Networks",
    "section": "",
    "text": "18.1 Nodes\nThis section guides you through navigating MIKE IO 1D’s Res1D object. You’ll learn how to access nodes, catchments, reaches, and individual gridpoints, along with their associated data and properties. This exploration is key to understanding the structure and results of your MIKE+ 1D models.\nNode data is accessible via the Res1D object, typically named res in our examples. The res.nodes attribute provides access to all nodes in your result file.\nres.nodes\n\n&lt;ResultNodes&gt; (119)\n    \n    Quantities (1)Water level (m)Derived Quantities (3)NodeFloodingNodeWaterDepthNodeWaterLevelAboveCritical\nThe nodes object behaves like a Python dictionary, where node IDs are the keys and specific node objects are the values. You can access a specific node by its ID.\nres.nodes[\"1\"]\n\n&lt;Manhole: 1&gt;\n    \n    Attributes (8)id: 1type: Manholexcoord: -687934.6000976562ycoord: -1056500.69921875ground_level: 197.07000732421875bottom_level: 195.0500030517578critical_level: infdiameter: 1.0Quantities (1)Water level (m)Derived Quantities (3)NodeFloodingNodeWaterDepthNodeWaterLevelAboveCritical\nEach node object contains both dynamic quantities (time series results like water level) and static properties (like invert level).\nTo access a dynamic quantity:\nres.nodes[\"1\"].WaterLevel\n\n&lt;Quantity: Water level (m)&gt;\nAs shown in the last section, dynamic quantities can be converted to a DataFrame with read(), or plotted with plot():\nres.nodes[\"1\"].WaterLevel.plot()\nTo access a static property:\nres.nodes[\"1\"].bottom_level\n\n195.0500030517578\nYou can iterate through all nodes, similar to a Python dictionary. For example:\nfor node_id, node in res.nodes.items():\n    if node.type == \"Outlet\":\n        display(node)\n\n&lt;Outlet: 120&gt;\n    \n    Attributes (8)id: 120type: Outletxcoord: -687917.30078125ycoord: -1056709.4993896484ground_level: 194.0bottom_level: 193.4499969482422critical_level: Nonediameter: NoneQuantities (1)Water level (m)Derived Quantities (3)NodeFloodingNodeWaterDepthNodeWaterLevelAboveCritical\n\n\n&lt;Outlet: Weir Outlet:119w1&gt;\n    \n    Attributes (8)id: Weir Outlet:119w1type: Outletxcoord: -687941.8002929688ycoord: -1056652.400024414ground_level: 197.1699981689453bottom_level: 183.47999572753906critical_level: Nonediameter: NoneQuantities (1)Water level (m)Derived Quantities (3)NodeFloodingNodeWaterDepthNodeWaterLevelAboveCritical",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Exploring Networks</span>"
    ]
  },
  {
    "objectID": "module3_network_results/03_exploring_network_and_results.html#nodes",
    "href": "module3_network_results/03_exploring_network_and_results.html#nodes",
    "title": "18  Exploring Networks",
    "section": "",
    "text": "Tip\n\n\n\nNotice that displaying res.nodes shows relevant metadata. This is true for all location objects.",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Exploring Networks</span>"
    ]
  },
  {
    "objectID": "module3_network_results/03_exploring_network_and_results.html#catchments",
    "href": "module3_network_results/03_exploring_network_and_results.html#catchments",
    "title": "18  Exploring Networks",
    "section": "18.2 Catchments",
    "text": "18.2 Catchments\nYou handle catchments similarly to nodes, but they are typically in a separate result file, often from rainfall-runoff simulations.\n\nres_rr.catchments\n\n&lt;ResultCatchments&gt; (31)\n    \n    Quantities (5)Total Runoff (m^3/s)Actual Rainfall (m/s)Zink, Load, RR (kg/s)Zink, Mass, Accumulated, RR (kg)Zink, RR (mg/l)Derived Quantities (0)\n\n\n\n\n\n\n\n\nWhy are separate result files used?\n\n\n\n\n\nRecall that all quantities in a network result file share the same time axis. Hydraulic simulations require finer time steps than hydrologic simulations, and thus a different time index. This is the main reason hydrologic and hydraulic results are stored in separate files.\n\n\n\nThe catchments object also acts like a dictionary, with catchment IDs as keys.\n\nres_rr.catchments[\"100_16_16\"]\n\n&lt;Catchment: 100_16_16&gt;\n    \n    Attributes (3)id: 100_16_16area: 22800.0type: Kinematic WaveQuantities (5)Total Runoff (m^3/s)Actual Rainfall (m/s)Zink, Load, RR (kg/s)Zink, Mass, Accumulated, RR (kg)Zink, RR (mg/l)Derived Quantities (0)\n\n\nCatchment objects also have dynamic quantities and static properties. For instance, TotalRunOff is a dynamic quantity, and area is a static property.\n\nres_rr.catchments[\"100_16_16\"].TotalRunOff.plot()\n\n\n\n\n\n\n\n\n\nres_rr.catchments[\"100_16_16\"].area\n\n22800.0\n\n\nJust like nodes, you can iterate through catchments like a Python dictionary. For example:\n\nfor catchment_id, catchment in res_rr.catchments.items():\n    if \"28\" in catchment_id:\n        display(catchment)\n\n&lt;Catchment: 28_6_6&gt;\n    \n    Attributes (3)id: 28_6_6area: 3500.0type: Kinematic WaveQuantities (5)Total Runoff (m^3/s)Actual Rainfall (m/s)Zink, Load, RR (kg/s)Zink, Mass, Accumulated, RR (kg)Zink, RR (mg/l)Derived Quantities (0)\n\n\n&lt;Catchment: 90_28_28&gt;\n    \n    Attributes (3)id: 90_28_28area: 48000.0type: Kinematic WaveQuantities (5)Total Runoff (m^3/s)Actual Rainfall (m/s)Zink, Load, RR (kg/s)Zink, Mass, Accumulated, RR (kg)Zink, RR (mg/l)Derived Quantities (0)",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Exploring Networks</span>"
    ]
  },
  {
    "objectID": "module3_network_results/03_exploring_network_and_results.html#reaches",
    "href": "module3_network_results/03_exploring_network_and_results.html#reaches",
    "title": "18  Exploring Networks",
    "section": "18.3 Reaches",
    "text": "18.3 Reaches\nReaches are accessed similarly to nodes and catchments. A key difference is that individual reach objects also contain gridpoints.\n\nres.reaches\n\n&lt;ResultReaches&gt; (118)\n    \n    Quantities (2)Water level (m)Discharge (m^3/s)Derived Quantities (6)ReachAbsoluteDischargeReachFillingReachFloodingReachQQManningReachWaterDepthReachWaterLevelAboveCritical\n\n\nThe reaches object is dictionary-like, with reach names as keys and specific reach objects as values.\n\nres.reaches[\"101l1\"]\n\n&lt;Reach: 101l1&gt;\n    \n    Attributes (9)name: 101l1length: 66.4360966980845start_chainage: 0.0end_chainage: 66.4360966980845n_gridpoints: 3start_node: 101end_node: 100height: 0.30000001192092896full_flow_discharge: 0.0809713208695954Quantities (2)Water level (m)Discharge (m^3/s)Derived Quantities (6)ReachAbsoluteDischargeReachFillingReachFloodingReachQQManningReachWaterDepthReachWaterLevelAboveCritical\n\n\nEach reach object contains dynamic quantities and static properties.\n\nres.reaches[\"101l1\"].WaterLevel.plot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nNotice that two water levels are plotted above, representing the start and end H-points of the reach.\n\n\n\nres.reaches[\"101l1\"].n_gridpoints\n\n3\n\n\nJust like before, you can iterate through reaches like a Python dictionary. For example:\n\n# This may print multiple lines\nfor reach_name, reach in res.reaches.items():\n    if reach.start_node == \"1\":\n        display(reach)\n\n&lt;Reach: 1l1&gt;\n    \n    Attributes (9)name: 1l1length: 12.0784172521484start_chainage: 0.0end_chainage: 12.0784172521484n_gridpoints: 3start_node: 1end_node: 16height: 0.6000000238418579full_flow_discharge: 0.7808684423624622Quantities (2)Water level (m)Discharge (m^3/s)Derived Quantities (6)ReachAbsoluteDischargeReachFillingReachFloodingReachQQManningReachWaterDepthReachWaterLevelAboveCritical",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Exploring Networks</span>"
    ]
  },
  {
    "objectID": "module3_network_results/03_exploring_network_and_results.html#gridpoints",
    "href": "module3_network_results/03_exploring_network_and_results.html#gridpoints",
    "title": "18  Exploring Networks",
    "section": "18.4 Gridpoints",
    "text": "18.4 Gridpoints\nSpecific reach objects are also both list- and dictionary-like, mapping to their constituent gridpoint objects. Gridpoints are the locations along a reach where results are calculated.\n\n\n\n\n\n\nNote\n\n\n\nUnlike Python dictionaries, reaches also support sequential indexing (e.g., [0] for the first gridpoint) for convenient gridpoint access.\n\n\nAccess the first gridpoint of reach “101l1”:\n\nres.reaches[\"101l1\"][0]\n\n&lt;ResultGridPoint&gt;\n    \n    Attributes (5)reach_name: 101l1chainage: 0.0xcoord: -687859.5004882812ycoord: -1056308.700012207bottom_level: 195.92999267578125Quantities (1)Water level (m)Derived Quantities (0)\n\n\nAccess the last gridpoint of reach “101l1”:\n\nres.reaches[\"101l1\"][-1]\n\n&lt;ResultGridPoint&gt;\n    \n    Attributes (5)reach_name: 101l1chainage: 66.4360966980845xcoord: -687887.6008911133ycoord: -1056368.9006958008bottom_level: 195.44000244140625Quantities (1)Water level (m)Derived Quantities (0)\n\n\nYou can also access gridpoints by their chainage value. Chainage can be a float or a string.\n\nres.reaches[\"100l1\"]['23.841']\n\n&lt;ResultGridPoint&gt;\n    \n    Attributes (5)reach_name: 100l1chainage: 23.8413574216414xcoord: -687897.8000488281ycoord: -1056390.4503479004bottom_level: 195.0500030517578Quantities (1)Discharge (m^3/s)Derived Quantities (0)\n\n\nNotice that each gridpoint has its own dynamic quantities and static properties.\nAccess WaterLevel at the first gridpoint:\n\nres.reaches[\"100l1\"][0].WaterLevel.plot()\n\n\n\n\n\n\n\n\nAccess Discharge at the second gridpoint:\n\nres.reaches[\"100l1\"][1].Discharge.plot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nGridpoint quantities can vary along a reach. For example, WaterLevel is typically available at H-points (calculation points), while Discharge is available at Q-points (flow points, often at gridpoint centers or structures). Refer to the MIKE+ documentation for details on H-points and Q-points.\n\n\nAccess the chainage static property of a gridpoint:\n\nres.reaches[\"100l1\"][-1].chainage\n\n47.6827148432828\n\n\nJust like before, you can iterate through reaches like a Python dictionary. The keys are the gridpoint chainage along the reach. For example:\n\nfor chainage, gridpoint in res.reaches[\"100l1\"].items():\n    print(f\"Bottom level at chainage {chainage} is {gridpoint.bottom_level}\")\n\nBottom level at chainage 0.000 is 195.44000244140625\nBottom level at chainage 23.841 is 195.0500030517578\nBottom level at chainage 47.683 is 194.66000366210938",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Exploring Networks</span>"
    ]
  },
  {
    "objectID": "module3_network_results/03_exploring_network_and_results.html#example---dynamic-autocompletion",
    "href": "module3_network_results/03_exploring_network_and_results.html#example---dynamic-autocompletion",
    "title": "18  Exploring Networks",
    "section": "18.5 Example - Dynamic Autocompletion",
    "text": "18.5 Example - Dynamic Autocompletion\nDynamic autocompletion in environments like Jupyter or VS Code significantly aids in exploring these objects. Watch this video to see it in action.",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Exploring Networks</span>"
    ]
  },
  {
    "objectID": "module3_network_results/04_selecting_network_data.html",
    "href": "module3_network_results/04_selecting_network_data.html",
    "title": "19  Data Selection",
    "section": "",
    "text": "19.1 Alternative Methods\nThis section covers how to select specific subsets of network result data.\nThere are various ways of selecting subsets of network result data. This section covers three main approaches:",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Data Selection</span>"
    ]
  },
  {
    "objectID": "module3_network_results/04_selecting_network_data.html#alternative-methods",
    "href": "module3_network_results/04_selecting_network_data.html#alternative-methods",
    "title": "19  Data Selection",
    "section": "",
    "text": "Using Pandas DataFrame.\nUsing the Res1D object to subset locations and quantities.\nUsing mikeio1d.open().\n\n\n\n\n\n\n\nMemory considerations\n\n\n\n\n\nSimilar to MIKE IO, selecting data via open() is generally most performant since it avoids loading the entire file into memory.",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Data Selection</span>"
    ]
  },
  {
    "objectID": "module3_network_results/04_selecting_network_data.html#selecting-locations",
    "href": "module3_network_results/04_selecting_network_data.html#selecting-locations",
    "title": "19  Data Selection",
    "section": "19.2 Selecting Locations",
    "text": "19.2 Selecting Locations\nWhen dealing with network results, you often want to focus on specific parts of your model, like certain nodes or reaches.\nSelecting data purely with DataFrames can be difficult due to the large number of columns and their header format. For example, this DataFrame has 495 columns:\n\ndf = res.read()\ndf.head()\n\n\n\n\n\n\n\n\nWaterLevel:1\nWaterLevel:2\nWaterLevel:3\nWaterLevel:4\nWaterLevel:5\nWaterLevel:6\nWaterLevel:7\nWaterLevel:8\nWaterLevel:9\nWaterLevel:10\n...\nDischarge:99l1:22.2508\nWaterLevel:9l1:0\nWaterLevel:9l1:10\nDischarge:9l1:5\nWaterLevel:Weir:119w1:0\nWaterLevel:Weir:119w1:1\nDischarge:Weir:119w1:0.5\nWaterLevel:Pump:115p1:0\nWaterLevel:Pump:115p1:82.4281\nDischarge:Pump:115p1:41.214\n\n\n\n\n1994-08-07 16:35:00.000\n195.052994\n195.821503\n195.8815\n193.604996\n193.615005\n193.625000\n193.675003\n193.764999\n193.774994\n193.804993\n...\n0.000002\n193.774994\n193.764999\n0.000031\n193.550003\n188.479996\n0.0\n193.304993\n195.005005\n0.0\n\n\n1994-08-07 16:36:01.870\n195.052994\n195.821701\n195.8815\n193.604996\n193.615005\n193.625320\n193.675110\n193.765060\n193.775116\n193.804993\n...\n0.000002\n193.775070\n193.765060\n0.000031\n193.550003\n188.479996\n0.0\n193.306061\n195.005005\n0.0\n\n\n1994-08-07 16:37:07.560\n195.052994\n195.821640\n195.8815\n193.604996\n193.615005\n193.625671\n193.675369\n193.765106\n193.775513\n193.804993\n...\n0.000002\n193.775391\n193.765106\n0.000033\n193.550034\n188.479996\n0.0\n193.307144\n195.005005\n0.0\n\n\n1994-08-07 16:38:55.828\n195.052994\n195.821503\n195.8815\n193.604996\n193.615005\n193.626236\n193.675751\n193.765228\n193.776077\n193.804993\n...\n0.000002\n193.775894\n193.765228\n0.000037\n193.550079\n188.479996\n0.0\n193.308884\n195.005005\n0.0\n\n\n1994-08-07 16:39:55.828\n195.052994\n195.821503\n195.8815\n193.604996\n193.615005\n193.626556\n193.675949\n193.765335\n193.776352\n193.804993\n...\n0.000002\n193.776154\n193.765335\n0.000039\n193.550095\n188.479996\n0.0\n193.309860\n195.005005\n0.0\n\n\n\n\n5 rows × 495 columns\n\n\n\nTo select the ‘Water Level’ quantity for node ‘1’ from the DataFrame, you need to use a column name formed by concatenating the quantity and ID with a ‘:’ separator:\n\ndf[[\"WaterLevel:1\"]]\n\n\n\n\n\n\n\n\nWaterLevel:1\n\n\n\n\n1994-08-07 16:35:00.000\n195.052994\n\n\n1994-08-07 16:36:01.870\n195.052994\n\n\n1994-08-07 16:37:07.560\n195.052994\n\n\n1994-08-07 16:38:55.828\n195.052994\n\n\n1994-08-07 16:39:55.828\n195.052994\n\n\n...\n...\n\n\n1994-08-07 18:30:07.967\n195.119919\n\n\n1994-08-07 18:31:07.967\n195.118607\n\n\n1994-08-07 18:32:07.967\n195.117310\n\n\n1994-08-07 18:33:07.967\n195.115753\n\n\n1994-08-07 18:35:00.000\n195.112534\n\n\n\n\n110 rows × 1 columns\n\n\n\nSimilarly, to select the “Water Level” of reach “100l1” at chainage 47.6827:\n\ndf[[\"WaterLevel:100l1:47.6827\"]]\n\n\n\n\n\n\n\n\nWaterLevel:100l1:47.6827\n\n\n\n\n1994-08-07 16:35:00.000\n194.661499\n\n\n1994-08-07 16:36:01.870\n194.661621\n\n\n1994-08-07 16:37:07.560\n194.661728\n\n\n1994-08-07 16:38:55.828\n194.661804\n\n\n1994-08-07 16:39:55.828\n194.661972\n\n\n...\n...\n\n\n1994-08-07 18:30:07.967\n194.689072\n\n\n1994-08-07 18:31:07.967\n194.688934\n\n\n1994-08-07 18:32:07.967\n194.688812\n\n\n1994-08-07 18:33:07.967\n194.688354\n\n\n1994-08-07 18:35:00.000\n194.686172\n\n\n\n\n110 rows × 1 columns\n\n\n\nThis format is not very user-friendly, especially for selecting all quantities of a specific reach. The fluent-like API, shown earlier, offers a more intuitive way to do this.\n\nres.reaches[\"100l1\"].read()\n\n\n\n\n\n\n\n\nWaterLevel:100l1:0\nWaterLevel:100l1:47.6827\nDischarge:100l1:23.8414\n\n\n\n\n1994-08-07 16:35:00.000\n195.441498\n194.661499\n0.000006\n\n\n1994-08-07 16:36:01.870\n195.441498\n194.661621\n0.000006\n\n\n1994-08-07 16:37:07.560\n195.441498\n194.661728\n0.000006\n\n\n1994-08-07 16:38:55.828\n195.441498\n194.661804\n0.000006\n\n\n1994-08-07 16:39:55.828\n195.441498\n194.661972\n0.000006\n\n\n...\n...\n...\n...\n\n\n1994-08-07 18:30:07.967\n195.455109\n194.689072\n0.000588\n\n\n1994-08-07 18:31:07.967\n195.455063\n194.688934\n0.000583\n\n\n1994-08-07 18:32:07.967\n195.455002\n194.688812\n0.000579\n\n\n1994-08-07 18:33:07.967\n195.453049\n194.688354\n0.000526\n\n\n1994-08-07 18:35:00.000\n195.450409\n194.686172\n0.000343\n\n\n\n\n110 rows × 3 columns\n\n\n\nA more computationally efficient approach is to select locations when you open the file. You can specify lists of IDs for nodes, reaches, or catchments. For example:\n\nres = mikeio1d.open(\"data/network.res1d\", reaches=[\"100l1\"])\ndf = res.read()\ndf.head()\n\n\n\n\n\n\n\n\nWaterLevel:100l1:0\nWaterLevel:100l1:47.6827\nDischarge:100l1:23.8414\n\n\n\n\n1994-08-07 16:35:00.000\n195.441498\n194.661499\n0.000006\n\n\n1994-08-07 16:36:01.870\n195.441498\n194.661621\n0.000006\n\n\n1994-08-07 16:37:07.560\n195.441498\n194.661728\n0.000006\n\n\n1994-08-07 16:38:55.828\n195.441498\n194.661804\n0.000006\n\n\n1994-08-07 16:39:55.828\n195.441498\n194.661972\n0.000006\n\n\n\n\n\n\n\nSimilarly for nodes:\n\nres = mikeio1d.open(\"data/network.res1d\", nodes=[\"1\", \"2\"])\ndf = res.read()\ndf.head()\n\n\n\n\n\n\n\n\nWaterLevel:1\nWaterLevel:2\n\n\n\n\n1994-08-07 16:35:00.000\n195.052994\n195.821503\n\n\n1994-08-07 16:36:01.870\n195.052994\n195.821701\n\n\n1994-08-07 16:37:07.560\n195.052994\n195.821640\n\n\n1994-08-07 16:38:55.828\n195.052994\n195.821503\n\n\n1994-08-07 16:39:55.828\n195.052994\n195.821503\n\n\n\n\n\n\n\nAnd for catchments:\n\nres = mikeio1d.open(\"data/catchments.res1d\", catchments=[\"100_16_16\"])\ndf = res.read()\ndf.head()\n\n\n\n\n\n\n\n\nTotalRunOff:100_16_16\nActualRainfall:100_16_16\nZinkLoadRR:100_16_16\nZinkMassAccumulatedRR:100_16_16\nZinkRR:100_16_16\n\n\n\n\n1994-08-07 16:35:00\n0.0\n3.333333e-07\n0.0\n0.0\n100.0\n\n\n1994-08-07 16:36:00\n0.0\n3.333333e-07\n0.0\n0.0\n100.0\n\n\n1994-08-07 16:37:00\n0.0\n3.333333e-07\n0.0\n0.0\n100.0\n\n\n1994-08-07 16:38:00\n0.0\n3.333333e-07\n0.0\n0.0\n100.0\n\n\n1994-08-07 16:39:00\n0.0\n3.333333e-07\n0.0\n0.0\n100.0",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Data Selection</span>"
    ]
  },
  {
    "objectID": "module3_network_results/04_selecting_network_data.html#selecting-quantities",
    "href": "module3_network_results/04_selecting_network_data.html#selecting-quantities",
    "title": "19  Data Selection",
    "section": "19.3 Selecting Quantities",
    "text": "19.3 Selecting Quantities\nJust as with locations, you can select specific physical quantities (like Water Level or Discharge). Trying to pick these out from a full DataFrame is one approach:\n\ndf = res.read()\ndischarge_columns = [column for column in df.columns if \"Discharge\" in column]\ndf[discharge_columns].head()\n\n\n\n\n\n\n\n\nDischarge:100l1:23.8414\nDischarge:101l1:33.218\nDischarge:102l1:5.46832\nDischarge:103l1:13.0327\nDischarge:104l1:17.2065\nDischarge:105l1:13.4997\nDischarge:106l1:11.4056\nDischarge:107l1:8.46476\nDischarge:108l1:15.3589\nDischarge:109l1:13.546\n...\nDischarge:93l1:24.5832\nDischarge:94l1:21.2852\nDischarge:95l1:21.9487\nDischarge:96l1:14.9257\nDischarge:97l1:5.71207\nDischarge:98l1:8.00489\nDischarge:99l1:22.2508\nDischarge:9l1:5\nDischarge:Weir:119w1:0.5\nDischarge:Pump:115p1:41.214\n\n\n\n\n1994-08-07 16:35:00.000\n0.000006\n0.000004\n0.000000\n0.000003\n0.000005\n0.000003\n0.000005\n0.000005\n0.000003\n0.000002\n...\n0.000004\n0.000003\n0.000001\n0.000005\n0.000013\n0.000003\n0.000002\n0.000031\n0.0\n0.0\n\n\n1994-08-07 16:36:01.870\n0.000006\n0.000004\n0.000008\n0.000003\n0.000005\n0.000003\n0.000005\n0.000005\n0.000003\n0.000002\n...\n0.000004\n0.000003\n0.000001\n0.000005\n0.000010\n0.000003\n0.000002\n0.000031\n0.0\n0.0\n\n\n1994-08-07 16:37:07.560\n0.000006\n0.000004\n0.000016\n0.000003\n0.000005\n0.000003\n0.000005\n0.000005\n0.000003\n0.000002\n...\n0.000004\n0.000003\n0.000001\n0.000005\n0.000010\n0.000003\n0.000002\n0.000033\n0.0\n0.0\n\n\n1994-08-07 16:38:55.828\n0.000006\n0.000004\n0.000022\n0.000003\n0.000005\n0.000003\n0.000004\n0.000004\n0.000003\n0.000002\n...\n0.000004\n0.000003\n0.000001\n0.000005\n0.000009\n0.000003\n0.000002\n0.000037\n0.0\n0.0\n\n\n1994-08-07 16:39:55.828\n0.000006\n0.000004\n0.000024\n0.000003\n0.000005\n0.000003\n0.000004\n0.000004\n0.000003\n0.000002\n...\n0.000004\n0.000003\n0.000001\n0.000005\n0.000009\n0.000003\n0.000002\n0.000039\n0.0\n0.0\n\n\n\n\n5 rows × 129 columns\n\n\n\nHowever, it’s often expressed more succinctly using the Res1D fluent-like API:\n\ndf = res.reaches.Discharge.read()\ndf.head()\n\n\n\n\n\n\n\n\nDischarge:100l1:23.8414\nDischarge:101l1:33.218\nDischarge:102l1:5.46832\nDischarge:103l1:13.0327\nDischarge:104l1:17.2065\nDischarge:105l1:13.4997\nDischarge:106l1:11.4056\nDischarge:107l1:8.46476\nDischarge:108l1:15.3589\nDischarge:109l1:13.546\n...\nDischarge:93l1:24.5832\nDischarge:94l1:21.2852\nDischarge:95l1:21.9487\nDischarge:96l1:14.9257\nDischarge:97l1:5.71207\nDischarge:98l1:8.00489\nDischarge:99l1:22.2508\nDischarge:9l1:5\nDischarge:Weir:119w1:0.5\nDischarge:Pump:115p1:41.214\n\n\n\n\n1994-08-07 16:35:00.000\n0.000006\n0.000004\n0.000000\n0.000003\n0.000005\n0.000003\n0.000005\n0.000005\n0.000003\n0.000002\n...\n0.000004\n0.000003\n0.000001\n0.000005\n0.000013\n0.000003\n0.000002\n0.000031\n0.0\n0.0\n\n\n1994-08-07 16:36:01.870\n0.000006\n0.000004\n0.000008\n0.000003\n0.000005\n0.000003\n0.000005\n0.000005\n0.000003\n0.000002\n...\n0.000004\n0.000003\n0.000001\n0.000005\n0.000010\n0.000003\n0.000002\n0.000031\n0.0\n0.0\n\n\n1994-08-07 16:37:07.560\n0.000006\n0.000004\n0.000016\n0.000003\n0.000005\n0.000003\n0.000005\n0.000005\n0.000003\n0.000002\n...\n0.000004\n0.000003\n0.000001\n0.000005\n0.000010\n0.000003\n0.000002\n0.000033\n0.0\n0.0\n\n\n1994-08-07 16:38:55.828\n0.000006\n0.000004\n0.000022\n0.000003\n0.000005\n0.000003\n0.000004\n0.000004\n0.000003\n0.000002\n...\n0.000004\n0.000003\n0.000001\n0.000005\n0.000009\n0.000003\n0.000002\n0.000037\n0.0\n0.0\n\n\n1994-08-07 16:39:55.828\n0.000006\n0.000004\n0.000024\n0.000003\n0.000005\n0.000003\n0.000004\n0.000004\n0.000003\n0.000002\n...\n0.000004\n0.000003\n0.000001\n0.000005\n0.000009\n0.000003\n0.000002\n0.000039\n0.0\n0.0\n\n\n\n\n5 rows × 129 columns\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou must use the fluent-like API for quantity selection on a location group (e.g., res.reaches.Discharge) or a specific element, not directly on the top-level Res1D object. For top-level quantity filtering, use the quantities parameter in open().\n\n\nSimilar to location filtering, it’s also more computationally efficient to do this on open():\n\nres = mikeio1d.open(\"data/network.res1d\", quantities=[\"Discharge\"])\ndf = res.read()\ndf.head()\n\n\n\n\n\n\n\n\nDischarge:100l1:23.8414\nDischarge:101l1:33.218\nDischarge:102l1:5.46832\nDischarge:103l1:13.0327\nDischarge:104l1:17.2065\nDischarge:105l1:13.4997\nDischarge:106l1:11.4056\nDischarge:107l1:8.46476\nDischarge:108l1:15.3589\nDischarge:109l1:13.546\n...\nDischarge:93l1:24.5832\nDischarge:94l1:21.2852\nDischarge:95l1:21.9487\nDischarge:96l1:14.9257\nDischarge:97l1:5.71207\nDischarge:98l1:8.00489\nDischarge:99l1:22.2508\nDischarge:9l1:5\nDischarge:Weir:119w1:0.5\nDischarge:Pump:115p1:41.214\n\n\n\n\n1994-08-07 16:35:00.000\n0.000006\n0.000004\n0.000000\n0.000003\n0.000005\n0.000003\n0.000005\n0.000005\n0.000003\n0.000002\n...\n0.000004\n0.000003\n0.000001\n0.000005\n0.000013\n0.000003\n0.000002\n0.000031\n0.0\n0.0\n\n\n1994-08-07 16:36:01.870\n0.000006\n0.000004\n0.000008\n0.000003\n0.000005\n0.000003\n0.000005\n0.000005\n0.000003\n0.000002\n...\n0.000004\n0.000003\n0.000001\n0.000005\n0.000010\n0.000003\n0.000002\n0.000031\n0.0\n0.0\n\n\n1994-08-07 16:37:07.560\n0.000006\n0.000004\n0.000016\n0.000003\n0.000005\n0.000003\n0.000005\n0.000005\n0.000003\n0.000002\n...\n0.000004\n0.000003\n0.000001\n0.000005\n0.000010\n0.000003\n0.000002\n0.000033\n0.0\n0.0\n\n\n1994-08-07 16:38:55.828\n0.000006\n0.000004\n0.000022\n0.000003\n0.000005\n0.000003\n0.000004\n0.000004\n0.000003\n0.000002\n...\n0.000004\n0.000003\n0.000001\n0.000005\n0.000009\n0.000003\n0.000002\n0.000037\n0.0\n0.0\n\n\n1994-08-07 16:39:55.828\n0.000006\n0.000004\n0.000024\n0.000003\n0.000005\n0.000003\n0.000004\n0.000004\n0.000003\n0.000002\n...\n0.000004\n0.000003\n0.000001\n0.000005\n0.000009\n0.000003\n0.000002\n0.000039\n0.0\n0.0\n\n\n\n\n5 rows × 129 columns\n\n\n\n\n\n\n\n\n\nWhere do I find quantity names?\n\n\n\n\n\nTo see all available quantities in a Res1D object (res), you can inspect res.quantities. To see all possible MIKE 1D quantities, you can get a list as follows:\n\nfrom mikeio1d.res1d import mike1d_quantities\n\nall_quantities = mike1d_quantities()",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Data Selection</span>"
    ]
  },
  {
    "objectID": "module3_network_results/04_selecting_network_data.html#selecting-time-steps",
    "href": "module3_network_results/04_selecting_network_data.html#selecting-time-steps",
    "title": "19  Data Selection",
    "section": "19.4 Selecting Time Steps",
    "text": "19.4 Selecting Time Steps\nFiltering by time is another common requirement. If you have your data in a Pandas DataFrame, you can use the time indexing techniques covered in Module 2. For example, the first three time steps:\n\ndf = res.read()\ndf.iloc[:3]\n\n\n\n\n\n\n\n\nWaterLevel:1\nWaterLevel:2\nWaterLevel:3\nWaterLevel:4\nWaterLevel:5\nWaterLevel:6\nWaterLevel:7\nWaterLevel:8\nWaterLevel:9\nWaterLevel:10\n...\nDischarge:99l1:22.2508\nWaterLevel:9l1:0\nWaterLevel:9l1:10\nDischarge:9l1:5\nWaterLevel:Weir:119w1:0\nWaterLevel:Weir:119w1:1\nDischarge:Weir:119w1:0.5\nWaterLevel:Pump:115p1:0\nWaterLevel:Pump:115p1:82.4281\nDischarge:Pump:115p1:41.214\n\n\n\n\n1994-08-07 16:35:00.000\n195.052994\n195.821503\n195.8815\n193.604996\n193.615005\n193.625000\n193.675003\n193.764999\n193.774994\n193.804993\n...\n0.000002\n193.774994\n193.764999\n0.000031\n193.550003\n188.479996\n0.0\n193.304993\n195.005005\n0.0\n\n\n1994-08-07 16:36:01.870\n195.052994\n195.821701\n195.8815\n193.604996\n193.615005\n193.625320\n193.675110\n193.765060\n193.775116\n193.804993\n...\n0.000002\n193.775070\n193.765060\n0.000031\n193.550003\n188.479996\n0.0\n193.306061\n195.005005\n0.0\n\n\n1994-08-07 16:37:07.560\n195.052994\n195.821640\n195.8815\n193.604996\n193.615005\n193.625671\n193.675369\n193.765106\n193.775513\n193.804993\n...\n0.000002\n193.775391\n193.765106\n0.000033\n193.550034\n188.479996\n0.0\n193.307144\n195.005005\n0.0\n\n\n\n\n3 rows × 495 columns\n\n\n\nAs with other selections, filtering by time when opening the file with open() is more efficient. To select time steps when opening the file, use the time parameter to specify the start and end bounds.\n\nres = mikeio1d.open(\"data/network.res1d\", time=('1994-08-07 16:35:00', '1994-08-07 16:38'))\nres.read()\n\n\n\n\n\n\n\n\nWaterLevel:1\nWaterLevel:2\nWaterLevel:3\nWaterLevel:4\nWaterLevel:5\nWaterLevel:6\nWaterLevel:7\nWaterLevel:8\nWaterLevel:9\nWaterLevel:10\n...\nDischarge:99l1:22.2508\nWaterLevel:9l1:0\nWaterLevel:9l1:10\nDischarge:9l1:5\nWaterLevel:Weir:119w1:0\nWaterLevel:Weir:119w1:1\nDischarge:Weir:119w1:0.5\nWaterLevel:Pump:115p1:0\nWaterLevel:Pump:115p1:82.4281\nDischarge:Pump:115p1:41.214\n\n\n\n\n1994-08-07 16:35:00.000\n195.052994\n195.821503\n195.8815\n193.604996\n193.615005\n193.625000\n193.675003\n193.764999\n193.774994\n193.804993\n...\n0.000002\n193.774994\n193.764999\n0.000031\n193.550003\n188.479996\n0.0\n193.304993\n195.005005\n0.0\n\n\n1994-08-07 16:36:01.870\n195.052994\n195.821701\n195.8815\n193.604996\n193.615005\n193.625320\n193.675110\n193.765060\n193.775116\n193.804993\n...\n0.000002\n193.775070\n193.765060\n0.000031\n193.550003\n188.479996\n0.0\n193.306061\n195.005005\n0.0\n\n\n1994-08-07 16:37:07.560\n195.052994\n195.821640\n195.8815\n193.604996\n193.615005\n193.625671\n193.675369\n193.765106\n193.775513\n193.804993\n...\n0.000002\n193.775391\n193.765106\n0.000033\n193.550034\n188.479996\n0.0\n193.307144\n195.005005\n0.0\n\n\n\n\n3 rows × 495 columns\n\n\n\nTo select every nth time step, you can use the step_every parameter:\n\nres = mikeio1d.open(\"data/network.res1d\", step_every=5)\nres.read().head()\n\n\n\n\n\n\n\n\nWaterLevel:1\nWaterLevel:2\nWaterLevel:3\nWaterLevel:4\nWaterLevel:5\nWaterLevel:6\nWaterLevel:7\nWaterLevel:8\nWaterLevel:9\nWaterLevel:10\n...\nDischarge:99l1:22.2508\nWaterLevel:9l1:0\nWaterLevel:9l1:10\nDischarge:9l1:5\nWaterLevel:Weir:119w1:0\nWaterLevel:Weir:119w1:1\nDischarge:Weir:119w1:0.5\nWaterLevel:Pump:115p1:0\nWaterLevel:Pump:115p1:82.4281\nDischarge:Pump:115p1:41.214\n\n\n\n\n1994-08-07 16:35:00.000\n195.052994\n195.821503\n195.8815\n193.604996\n193.615005\n193.625000\n193.675003\n193.764999\n193.774994\n193.804993\n...\n0.000002\n193.774994\n193.764999\n0.000031\n193.550003\n188.479996\n0.0\n193.304993\n195.005005\n0.0\n\n\n1994-08-07 16:40:55.828\n195.052994\n195.821503\n195.8815\n193.604996\n193.615005\n193.626877\n193.676117\n193.765457\n193.776596\n193.805038\n...\n0.000002\n193.776382\n193.765457\n0.000042\n193.550110\n188.479996\n0.0\n193.310852\n195.005005\n0.0\n\n\n1994-08-07 16:45:55.828\n195.052994\n195.821503\n195.8815\n193.604996\n193.615128\n193.628540\n193.676895\n193.766235\n193.777557\n193.805786\n...\n0.000003\n193.777252\n193.766235\n0.000053\n193.550125\n188.479996\n0.0\n193.315674\n195.005005\n0.0\n\n\n1994-08-07 16:51:29.529\n195.099609\n195.821655\n195.8815\n193.605225\n193.624008\n193.648056\n193.678299\n193.774384\n193.795883\n193.829300\n...\n0.000717\n193.794693\n193.774384\n0.000513\n193.550461\n188.479996\n0.0\n193.321274\n195.005005\n0.0\n\n\n1994-08-07 16:58:12.888\n195.088943\n195.821503\n195.8815\n193.607758\n193.634552\n193.672958\n193.706161\n193.829422\n193.852097\n193.869766\n...\n0.001972\n193.846848\n193.829422\n0.005163\n193.555649\n188.479996\n0.0\n193.353027\n195.005005\n0.0\n\n\n\n\n5 rows × 495 columns\n\n\n\nNotice that these options are similar to when loading network result files in MIKE+:\n\n\n\nLoading network results in MIKE+",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Data Selection</span>"
    ]
  },
  {
    "objectID": "module3_network_results/04_selecting_network_data.html#practical-example",
    "href": "module3_network_results/04_selecting_network_data.html#practical-example",
    "title": "19  Data Selection",
    "section": "19.5 Practical Example",
    "text": "19.5 Practical Example\nImagine you’re calibrating a MIKE+ model. There’s two specific points in the network where you have observed flow data. You also have a list of calibration event time stamps you deem relevant. Let’s use Python to automate generating some plots that could be useful during the calibration process.\nFirst, make a list of the reach IDs where the flow meters are.\n\ncalibration_points = [\"12l1\", \"116l1\"]\n\nNext let’s define the event start and stop times.\n\ncalibration_events = [\n    (\"1994-08-07 17:00\", \"1994-08-07 18:30\"),\n    # here we could add another event, but for this example we only use one\n]\n\nLoad the flow meter data from csv.\n\nimport pandas as pd\ndf_obs = pd.read_csv(\"data/flow_meter_data.csv\", index_col=0, parse_dates=True)\ndf_obs.head()\n\n\n\n\n\n\n\n\n116l1_observed\n12l1_observed\n\n\ntime\n\n\n\n\n\n\n1994-08-07 16:35:00\n-0.014113\n-0.095583\n\n\n1994-08-07 16:36:00\n0.043355\n-0.058748\n\n\n1994-08-07 16:37:00\n-0.129244\n0.040089\n\n\n1994-08-07 16:38:00\n-0.052462\n0.068110\n\n\n1994-08-07 16:39:00\n-0.051976\n-0.024882\n\n\n\n\n\n\n\nNow let’s create plots for each calibration point:\n\nfor event in calibration_events:\n    event_start = event[0]  # event start time\n    event_end = event[1]    # event end time\n\n    res = mikeio1d.open(\"data/network.res1d\", time=(event_start, event_end))\n    df_obs_event = df_obs.loc[event_start : event_end]\n\n    for reach in calibration_points:\n        ax = res.reaches[reach].Discharge.plot()\n        df_obs_event[f\"{reach}_observed\"].plot(ax=ax, color='grey', linestyle=\"--\", zorder=-1)\n        ax.legend()\n        ax.grid()\n        ax.set_title(f\"Calibration Plot for Reach '{reach}'\")\n        \n        # optional: save the figure to a PNG file using standard Matplotlib functionality",
    "crumbs": [
      "Module 3 - Network Results",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Data Selection</span>"
    ]
  },
  {
    "objectID": "module3_network_results/homework.html",
    "href": "module3_network_results/homework.html",
    "title": "Homework",
    "section": "",
    "text": "Exercise 1\n\nDownload network.res1d into a new project folder.\nCreate an empty Jupyter Notebook and import mikeio1d.\nOpen the network.res1d file into a Res1D object.\nUse the info() method to get an overview of the results.\nRead all ‘WaterLevel’ quantities for all nodes into a Pandas DataFrame.\nDisplay the describe() statistics for this DataFrame.\n\nExercise 2\n\nContinue with the Res1D object from Exercise 1 (or reopen network.res1d).\nAccess the reach named “101l1”.\nPrint the static properties: length and n_gridpoints for this reach.\nPlot the ‘WaterLevel’ time series for reach “101l1”.\nOn a separate plot, plot the ‘Discharge’ time series for reach “101l1”.\n\nExercise 3\n\nReopen network.res1d using mikeio1d.open().\nThis time, during the open() call, specify that you only want to load:\n\nNodes: “1”, “5”, and “10”.\nQuantities: Only “WaterLevel”.\nTime range: From “1994-08-07 17:00:00” to “1994-08-07 17:30:00”.\n\nRead the data from the resulting Res1D object into a Pandas DataFrame.\nPrint the head() of this DataFrame. How many columns does it have?\n\nExercise 4\n\nOpen network.res1d into a Res1D object.\nAccess the first grid point on the reach named “100l1”.\nPlot its water level.\nAccess the last grid point on the reach named “100l1”.\nPlot its water level.\n\nExercise 5\n\nDownload model.res1d into a new project folder.\nOpen model.res1d into a Res1D object.\nRead the discharge of reach G60F260_G60F240_l1 into a Pandas DateFrame. Use the discharge closest to the end node.\nConfirm the end node of the reach is a node of type Outlet with id G60F240.\nDownload observed.csv into your project folder.\nRead observed flows from the CSV file into a DataFrame (remembering to use a DatetimeIndex). These represent observed flows of the reach referenced in step 3 above.\nPlot the time series comparison of model vs observed for the following events:\n\n2021-07-31 12:00 to 2021-07-31 15:45\n2021-07-28 17:25 to 2021-07-28 20:55\n2021-06-21 06:55 to 2021-06-21 18:00",
    "crumbs": [
      "Module 3 - Network Results",
      "Homework"
    ]
  },
  {
    "objectID": "module4_calibration_plots_and_statistics/index.html",
    "href": "module4_calibration_plots_and_statistics/index.html",
    "title": "Welcome to Module 4!",
    "section": "",
    "text": "This module introduces powerful techniques for model calibration and validation. We will explore ModelSkill, a dedicated Python package for comparing MIKE+ model outputs against observed data.\nYou’ll see how ModelSkill builds upon MIKE IO, leveraging your existing skills in data handling.\nThroughout this module, you will learn to:\n\nPrepare Observation and ModelResult objects.\nMatch observational data with model results.\nVisualize model performance using standard validation plots.\nQuantify model accuracy using statistical skill scores.\n\nThis module culminates in a practical homework assignment where you’ll apply these skills to validate a sample MIKE+ model.\nLet’s dive in!\n\n\n\n\n\n\nWhere can I download sample data to follow along?\n\n\n\n\n\nAll of the sample data used in this module is available for download:\n\nflow_meter_data.dfs0\nflow_meter_data.csv\nmodel_results.dfs0\nnetwork.res1d",
    "crumbs": [
      "Module 4 - Calibration Plots and Statistics",
      "Welcome to Module 4!"
    ]
  },
  {
    "objectID": "module4_calibration_plots_and_statistics/01_intro_modelskill.html",
    "href": "module4_calibration_plots_and_statistics/01_intro_modelskill.html",
    "title": "ModelSkill",
    "section": "",
    "text": "What is ModelSkill?\nModelSkill is a Python package designed to streamline and standardize the validation of models, including those built with MIKE+. It helps you compare your model results against observed data, calculate skill scores, and generate insightful visualizations.\nThe primary purpose of ModelSkill is to provide a robust framework for quantitative model skill assessment. It builds upon libraries like MIKE IO by leveraging their data reading capabilities to easily ingest model outputs and observational data from various formats. This makes it easier to integrate model validation into your Python-based workflows.\nFor comprehensive information, refer to the modelskill documentation.",
    "crumbs": [
      "Module 4 - Calibration Plots and Statistics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>ModelSkill</span>"
    ]
  },
  {
    "objectID": "module4_calibration_plots_and_statistics/01_intro_modelskill.html#key-features",
    "href": "module4_calibration_plots_and_statistics/01_intro_modelskill.html#key-features",
    "title": "ModelSkill",
    "section": "Key Features",
    "text": "Key Features\nModelSkill offers several high-impact features for model validation:\n\nEasy comparison of one or more model results against observations.\nAutomatic calculation of a wide range of statistical skill metrics.\nGeneration of standard validation plots (e.g., time series, scatter plots).\nFlexible handling of various data types and structures.\n\n\n\n\n\n\n\nNote\n\n\n\nModelSkill offers a rich set of functionalities. For a detailed list, please refer to the official documentation’s feature overview.",
    "crumbs": [
      "Module 4 - Calibration Plots and Statistics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>ModelSkill</span>"
    ]
  },
  {
    "objectID": "module4_calibration_plots_and_statistics/01_intro_modelskill.html#installation",
    "href": "module4_calibration_plots_and_statistics/01_intro_modelskill.html#installation",
    "title": "ModelSkill",
    "section": "Installation",
    "text": "Installation\nYou can install ModelSkill using uv in your terminal.\nuv pip install modelskill\n\n\n\n\n\n\nTip\n\n\n\nInstallation methods and specific package versions can change. Always refer to the official ModelSkill installation guide for the most up-to-date instructions and troubleshooting.",
    "crumbs": [
      "Module 4 - Calibration Plots and Statistics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>ModelSkill</span>"
    ]
  },
  {
    "objectID": "module4_calibration_plots_and_statistics/01_intro_modelskill.html#core-concepts",
    "href": "module4_calibration_plots_and_statistics/01_intro_modelskill.html#core-concepts",
    "title": "ModelSkill",
    "section": "Core Concepts",
    "text": "Core Concepts\nUnderstanding a few core concepts will help you use ModelSkill effectively. The main components are:\n\nObservation: Represents your observed data (e.g., sensor time series).\nModelResult: Represents your model simulation data (e.g., MIKE+ time series output).\nComparer: Matches one Observation with one ModelResult. It aligns them (e.g., spatially and in time) and is used to calculate skill scores and generate plots for this specific pair.\nComparerCollection: Groups multiple Comparer objects. Use it to assess model performance against several observation points or to get overall skill scores.\n\nThe general workflow involves preparing Observation and ModelResult objects, using ms.match() to create Comparer objects, optionally grouping them into a ComparerCollection, and then extracting skill metrics and visualizations.\n\n\n\n\n\n\nFiltering and Selecting Data in ModelSkill\n\n\n\n\n\nModelSkill includes powerful methods like .sel(), .query(), and .where() to select and filter data in its Observation, ModelResult, and Comparer objects. These are excellent for refining your analysis, for example, by focusing on specific events or conditions.\nThis module focuses on the essentials. While these advanced selection tools are highly useful, they are not covered in detail here. We encourage you to explore them after the course; they are valuable for more in-depth analysis.",
    "crumbs": [
      "Module 4 - Calibration Plots and Statistics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>ModelSkill</span>"
    ]
  },
  {
    "objectID": "module4_calibration_plots_and_statistics/02_preparing_data.html",
    "href": "module4_calibration_plots_and_statistics/02_preparing_data.html",
    "title": "Preparing Data",
    "section": "",
    "text": "Observations\nModelSkill requires data in Observation and ModelResult objects. These objects are inputs for ModelSkill’s Comparer, which matches data and assesses skill. This section covers PointObservation and PointModelResult for comparing time series at specific points.\nA PointObservation represents measured data, often a time series from one sensor. Each object handles one point and variable. For API details, see the PointObservation documentation.\nKey parameters for PointObservation:",
    "crumbs": [
      "Module 4 - Calibration Plots and Statistics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Preparing Data</span>"
    ]
  },
  {
    "objectID": "module4_calibration_plots_and_statistics/02_preparing_data.html#observations",
    "href": "module4_calibration_plots_and_statistics/02_preparing_data.html#observations",
    "title": "Preparing Data",
    "section": "",
    "text": "Parameter\nDescription\n\n\n\n\nname\nA unique identifier (e.g., “Gauge_A_WaterLevel”). Useful for distinguishing observations and labeling plots.\n\n\ndata\nThe data source: a dfs0 file path, MIKE IO Dataset, or Pandas DataFrame.\n\n\nitem\nSpecifies the data column (for Pandas DataFrame) or item (for MIKE IO Dataset or dfs0 path) from the source. Refer by name (string) or numerical index.\n\n\nquantity\nA modelskill.Quantity defining the variable name (e.g., “Water Level”) and unit (e.g., “m”). Essential if the data source (e.g., Pandas DataFrame) lacks this metadata. ModelSkill often infers this from dfs0 files with EUM information.\n\n\n\n\n\n\n\n\n\nUnderstanding the Quantity object in ModelSkill\n\n\n\n\n\nThe quantity parameter (ms.Quantity(name=\"...\", unit=\"...\")) is vital for ModelSkill. It defines the data’s variable (e.g., “Water Level,” “Discharge”) and unit (e.g., “m,” “m^3/s”). This information is used for:\n\nClear plot labeling.\nCompatibility checks between observations and model results.\n\nModelSkill often infers quantity from dfs0 files with EUM information. For other sources like Pandas DataFrames or CSV files, you must define quantity explicitly.\nConsult the ModelSkill documentation on Quantity for details, including EUM handling and more examples.\n\n\n\n\n\n\n\n\n\nCoordinates (x, y) for a PointObservation\n\n\n\n\n\nModelSkill examples often include x and y coordinates for PointObservation objects. ModelSkill uses these coordinates mainly to interpolate data from spatial model outputs (e.g., dfsu, dfs2 files) to the observation point. This is useful for comparing point observations to 2D or 3D model fields.\nThis module focuses on comparing time series already extracted for specific points (e.g., from a res1d node to dfs0, or a point sensor dfs0). Thus, we won’t use the x and y spatial interpolation capability extensively here.\n\n\n\n\nFrom Dataset\nFirst, read a dfs0 file into a MIKE IO Dataset.\n\nds_obs = mikeio.read(\"data/flow_meter_data.dfs0\")\nds_obs\n\n&lt;mikeio.Dataset&gt;\ndims: (time:121)\ntime: 1994-08-07 16:35:00 - 1994-08-07 18:35:00 (121 records)\ngeometry: GeometryUndefined()\nitems:\n  0:  116l1_observed &lt;Discharge&gt; (meter pow 3 per sec)\n  1:  12l1_observed &lt;Discharge&gt; (meter pow 3 per sec)\n\n\nCreate a PointObservation from this Dataset, selecting one item.\n\nimport modelskill as ms\n\nobs_116l1 = ms.PointObservation(\n    data=ds_obs,\n    item=\"116l1_observed\",    # Selects one column/item\n    name=\"116l1_Gauge\",       # Descriptive name for this specific observation\n)\nobs_116l1\n\n&lt;PointObservation&gt;: 116l1_Gauge\nLocation: nan, nan\nTime: 1994-08-07 16:35:00 - 1994-08-07 18:35:00\nQuantity: Discharge [m^3/s]\n\n\nA PointObservation has useful attributes and methods. Plot to verify:\n\nobs_116l1.plot()\n\n\n\n\n\n\n\n\n\n\nFrom dfs0 file\nAlternatively, create a PointObservation using the dfs0 file path directly:\n\nobs_116l1_from_file = ms.PointObservation(\n    data=\"data/flow_meter_data.dfs0\",\n    item=\"116l1_observed\",\n    name=\"116l1_Gauge\",\n)\nobs_116l1_from_file.to_dataframe().head()\n\n\n\n\n\n\n\n\n116l1_Gauge\n\n\ntime\n\n\n\n\n\n1994-08-07 16:35:00\n-0.014113\n\n\n1994-08-07 16:36:00\n0.043355\n\n\n1994-08-07 16:37:00\n-0.129244\n\n\n1994-08-07 16:38:00\n-0.052462\n\n\n1994-08-07 16:39:00\n-0.051976\n\n\n\n\n\n\n\n\n\nFrom Pandas DataFrame\nFirst, prepare a Pandas DataFrame.\n\ndf_obs_csv = pd.read_csv(\"data/flow_meter_data.csv\", index_col=\"time\", parse_dates=True)\ndf_obs_csv.head()\n\n\n\n\n\n\n\n\n116l1_observed\n12l1_observed\n\n\ntime\n\n\n\n\n\n\n1994-08-07 16:35:00\n-0.014113\n-0.095583\n\n\n1994-08-07 16:36:00\n0.043355\n-0.058748\n\n\n1994-08-07 16:37:00\n-0.129244\n0.040089\n\n\n1994-08-07 16:38:00\n-0.052462\n0.068110\n\n\n1994-08-07 16:39:00\n-0.051976\n-0.024882\n\n\n\n\n\n\n\nCreate a PointObservation from the DataFrame. Provide quantity as DataFrames lack EUM information.\n\nobs_12l1_from_df = ms.PointObservation(\n    data=df_obs_csv,\n    item=\"12l1_observed\",\n    name=\"12l1_Gauge\",\n    quantity=ms.Quantity(name=\"Discharge\", unit=\"m^3/s\"),\n)\nobs_12l1_from_df.plot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nEnsure DataFrames have a DatetimeIndex, as mentioned in previous modules.",
    "crumbs": [
      "Module 4 - Calibration Plots and Statistics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Preparing Data</span>"
    ]
  },
  {
    "objectID": "module4_calibration_plots_and_statistics/02_preparing_data.html#model-results",
    "href": "module4_calibration_plots_and_statistics/02_preparing_data.html#model-results",
    "title": "Preparing Data",
    "section": "Model Results",
    "text": "Model Results\nPointModelResult objects represent model simulation outputs. Each PointModelResult handles one variable from a specific model output point and represents a model simulation run. See the PointModelResult documentation for API details.\nKey parameters for PointModelResult are similar to PointObservation:\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nname\nIdentifies the model simulation run (e.g., “MIKE_Plus_Scenario_A”).\n\n\ndata\nThe data source: a dfs0 file path, MIKE IO Dataset, or Pandas DataFrame.\n\n\nitem\nSpecifies the data column (for Pandas DataFrame) or item (for MIKE IO Dataset or dfs0 path) from the source.\n\n\nquantity\nA modelskill.Quantity. Crucial if metadata is missing (e.g., Pandas DataFrame). Often inferred from dfs0 files with EUM info.\n\n\n\n\n\n\n\n\n\nMany PointModelResult objects can share the same name\n\n\n\n\n\nThe name parameter in PointModelResult identifies the overall model simulation, not a specific point. You may create several PointModelResult objects that all come from the same simulation but represent different output locations (e.g., water level at point A, discharge at point B). All these objects should share the same name (e.g., “Model_Run_Alpha”). This shared name signifies they originate from the same model execution. Later, when using ModelSkill’s Comparer, you will explicitly match each of these individual PointModelResult objects to its corresponding Observation object.\n\n\n\n\nFrom Dataset\nFirst, read a dfs0 file with model output into a MIKE IO Dataset.\n\nds_model_data = mikeio.read(\"data/model_results.dfs0\")\nds_model_data\n\n&lt;mikeio.Dataset&gt;\ndims: (time:110)\ntime: 1994-08-07 16:35:00 - 1994-08-07 18:35:00 (110 non-equidistant records)\ngeometry: GeometryUndefined()\nitems:\n  0:  reach:Discharge:116l1:37.651 &lt;Discharge&gt; (meter pow 3 per sec)\n  1:  reach:Discharge:12l1:28.410 &lt;Discharge&gt; (meter pow 3 per sec)\n\n\nCreate the PointModelResult from the Dataset. name identifies the model simulation. quantity is often inferred from dfs0 files with EUM information.\n\nmod_116l1_dataset = ms.PointModelResult(\n    data=ds_model_data,\n    item=\"reach:Discharge:116l1:37.651\",       # Item name from the dfs0\n    name=\"MIKE+_RunA\",                         # Model simulation identifier\n)\nmod_116l1_dataset\n\n&lt;PointModelResult&gt;: MIKE+_RunA\nLocation: nan, nan\nTime: 1994-08-07 16:35:00 - 1994-08-07 18:35:00\nQuantity: Discharge [m^3/s]\n\n\nLike with observations, the PointModelResult object has useful attributes and methods. For example, plot to verify:\n\nmod_116l1_dataset.plot()\n\n\n\n\n\n\n\n\n\n\nFrom dfs0 file\nCreate a PointModelResult using the dfs0 file path directly.\n\nmod_12l1_file = ms.PointModelResult(\n    data=\"data/model_results.dfs0\",\n    item=\"reach:Discharge:12l1:28.410\",\n    name=\"MIKE+_RunA\",                      # Same simulation as above, different location/item\n)\nmod_12l1_file.to_dataframe().head()\n\n\n\n\n\n\n\n\nMIKE+_RunA\n\n\ntime\n\n\n\n\n\n1994-08-07 16:35:00.000\n0.000000\n\n\n1994-08-07 16:36:01.870\n-0.000004\n\n\n1994-08-07 16:37:07.560\n-0.000009\n\n\n1994-08-07 16:38:55.828\n-0.000004\n\n\n1994-08-07 16:39:55.828\n0.000006\n\n\n\n\n\n\n\n\n\nFrom Pandas DataFrame\nFirst, prepare a Pandas DataFrame with model data. This example reads a dfs0 file into a DataFrame.\n\ndf_model = mikeio.read(\"data/model_results.dfs0\").to_dataframe()\ndf_model.head()\n\n\n\n\n\n\n\n\nreach:Discharge:116l1:37.651\nreach:Discharge:12l1:28.410\n\n\n\n\n1994-08-07 16:35:00.000\n0.000000\n0.000000\n\n\n1994-08-07 16:36:01.870\n0.000007\n-0.000004\n\n\n1994-08-07 16:37:07.560\n0.000022\n-0.000009\n\n\n1994-08-07 16:38:55.828\n0.000043\n-0.000004\n\n\n1994-08-07 16:39:55.828\n0.000054\n0.000006\n\n\n\n\n\n\n\nCreate a PointModelResult from the DataFrame. Provide quantity as DataFrames lack EUM information.\n\nmod_116l1_df = ms.PointModelResult(\n    data=df_model,\n    item=\"reach:Discharge:116l1:37.651\",            # Column name in the DataFrame\n    name=\"MIKE+_RunA\",                                   # Identifies the overall model simulation\n    quantity=ms.Quantity(name=\"Discharge\", unit=\"m^3/s\"),\n)\nmod_116l1_df.plot()\n\n\n\n\n\n\n\n\n\n\nFrom res1d file\nMIKE+ res1d files store results for an entire network. For point comparisons with PointObservation in ModelSkill, first extract the specific time series for the point(s) into an intermediate format (e.g., dfs0 file, Pandas DataFrame). This example extracts one model output point to a dfs0 file, then creates a PointModelResult.\nFirst, extract model output (one point, one variable) to a dfs0 file.\n\nres = mikeio1d.open(\"data/network.res1d\")\nres.reaches[\"116l1\"][\"37.651\"].Discharge.to_dfs0(\"data/model_Q_116l1.dfs0\")\nds = mikeio.read(\"data/model_Q_116l1.dfs0\")\nds\n\n&lt;mikeio.Dataset&gt;\ndims: (time:110)\ntime: 1994-08-07 16:35:00 - 1994-08-07 18:35:00 (110 non-equidistant records)\ngeometry: GeometryUndefined()\nitems:\n  0:  reach:Discharge:116l1:37.651 &lt;Discharge&gt; (meter pow 3 per sec)\n\n\nNow, create a PointModelResult from this new dfs0 file. Use the item name from the Dataset object created above.\n\nmod_116l1 = ms.PointModelResult(\n    data=ds,\n    item=\"reach:Discharge:116l1:37.651\",\n    name=\"MIKE+\",\n)\nmod_116l1.plot() # Verify\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFuture: better integration of res1d with ModelSkill\n\n\n\n\n\nFuture versions of ModelSkill may allow creating a network result, instead of a point result. This would allow network results to automatically be matched with corresponding observations, eliminating the need to manually match individual model result points with observation points.",
    "crumbs": [
      "Module 4 - Calibration Plots and Statistics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Preparing Data</span>"
    ]
  },
  {
    "objectID": "module4_calibration_plots_and_statistics/02_preparing_data.html#best-practices",
    "href": "module4_calibration_plots_and_statistics/02_preparing_data.html#best-practices",
    "title": "Preparing Data",
    "section": "Best Practices",
    "text": "Best Practices\nConsistent data organization and naming are key.\n\nOrganize Data: Structure observation and model result files (e.g., separate folders, clear names). This helps when programmatically accessing many files.\nDescriptive Names: Use name in PointObservation and PointModelResult for clear identifiers (e.g., PointObservation(name=\"Flow_Gauge_West\")). This aids in managing objects, improves plot clarity, and helps programmatic creation with many observations or runs.\nSpecify Units and Quantities: Always provide quantity for sources like DataFrames or CSVs. ModelSkill often infers this from dfs0 files with EUM information. Correct metadata is crucial for comparisons, visualizations, and automated workflows. See the modelskill.Quantity callout and official documentation.\n\n\n\n\n\n\n\nBeyond points: Exploring other data types in ModelSkill\n\n\n\n\n\nModelSkill is versatile. This section focuses on point data, but the package also supports TrackObservation (data along a path) and GridObservation (gridded data). These are useful for different validation scenarios. See the official documentation for examples and use cases.",
    "crumbs": [
      "Module 4 - Calibration Plots and Statistics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Preparing Data</span>"
    ]
  },
  {
    "objectID": "module4_calibration_plots_and_statistics/03_matching_data.html",
    "href": "module4_calibration_plots_and_statistics/03_matching_data.html",
    "title": "Matching Data",
    "section": "",
    "text": "Comparer\nNow that you’ve prepared your Observation and ModelResult data, the next crucial step is to bring them together for direct comparison. ModelSkill makes this easy with ms.match() which creates a Comparer object. For situations where you need to assess performance across multiple locations or aggregate results, you can group several Comparer objects into a ComparerCollection.\nThis section uses Observation and ModelResult objects prepared as in previous examples.\nUse ms.match() to create a Comparer object. A Comparer is designed to match one Observation with one ModelResult. For point data, match() intelligently interpolates model results to observation timestamps, ensuring a direct, like-for-like comparison.\nLet’s match our previously defined obs_116l1 with mod_116l1:\nobs_116l1\n\n&lt;PointObservation&gt;: 116l1\nLocation: nan, nan\nTime: 1994-08-07 16:35:00 - 1994-08-07 18:35:00\nQuantity: Discharge [m^3/s]\nmod_116l1\n\n&lt;PointModelResult&gt;: MIKE+\nLocation: nan, nan\nTime: 1994-08-07 16:35:00 - 1994-08-07 18:35:00\nQuantity: Discharge [m^3/s]\ncomparer_116l1 = ms.match(obs_116l1, mod_116l1)\ncomparer_116l1\n\n&lt;Comparer&gt;\nQuantity: Discharge [m^3/s]\nObservation: 116l1, n_points=121\nModel(s):\n0: MIKE+\nThe Comparer now conveniently holds your matched data, interpolated and aligned, ready for detailed analysis and visualization. You can convert this to a Pandas DataFrame to inspect the aligned data. Notice how the DataFrame contains raw observation values and model values interpolated to the exact observation times.\ndf_aligned_single = comparer_116l1.to_dataframe()\ndf_aligned_single.head()\n\n\n\n\n\n\n\n\nObservation\nMIKE+\n\n\ntime\n\n\n\n\n\n\n1994-08-07 16:35:00\n-0.014113\n0.000000\n\n\n1994-08-07 16:36:00\n0.043355\n0.000007\n\n\n1994-08-07 16:37:00\n-0.129244\n0.000021\n\n\n1994-08-07 16:38:00\n-0.052462\n0.000032\n\n\n1994-08-07 16:39:00\n-0.051976\n0.000044\nA Comparer object is your gateway to generating insightful plots and calculating a range of skill scores. Plotting and skill assessment are detailed later in this module, but here’s a quick preview:\ncomparer_116l1.plot.timeseries()\ncomparer_116l1.skill()\n\n\n\n\n\n\n\n\nn\nbias\nrmse\nurmse\nmae\ncc\nsi\nr2\n\n\nobservation\n\n\n\n\n\n\n\n\n\n\n\n\n116l1\n121\n0.003229\n0.068114\n0.068037\n0.054517\n0.991234\n0.172594\n0.982405",
    "crumbs": [
      "Module 4 - Calibration Plots and Statistics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Matching Data</span>"
    ]
  },
  {
    "objectID": "module4_calibration_plots_and_statistics/03_matching_data.html#comparer",
    "href": "module4_calibration_plots_and_statistics/03_matching_data.html#comparer",
    "title": "Matching Data",
    "section": "",
    "text": "Ensure Correct Pairing\n\n\n\nIt’s crucial that the Observation and ModelResult objects passed to ms.match() represent the same physical location and variable. ModelSkill relies on you to provide these correctly paired inputs.\n\n\n\n\n\n\n\n\nHandling Gaps in Model Data During Matching\n\n\n\n\n\nWhen matching, ModelSkill interpolates model results to observation times. If your model data has significant time gaps, you might not want to interpolate across very large intervals. For example, this is often the case with LTS simulations. The max_model_gap parameter in ms.match() controls this. It specifies the maximum gap (in seconds) in the model data over which to interpolate. If a gap is larger, the corresponding observation points will not have a matched model value.\n\ncomparer_116l1_gapped = ms.match(obs_116l1, mod_116l1, max_model_gap=600)\ncomparer_116l1_gapped\n\n&lt;Comparer&gt;\nQuantity: Discharge [m^3/s]\nObservation: 116l1, n_points=121\nModel(s):\n0: MIKE+\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFiltering Matched Data\n\n\n\n\n\nAfter matching, you might want to further filter the Comparer data before calculating skill scores. For example, you might want to exclude periods of low flow or focus only on specific events. The .query() method allows you to apply conditions, similar to Pandas. It returns a new Comparer object with the filtered data.\n\ncomparer_116l1_filtered = comparer_116l1.query(\"Observation &gt; 0.8\")\ncomparer_116l1_filtered.plot.timeseries()\n\n\n\n\n\n\n\n\nModelSkill offers more filtering options not covered here — see the documentation for details.",
    "crumbs": [
      "Module 4 - Calibration Plots and Statistics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Matching Data</span>"
    ]
  },
  {
    "objectID": "module4_calibration_plots_and_statistics/03_matching_data.html#comparercollection",
    "href": "module4_calibration_plots_and_statistics/03_matching_data.html#comparercollection",
    "title": "Matching Data",
    "section": "ComparerCollection",
    "text": "ComparerCollection\nOften, you’ll want to evaluate your model against multiple observation points simultaneously or assess its overall performance across different locations. For this, ModelSkill provides the ComparerCollection, which groups multiple Comparer objects.\n\n\n\n\n\n\nComparing models against other models\n\n\n\n\n\nA ComparerCollection can also be used to compare multiple different models against the same set of observations. That use case is not covered in this module. Refer to ModelSkill’s documentation for details on this powerful feature.\n\n\n\nFirst, let’s create another Comparer for our second observation point, obs_12l1, and its corresponding model result, mod_12l1 (which comes from the same MIKE+ simulation).\n\nobs_12l1\n\n&lt;PointObservation&gt;: 12l1\nLocation: nan, nan\nTime: 1994-08-07 16:35:00 - 1994-08-07 18:35:00\nQuantity: Discharge [m^3/s]\n\n\n\nmod_12l1\n\n&lt;PointModelResult&gt;: MIKE+\nLocation: nan, nan\nTime: 1994-08-07 16:35:00 - 1994-08-07 18:35:00\nQuantity: Discharge [m^3/s]\n\n\n\ncomparer_12l1 = ms.match(obs_12l1, mod_12l1)\ncomparer_12l1\n\n&lt;Comparer&gt;\nQuantity: Discharge [m^3/s]\nObservation: 12l1, n_points=121\nModel(s):\n0: MIKE+\n\n\nNow, we can combine comparer_116l1 and comparer_12l1 into a ComparerCollection:\n\ncc = ms.ComparerCollection([comparer_116l1, comparer_12l1])\ncc\n\n&lt;ComparerCollection&gt;\nComparers:\n0: 116l1 - Discharge [m^3/s]\n1: 12l1 - Discharge [m^3/s]\n\n\nThe ComparerCollection (cc) now manages both comparisons. This allows for powerful aggregate views. For instance, it provides skill assessment for each individual observation:\n\ncc.skill()\n\n\n\n\n\n\n\n\nn\nbias\nrmse\nurmse\nmae\ncc\nsi\nr2\n\n\nobservation\n\n\n\n\n\n\n\n\n\n\n\n\n116l1\n121\n0.003229\n0.068114\n0.068037\n0.054517\n0.991234\n0.172594\n0.982405\n\n\n12l1\n121\n-0.004083\n0.063414\n0.063282\n0.049679\n0.971574\n0.305942\n0.942928\n\n\n\n\n\n\n\nMore importantly, it enables aggregate views of model performance, such as overall skill scores that consider all included comparisons:\n\ncc.mean_skill()\n\n\n\n\n\n\n\n\nn\nbias\nrmse\nurmse\nmae\ncc\nsi\nr2\n\n\nmodel\n\n\n\n\n\n\n\n\n\n\n\n\nMIKE+\n242\n-0.000427\n0.065764\n0.06566\n0.052098\n0.981404\n0.239268\n0.962667\n\n\n\n\n\n\n\nAnd aggregate plots, like a histogram of residual errors across all matched observation points:\n\ncc.plot.residual_hist()\n\n\n\n\n\n\n\n\nYou can inspect the collection’s properties to see which observations and models are included:\nUnique observation names within the collection:\n\ncc.obs_names\n\n['116l1', '12l1']\n\n\nUnique model names. Since both mod_116l1 and mod_12l1 were created with name=\"MIKE+\", “MIKE+” appears only once, signifying they are from the same model run.\n\ncc.mod_names\n\n['MIKE+']\n\n\nYou can also select an individual Comparer from the collection by its observation name, allowing you to focus on a specific comparison:\n\nselected_comparer = cc['116l1']\nselected_comparer.plot.timeseries()",
    "crumbs": [
      "Module 4 - Calibration Plots and Statistics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Matching Data</span>"
    ]
  },
  {
    "objectID": "module4_calibration_plots_and_statistics/04_visualizing_performance.html",
    "href": "module4_calibration_plots_and_statistics/04_visualizing_performance.html",
    "title": "Visualization",
    "section": "",
    "text": "Comparer Plots\nAfter matching observations and model results into Comparer and ComparerCollection objects (as shown in the previous section), you can visualize these comparisons. ModelSkill simplifies generating standard validation plots, offering a more direct approach than manually creating them with Pandas and Matplotlib as covered in Module 2. This section demonstrates these built-in plotting capabilities for assessing model performance, both for individual comparison points and aggregated across multiple locations.\nA Comparer (one observation vs. one model result) offers several plot types for detailed inspection.",
    "crumbs": [
      "Module 4 - Calibration Plots and Statistics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "module4_calibration_plots_and_statistics/04_visualizing_performance.html#comparer-plots",
    "href": "module4_calibration_plots_and_statistics/04_visualizing_performance.html#comparer-plots",
    "title": "Visualization",
    "section": "",
    "text": "Time Series Plot\nOverlays observed and model time series. Shows how well the model captures temporal patterns, peaks, and timing.\n\ncomparer_116l1.plot.timeseries()\n\n\n\n\n\n\n\n\n\n\nScatter Plot\nPlots observed values against model values. Points near the 1:1 line indicate good agreement. Helps identify bias or scaling issues.\n\ncomparer_116l1.plot.scatter()\n\n\n\n\n\n\n\n\n\n\nHistogram Plot\nCompares frequency distributions of observed and model data. Shows if the model reproduces the overall value spread.\n\ncomparer_116l1.plot.hist()\n\n\n\n\n\n\n\n\nModelSkill offers additional Comparer plots, such as residual histograms and Q-Q plots. See the official ModelSkill ComparerPlotter API documentation for a complete list.",
    "crumbs": [
      "Module 4 - Calibration Plots and Statistics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "module4_calibration_plots_and_statistics/04_visualizing_performance.html#comparercollection-plots",
    "href": "module4_calibration_plots_and_statistics/04_visualizing_performance.html#comparercollection-plots",
    "title": "Visualization",
    "section": "ComparerCollection Plots",
    "text": "ComparerCollection Plots\nA ComparerCollection allows for aggregate plots, summarizing performance across all included comparisons. These aggregated plots are powerful because they give you a broader picture of your model’s performance across all your chosen validation points, rather than just looking at one location in isolation.\n\nTemporal Coverage Plot\nShows the temporal data availability for each observation and model result in the collection, indicating periods of overlap and data gaps.\n\ncc.plot.temporal_coverage()\n\n\n\n\n\n\n\n\n\n\nScatter Plot\nAggregates all (observed, model) pairs from the collection. This gives an overview of model performance across all locations, providing a holistic view of point-by-point agreement.\n\ncc.plot.scatter()\n\n\n\n\n\n\n\n\n\n\nHistogram Plot\nCombines data from all comparisons. This shows if the model matches the overall statistical profile of the observed data when considering all sites together.\n\ncc.plot.hist()\n\n\n\n\n\n\n\n\nThe ComparerCollection offers other aggregate plots, like box plots. For more options, consult the ModelSkill ComparerCollectionPlotter API documentation.\n\n\n\n\n\n\nCustomizing and Saving Plots\n\n\n\nModelSkill plots are Matplotlib objects. Customize and save them using standard Matplotlib functions (e.g., ax.set_title(\"My Custom Title\"), plt.savefig(\"my_plot.png\")).\n\n\nThese plots offer a qualitative assessment of model performance. The next section will cover how to quantify performance using ModelSkill’s statistical skill scores.",
    "crumbs": [
      "Module 4 - Calibration Plots and Statistics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "module4_calibration_plots_and_statistics/05_quantifying_performance.html",
    "href": "module4_calibration_plots_and_statistics/05_quantifying_performance.html",
    "title": "Skill Scores",
    "section": "",
    "text": "Comparer\nVisualizing model performance provides qualitative insights, but quantitative metrics are essential for objective assessment and comparison. Skill scores serve this purpose by providing numerical measures of how well a model’s predictions match observed data. ModelSkill facilitates the calculation of these key statistics through its Comparer and ComparerCollection objects.\nThe Comparer object (e.g., comparer_116l1) calculates skill scores for a single observation-model pair.",
    "crumbs": [
      "Module 4 - Calibration Plots and Statistics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Skill Scores</span>"
    ]
  },
  {
    "objectID": "module4_calibration_plots_and_statistics/05_quantifying_performance.html#comparer",
    "href": "module4_calibration_plots_and_statistics/05_quantifying_performance.html#comparer",
    "title": "Skill Scores",
    "section": "",
    "text": "Skill Table\nThe skill() method returns a SkillTable object, which is a specialized data structure provided by ModelSkill for presenting multiple skill scores in a clear, tabular format.\n\nsk_single = comparer_116l1.skill()\nsk_single\n\n\n\n\n\n\n\n\nn\nbias\nrmse\nurmse\nmae\ncc\nsi\nr2\n\n\nobservation\n\n\n\n\n\n\n\n\n\n\n\n\n116l1\n121\n0.003229\n0.068114\n0.068037\n0.054517\n0.991234\n0.172594\n0.982405\n\n\n\n\n\n\n\n\n\n\n\n\n\nAvailable metrics\n\n\n\n\n\nYou’ll notice several metrics listed (e.g., bias, rmse, nse). We’ll cover the definitions of common metrics in more detail at the end of this section.\n\n\n\n\n\n\n\n\n\nSkill metrics in DataFrame\n\n\n\n\n\nThe SkillTable object can be converted to a Pandas DataFrame using sk_single.to_dataframe().\n\n\n\nTo get a subset of metrics, pass a list of metric names to the metrics argument.\n\nsk_subset_single = comparer_116l1.skill(metrics=['rmse', 'bias', 'nse'])\nsk_subset_single\n\n\n\n\n\n\n\n\nn\nrmse\nbias\nnse\n\n\nobservation\n\n\n\n\n\n\n\n\n116l1\n121\n0.068114\n0.003229\n0.982405\n\n\n\n\n\n\n\n\n\nSingle Score\nUse score() for direct access to a single numerical value for a specific metric. If model results within the Comparer are named (as in this example with “MIKE+”), this method returns a dictionary where keys are model names.\n\nrmse_val_dict = comparer_116l1.score(metric='rmse')\nprint(f\"RMSE for MIKE+ at 116l1: {rmse_val_dict['MIKE+']:.4f}\")\n\nbias_val_dict = comparer_116l1.score(metric='bias')\nprint(f\"Bias for MIKE+ at 116l1: {bias_val_dict['MIKE+']:.4f}\")\n\nRMSE for MIKE+ at 116l1: 0.0681\nBias for MIKE+ at 116l1: 0.0032",
    "crumbs": [
      "Module 4 - Calibration Plots and Statistics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Skill Scores</span>"
    ]
  },
  {
    "objectID": "module4_calibration_plots_and_statistics/05_quantifying_performance.html#comparercollection",
    "href": "module4_calibration_plots_and_statistics/05_quantifying_performance.html#comparercollection",
    "title": "Skill Scores",
    "section": "ComparerCollection",
    "text": "ComparerCollection\nThe ComparerCollection (e.g., cc) assesses model performance across multiple observation points.\n\nSkill Table\nCalling skill() on a ComparerCollection returns a SkillTable object summarizing skill for each Comparer within the collection.\n\nsk_coll = cc.skill()\nsk_coll\n\n\n\n\n\n\n\n\nn\nbias\nrmse\nurmse\nmae\ncc\nsi\nr2\n\n\nobservation\n\n\n\n\n\n\n\n\n\n\n\n\n116l1\n121\n0.003229\n0.068114\n0.068037\n0.054517\n0.991234\n0.172594\n0.982405\n\n\n12l1\n121\n-0.004083\n0.063414\n0.063282\n0.049679\n0.971574\n0.305942\n0.942928\n\n\n\n\n\n\n\nYou can request specific metrics for all comparisons.\n\nsk_subset_coll = cc.skill(metrics=['rmse', 'bias'])\nsk_subset_coll\n\n\n\n\n\n\n\n\nn\nrmse\nbias\n\n\nobservation\n\n\n\n\n\n\n\n116l1\n121\n0.068114\n0.003229\n\n\n12l1\n121\n0.063414\n-0.004083\n\n\n\n\n\n\n\n\n\nMean Skill Table\nThe mean_skill() method calculates average skill scores across all locations, presented in a SkillTable.\n\nsk_mean = cc.mean_skill()\nsk_mean\n\n\n\n\n\n\n\n\nn\nbias\nrmse\nurmse\nmae\ncc\nsi\nr2\n\n\nmodel\n\n\n\n\n\n\n\n\n\n\n\n\nMIKE+\n242\n-0.000427\n0.065764\n0.06566\n0.052098\n0.981404\n0.239268\n0.962667\n\n\n\n\n\n\n\nAnd for specific metrics:\n\nsk_mean_subset = cc.mean_skill(metrics=['rmse', 'bias', 'nse'])\nsk_mean_subset\n\n\n\n\n\n\n\n\nn\nrmse\nbias\nnse\n\n\nmodel\n\n\n\n\n\n\n\n\nMIKE+\n242\n0.065764\n-0.000427\n0.962667\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeighted mean skill\n\n\n\n\n\nThe mean_skill() method allows for weighted averages. You can provide weights for each observation if, for example, you trust some observation points more than others or if they represent areas of different importance. See the ModelSkill documentation for details on applying weights.\n\n\n\n\n\nScore\nThe score() method on a ComparerCollection calculates a score for each model across all relevant observations. It returns a Python dictionary where keys are the model names (e.g., ‘MIKE+’) and values are these scores (e.g., mean RMSE for ‘MIKE+’). This provides a single summary value for each model’s performance on a specific metric.\n\n# For our ComparerCollection 'cc' containing one model named \"MIKE+\"\nscore_rmse_scores = cc.score(metric='rmse')\nprint(f\"Mean RMSE for models: {score_rmse_scores}\")\n\nscore_bias_scores = cc.score(metric='bias')\nprint(f\"Mean Bias for models: {score_bias_scores}\")\n\nMean RMSE for models: {'MIKE+': 0.06576367286619714}\nMean Bias for models: {'MIKE+': -0.00042693940822553073}\n\n\n\n\n\n\n\n\nWeighted mean score\n\n\n\n\n\nSimilar to mean_skill(), the score() method on a ComparerCollection also supports weighting. This enables you to calculate a weighted mean score (e.g., weighted RMSE) for each model across all observations.",
    "crumbs": [
      "Module 4 - Calibration Plots and Statistics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Skill Scores</span>"
    ]
  },
  {
    "objectID": "module4_calibration_plots_and_statistics/05_quantifying_performance.html#working-with-skilltables",
    "href": "module4_calibration_plots_and_statistics/05_quantifying_performance.html#working-with-skilltables",
    "title": "Skill Scores",
    "section": "Working with SkillTables",
    "text": "Working with SkillTables\nSkillTable objects are more than just static tables; they offer several useful features for analysis and presentation.\n\nSorting Values\nYou can sort the SkillTable by any of its columns (metrics or identifiers). This is useful for ranking models or observations.\n\n# Sort by RMSE in ascending order\nsk_coll_sorted = sk_coll.sort_values('rmse', ascending=True)\nsk_coll_sorted\n\n\n\n\n\n\n\n\nn\nbias\nrmse\nurmse\nmae\ncc\nsi\nr2\n\n\nobservation\n\n\n\n\n\n\n\n\n\n\n\n\n12l1\n121\n-0.004083\n0.063414\n0.063282\n0.049679\n0.971574\n0.305942\n0.942928\n\n\n116l1\n121\n0.003229\n0.068114\n0.068037\n0.054517\n0.991234\n0.172594\n0.982405\n\n\n\n\n\n\n\n\n\nStyling Tables\nSkillTable objects integrate with Pandas’ styling capabilities, allowing you to highlight important values, apply color maps, or format numbers for better readability in Jupyter environments.\n\nsk_coll.style()\n\n\n\n\n\n\n \nn\nbias\nrmse\nurmse\nmae\ncc\nsi\nr2\n\n\nobservation\n \n \n \n \n \n \n \n \n\n\n\n\n116l1\n121\n0.003\n0.068\n0.068\n0.055\n0.991\n0.173\n0.982\n\n\n12l1\n121\n-0.004\n0.063\n0.063\n0.050\n0.972\n0.306\n0.943\n\n\n\n\n\n\n\nPlotting Skills\nSkillTable objects have a .plot accessor for quickly visualizing skill scores, such as creating bar charts of metrics.\n\n# Bar plot of RMSE for each observation point\nsk_coll[\"rmse\"].plot.bar()\n\n\n\n\n\n\n\n\nThese are just a few examples. The SkillTable’s .style and .plot accessors offer more customization. Refer to the ModelSkill documentation and Pandas styling documentation for further details.",
    "crumbs": [
      "Module 4 - Calibration Plots and Statistics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Skill Scores</span>"
    ]
  },
  {
    "objectID": "module4_calibration_plots_and_statistics/05_quantifying_performance.html#useful-skill-metrics",
    "href": "module4_calibration_plots_and_statistics/05_quantifying_performance.html#useful-skill-metrics",
    "title": "Skill Scores",
    "section": "Useful Skill Metrics",
    "text": "Useful Skill Metrics\nModelSkill calculates numerous metrics. The choice of metrics depends on your modelling goals. Some useful metrics include:\n\nBias (bias): Average difference (Modeled - Observed). Ideal: 0.\nRMSE (Root Mean Square Error) (rmse): Typical magnitude of error. Ideal: 0.\nNSE (Nash-Sutcliffe Efficiency) (nse): Measures the predictive power of the model compared to using the mean of the observed data as the prediction. Ranges from -\\(\\infty\\) to 1. Ideal: 1.\nKGE (Kling-Gupta Efficiency) (kge): A composite metric evaluating correlation, bias, and variability components. Ranges from -\\(\\infty\\) to 1. Ideal: 1.\nWillmott’s Index of Agreement (willmott): Measures the degree of model prediction error, standardized by observed variability. Ranges from 0 to 1. Ideal: 1.\nPeak Ratio (pr): Ratio of the maximum modeled value to the maximum observed value over the matched time period. Ideal: 1.0.\n\nYou can change the default list of metrics that are used by skill() and mean_skill() as follows:\n\nms.set_option(\"metrics.list\", ['bias', 'rmse', 'nse', 'kge', 'willmott', 'pr'])\ncc.skill()\n\n\n\n\n\n\n\n\nn\nbias\nrmse\nnse\nkge\nwillmott\npr\n\n\nobservation\n\n\n\n\n\n\n\n\n\n\n\n116l1\n121\n0.003229\n0.068114\n0.982405\n0.977502\n0.995498\n0.944704\n\n\n12l1\n121\n-0.004083\n0.063414\n0.942928\n0.932798\n0.984693\n0.891797\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nReset to default values with ms.reset_option(\"metrics.list\").\n\n\nFor a comprehensive list of all available metrics and their precise definitions, please refer to the official ModelSkill API documentation for metrics.\n\n\n\n\n\n\nAdding Custom Metrics\n\n\n\n\n\nModelSkill’s metrics are extensible. You can define and use your own custom skill score functions if needed. If you believe a metric would be broadly useful, consider suggesting it for inclusion in ModelSkill via a GitHub issue.",
    "crumbs": [
      "Module 4 - Calibration Plots and Statistics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Skill Scores</span>"
    ]
  },
  {
    "objectID": "module4_calibration_plots_and_statistics/homework.html",
    "href": "module4_calibration_plots_and_statistics/homework.html",
    "title": "Homework",
    "section": "",
    "text": "Exercise 1\n\nDownload module4_model.zip into a new project folder and extract its content.\nReview the folder structure and files of the model. It should be as follows:\n\nmodule4_model\n├── data\n│   ├── flow_meter_A_2020_09.dfs0\n│   ├── flow_meter_B_2020_09.dfs0\n│   └── rainfall_events_2020-09.dfs0\n├── Dyrup_uncalibrated.mupp\n├── Dyrup_uncalibrated.sqlite\n├── LTS\n│   └── rainfall_events_LTS_2020_09.MJL\n└── results\n    ├── rainfallBaseDefault_Network_HD.res1d\n    └── rainfallBaseDefault_Surface_runoff.res1d\n\nOpen Dyrup_uncalibrated.mupp in MIKE+ and familiarize yourself with the model.\n\nReview the boundary conditions. Can you see where the rainfall is applied?\nFind the system’s only node outlet: G60F360.\nLocate the two flow meters (A and B). Which reach IDs are they associated with?\nReview the simulation setup.\n\nMake a new LTS job list for the simulation. How does it compare with LTS/rainfall_events_LTS_2020_09.MJL?\nRun a simulation. Do you get the same results as those in the results folder?\n\nExercise 2\n\nCreate a new Jupyter Notebook and save it in your project folder.\nCreate a PointObservation object for “Flow Meter A” using data/flow_meter_A_2020_09.dfs0.\nCreate a PointObservation object for “Flow Meter B” using data/flow_meter_B_2020_09.dfs0.\nFor both obs_A and obs_B:\n\nPrint their objects and inspect the data.\nPlot their time series.\n\n\nExercise 3\n\nUse the model network results from Exercise 1 (or the provided reference results).\nFor “Flow Meter A” (model reach G60F380_G60F360_l1), extract its ‘Discharge’ quantity to a dfs0 file.\nCreate a PointModelResult object from the dfs0 file created above.\nFor “Flow Meter B” (model reach G62F070_G62F060_l1), extract its ‘Discharge’ quantity to a dfs0 file.\nCreate a PointModelResult object from the dfs0 file created above.\nFor both mod_A and mod_B:\n\nPrint the object summary.\nPlot the model result time series.\n\n\nExercise 4\n\nMatch obs_A with mod_A using ms.match(). Store in comparer_A.\nMatch obs_B with mod_B. Store in comparer_B.\nFor comparer_A:\n\nGenerate time series plot: comparer_A.plot.timeseries().\nGenerate scatter plot: comparer_A.plot.scatter().\nCalculate and display skill table: comparer_A.skill().\n\nRepeat step 3 for comparer_B.\nExamine the skill tables. Qualitatively, which metrics suggest better or worse performance for each location?\n\nExercise 5\n\nCreate a ComparerCollection named cc from comparer_A and comparer_B.\nDisplay cc. Check cc.obs_names and cc.mod_names.\nMake a temporal coverage plot: cc.plot.temporal_coverage().\nMake a scatter plot: cc.plot.scatter().\nMake a residual histogram: cc.plot.residual_hist().\nCalculate and display the skill table for the collection: cc.skill().\n\nModify to show only ‘rmse’, ‘bias’, ‘nse’, and ‘kge’ metrics.\n\nCalculate and display the mean skill table: cc.mean_skill().\n\nModify to show only ‘rmse’, ‘bias’, ‘nse’, and ‘kge’.\n\nUsing cc.score(), retrieve and print the mean ‘kge’ score for the model.\n\nExercise 6\n\nRepeat Exercise 4 and 5, but using the max_model_gap parameter of ms.match(). Use a value of 600 seconds.\nHow do the number of observation points in each Comparer change?\nHow does the temporal coverage plot change?\nHow does this approach impact the skill assessment?\nReview ModelSkill’s documentation on ‘nse’. How do both flow meters, and the overall model perform on this metric?",
    "crumbs": [
      "Module 4 - Calibration Plots and Statistics",
      "Homework"
    ]
  },
  {
    "objectID": "module5_mikepluspy/index.html",
    "href": "module5_mikepluspy/index.html",
    "title": "Welcome to Module 5!",
    "section": "",
    "text": "This module introduces MIKE+Py, a Python package for programmatic interaction with MIKE+ model databases. You’ll learn to automate common modelling tasks, enhancing your efficiency and reproducibility.\nThis package allows direct access to your MIKE+ project’s .sqlite database to read/write data, manage scenarios, and run simulations.\nThroughout this module, you will learn to:\n\nUnderstand the structure of MIKE+ project databases.\nConnect to a MIKE+ database using MIKE+Py.\nRead data from various model tables.\nModify model parameters and data.\nManage scenarios and alternatives programmatically.\nConfigure and execute MIKE+ simulations from Python.\n\nThis module culminates in a practical homework assignment where you’ll apply these skills to automate tasks for a sample MIKE+ model.\nLet’s get started!\n\n\n\n\n\n\nWhere can I download sample data to follow along?\n\n\n\n\n\nAll of the sample data used in this module is available for download:\n\nmodule5_model.zip\n\nNote: this is the exact same model from Module 4 (Homework - Exercise 1).",
    "crumbs": [
      "Module 5 - MIKE+Py",
      "Welcome to Module 5!"
    ]
  },
  {
    "objectID": "module5_mikepluspy/01_mikepluspy.html",
    "href": "module5_mikepluspy/01_mikepluspy.html",
    "title": "MIKE+Py",
    "section": "",
    "text": "What is MIKE+Py and Why Use It?\nThis section introduces MIKE+Py, a Python package for interacting with MIKE+ projects. You’ll learn its purpose, capabilities, installation, and core concepts.\nMIKE+Py is an open-source Python package that lets you work directly with MIKE+ model databases (.sqlite files). Its main goals are to automate modelling tasks and improve the reproducibility of your MIKE+ workflows.\nWith MIKE+Py, you can programmatically:",
    "crumbs": [
      "Module 5 - MIKE+Py",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>MIKE+Py</span>"
    ]
  },
  {
    "objectID": "module5_mikepluspy/01_mikepluspy.html#what-is-mikepy-and-why-use-it",
    "href": "module5_mikepluspy/01_mikepluspy.html#what-is-mikepy-and-why-use-it",
    "title": "MIKE+Py",
    "section": "",
    "text": "Read and modify model parameters: Change pipe diameters, node inverts, catchment properties, etc.\nManage scenarios and alternatives: Create and switch between different model configurations.\nRun simulations: Execute MIKE+ model simulations.\nRun tools: Execute some GUI tools pythonically (e.g. import/export tool).\n\n\n\n\n\n\n\nModifying Databases Correctly\n\n\n\nUse MIKE+Py for database modifications, not generic SQLite libraries (e.g., Python’s sqlite3). MIKE+Py leverages MIKE+’s internal .NET API, ensuring changes adhere to MIKE+ specific logic and maintain database integrity. Generic tools bypass this, risking model corruption.",
    "crumbs": [
      "Module 5 - MIKE+Py",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>MIKE+Py</span>"
    ]
  },
  {
    "objectID": "module5_mikepluspy/01_mikepluspy.html#installation",
    "href": "module5_mikepluspy/01_mikepluspy.html#installation",
    "title": "MIKE+Py",
    "section": "Installation",
    "text": "Installation\nInstall MIKE+Py using uv in your terminal:\nuv pip install mikeplus\n\n\n\n\n\n\nNote\n\n\n\nUse of MIKE+Py requires a valid MIKE+ license, unlike previously covered Python packages (e.g. MIKE IO, MIKE IO 1D).\n\n\n\n\n\n\n\n\nTip\n\n\n\nAlways refer to the official MIKE+Py documentation for the most up-to-date installation instructions.",
    "crumbs": [
      "Module 5 - MIKE+Py",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>MIKE+Py</span>"
    ]
  },
  {
    "objectID": "module5_mikepluspy/01_mikepluspy.html#core-components",
    "href": "module5_mikepluspy/01_mikepluspy.html#core-components",
    "title": "MIKE+Py",
    "section": "Core Components",
    "text": "Core Components\nYou’ll primarily work with these MIKE+Py components:\n\nDatabase object: Your main connection to the MIKE+ project, opened from a .sqlite file. It’s the gateway to all other functionalities.\nTables: Access and modify data stored in the model’s relational tables (e.g., msm_Node for nodes, msm_Link for pipes).\nScenarios and Alternatives: Programmatically manage different versions or configurations of your model.",
    "crumbs": [
      "Module 5 - MIKE+Py",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>MIKE+Py</span>"
    ]
  },
  {
    "objectID": "module5_mikepluspy/01_mikepluspy.html#example-use-cases",
    "href": "module5_mikepluspy/01_mikepluspy.html#example-use-cases",
    "title": "MIKE+Py",
    "section": "Example Use Cases",
    "text": "Example Use Cases\nCommon applications of MIKE+Py include:\n\nAutomating sensitivity analyses by systematically varying parameters across numerous scenarios.\nRunning batch simulations for different rainfall events or operational strategies.\nDeveloping scripted model calibration and validation workflows.\nExtracting specific model data for custom reports or further analysis.\n\nThe following sections will detail how to connect to databases, read and modify data, manage scenarios, and run simulations.",
    "crumbs": [
      "Module 5 - MIKE+Py",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>MIKE+Py</span>"
    ]
  },
  {
    "objectID": "module5_mikepluspy/01_mikepluspy.html#maturity-potential-and-challenges",
    "href": "module5_mikepluspy/01_mikepluspy.html#maturity-potential-and-challenges",
    "title": "MIKE+Py",
    "section": "Maturity, Potential, and Challenges",
    "text": "Maturity, Potential, and Challenges\nMIKE+Py is less mature than other DHI packages, like MIKE IO and MIKE IO 1D. It is actively being developed, with more features planned.\n\n\n\n\n\n\nCompatibility issues\n\n\n\nThere are known compatibility issues when using MIKE+Py alongside other DHI libraries (e.g. MIKE IO, MIKE IO 1D, ModelSkill).\n\nImporting MIKE IO after MIKE+Py is not supported and will cause errors.\nImporting MIKE IO 1D before MIKE+Py is not supported and will cause errors.\n\nThe following is the suggest import order which works for most use cases, but could still run into issues:\nimport mikeio\nimport modelskill as ms\nimport mikeplus as mp\nimport mikeio1d",
    "crumbs": [
      "Module 5 - MIKE+Py",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>MIKE+Py</span>"
    ]
  },
  {
    "objectID": "module5_mikepluspy/02_databases.html",
    "href": "module5_mikepluspy/02_databases.html",
    "title": "MIKE+ Database",
    "section": "",
    "text": "General\nUnderstanding how MIKE+ stores its data is crucial for effectively using MIKE+Py. This section covers the basics of the MIKE+ database, shows how to explore the database structure both within the MIKE+ GUI and programmatically with MIKE+Py.\nThis section provides a concise primer on useful background knowledge.",
    "crumbs": [
      "Module 5 - MIKE+Py",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>MIKE+ Database</span>"
    ]
  },
  {
    "objectID": "module5_mikepluspy/02_databases.html#general",
    "href": "module5_mikepluspy/02_databases.html#general",
    "title": "MIKE+ Database",
    "section": "",
    "text": "SQLite3\nMIKE+ stores model data in an SQLite database, a common and self-contained format.\n\nIt’s a single file\n\nA single .sqlite file holds all tables and data.\n\nIt’s relational\n\nTables are organized and linked together using keys (unique IDs like MUID).\n\nIt’s free and serverless\n\nAs open-source software, it requires no installation and works with many tools.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhile MIKE+ also supports PostgreSQL databases, MIKE+Py currently only supports SQLite.\n\n\n\n\nSQL\nSQL (Structured Query Language) is the standard language for managing data in a relational database. You use it to query and modify data with commands like:\n\nSELECT\n\nGet specific data from tables.\n\nUPDATE, INSERT, DELETE\n\nChange, add, or remove data.\n\nWHERE\n\nFilter data with conditions.\n\n\nYou do not need to know SQL to use MIKE+Py. However, MIKE+Py’s design intentionally mirrors common SQL patterns, so being familiar with the concepts is helpful.\n\n\n\n\n\n\nLearning SQL\n\n\n\n\n\nIf you’re new to SQL, there are many excellent resources available online. Here are a few starting points:\n\nLearn X in Y minutes - SQL\nSQLZoo",
    "crumbs": [
      "Module 5 - MIKE+Py",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>MIKE+ Database</span>"
    ]
  },
  {
    "objectID": "module5_mikepluspy/02_databases.html#mike-projects",
    "href": "module5_mikepluspy/02_databases.html#mike-projects",
    "title": "MIKE+ Database",
    "section": "MIKE+ Projects",
    "text": "MIKE+ Projects\nWhen you work with a MIKE+ project, you’ll typically interact with two main file types:\n\nProject File (.mupp)\n\nThe main file you open with MIKE+. It points to the database location and stores UI settings like map symbology, preferences, and background layers.\n\nProject Database (.sqlite)\n\nThis file is the model database. It stores all the core model data, including network elements (nodes, links, catchments), their parameters, simulation setups, scenarios, and alternatives.\n\n\nMIKE+Py works on the project database (.sqlite), not the project file.\n\n\n\n\n\n\nNeed to modify a .mupp file?\n\n\n\n\n\nSimply open it with any text editor. It uses DHI’s PFS format, which is structured plain-text. For more advanced edits, consider using MIKE IO’s PFS functionality.",
    "crumbs": [
      "Module 5 - MIKE+Py",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>MIKE+ Database</span>"
    ]
  },
  {
    "objectID": "module5_mikepluspy/02_databases.html#mike-database",
    "href": "module5_mikepluspy/02_databases.html#mike-database",
    "title": "MIKE+ Database",
    "section": "MIKE+ Database",
    "text": "MIKE+ Database\nThe MIKE+ Database organizes tables into groups with the following naming conventions.\n\n\n\n\n\n\n\n\nPrefix\nDescription\nExample(s)\n\n\n\n\nm_\nGeneral MIKE+ tables\nm_ModelSetting\n\n\nms_\nTables common to collection systems and water distribution\nms_Tab\n\n\nmw_\nWater distribution specific tables\nmw_Pipe, mw_Junction\n\n\nmsm_\nMIKE 1D collection systems\nmsm_Node, msm_Link\n\n\nmrm_\nMIKE 1D rivers\nmrm_Branch\n\n\nmss_\nSWMM specific tables\nmss_Node, mss_Link\n\n\nm2d_\nMIKE 21 specific tables\nm2d_Boundary\n\n\n\nMost tables have a MUID column, which serves as a unique identifier for each record (row) within that table.\nRefer to MIKE+’s documentation for more detailed descriptions.",
    "crumbs": [
      "Module 5 - MIKE+Py",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>MIKE+ Database</span>"
    ]
  },
  {
    "objectID": "module5_mikepluspy/02_databases.html#exploring-database-in-the-mike-gui",
    "href": "module5_mikepluspy/02_databases.html#exploring-database-in-the-mike-gui",
    "title": "MIKE+ Database",
    "section": "Exploring Database in the MIKE+ GUI",
    "text": "Exploring Database in the MIKE+ GUI\nBefore diving into programmatic access, it’s helpful to know how to identify table and column names using the MIKE+ GUI. This will help you identify the table and column of the data you’re interested in.\nThe easiest way is with MIKE+’s tooltips. When you hover over a field, a tooltip is displayed containing both the table name and column name. The format is: TableName.ColumnName. For example, the video below shows three different columns that all reside in the table msm_Catchment.\n\nA typical workflow in MIKE+Py is first identifying where the data is via the GUI. This information is essential for querying and modifying specific data.\n\n\n\n\n\n\nTip\n\n\n\nYou can also hover over the column headers in a MIKE+ table view.",
    "crumbs": [
      "Module 5 - MIKE+Py",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>MIKE+ Database</span>"
    ]
  },
  {
    "objectID": "module5_mikepluspy/02_databases.html#exploring-the-database-with-mikepy",
    "href": "module5_mikepluspy/02_databases.html#exploring-the-database-with-mikepy",
    "title": "MIKE+ Database",
    "section": "Exploring the Database with MIKE+Py",
    "text": "Exploring the Database with MIKE+Py\n\nOpening Database\nFirst, open a database with mp.open(). This returns a Database object, which is central for all database operations.\n\nimport mikeplus as mp\n\ndb = mp.open(\"data/Dyrup_uncalibrated.sqlite\")\nprint(f\"MIKE+ Database Version: {db.version}\")\ndb.close()\n\nMIKE+ Database Version: 2025.0.0\n\n\nIt’s important to close the Database object at the end of your workflow with close().\nIt’s easy to forget, therefore it’s recommended to use Python’s context manager syntax to handle closing automatically:\n\nwith mp.open(\"data/Dyrup_uncalibrated.sqlite\") as db:\n    print(f\"MIKE+ Active Model: {db.active_model}\")\n    # database remains open within this code block...\n\n# now it's closed :)\n\nMIKE+ Active Model: CS_MIKE1D\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe remaining code examples assume a Database object was obtained per the above examples.\n\n\n\n\nTables\nThe db.tables attribute provides access to a collection of all tables in the database.\n\ndb.tables\n\nTableCollection&lt;323 tables&gt;\n\n\nYou can access a specific Table object by attribute-style access. Notice this conveniently includes auto-completion.\n\ndb.tables.msm_Catchment\n\nmsm_CatchmentTable&lt;Catchments&gt;\n\n\nWe’ll look into working with tables in the next section, but notice it’s integrated with Pandas:\n\ndf = db.tables.msm_Catchment.to_dataframe()\ndf.head()\n\n\n\n\n\n\n\n\nMUID\nEnabled\nGeomCentroidX\nGeomCentroidY\nArea\nGeomArea\nPersons\nHydrologicalModelNo\nLossDefinition\nModelAImpArea\n...\nSWMM_InitDef\nSWMM_Conduct\nSWMM_RunoffCN\nSWMM_CRegen\nSWMM_Tag\nDataSource\nAssetName\nElement_S\nNetTypeNo\nDescription\n\n\n\n\nG61F011_7292\nG61F011_7292\n1\n585625.618252\n6134811.993345\n4501.460376\n4501.456959\n0.0\n1\n1\n0.375906\n...\nNone\nNone\n0\nNone\nNone\nNone\nNone\nNone\n3\nD10F050\n\n\nG61F013_7293\nG61F013_7293\n1\n585673.073141\n6134756.306611\n2376.508964\n2376.511229\n0.0\n1\n1\n0.327839\n...\nNone\nNone\n0\nNone\nNone\nNone\nNone\nNone\n3\nD10F050\n\n\nG61F030_7294\nG61F030_7294\n1\n585564.527279\n6134874.864836\n4567.727442\n4567.727991\n0.0\n1\n1\n0.416389\n...\nNone\nNone\n0\nNone\nNone\nNone\nNone\nNone\n3\nD10F050\n\n\nG61F031_7295\nG61F031_7295\n1\n585533.822109\n6134832.92312\n3759.297083\n3759.301193\n0.0\n1\n1\n0.387001\n...\nNone\nNone\n0\nNone\nNone\nNone\nNone\nNone\n3\nD10F050\n\n\nG61F032_7296\nG61F032_7296\n1\n585499.225735\n6134786.067181\n5635.766493\n5635.764436\n0.0\n1\n1\n0.344482\n...\nNone\nNone\n0\nNone\nNone\nNone\nNone\nNone\n3\nD10F050\n\n\n\n\n5 rows × 109 columns\n\n\n\n\n\nColumns\nEach table object has a columns attribute that allows you to see the names of its columns. This is mostly useful for auto-completion and avoiding typos.\n\ndb.tables.msm_Catchment.columns.ModelAConcTime\n\n'ModelAConcTime'\n\n\n\n\n\n\n\n\n\nAuto-completion slow to load?\n\n\n\n\n\nMIKE+ has many tables which can take a few seconds to load on first import. In VS Code, you can open the auto-completion menu with Ctrl+Space, which will indicate it’s loading, then update when finished.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nCheck out MIKE+Py’s API documentation for a searchable overview of tables and columns.\n\n\nThe next section will dive into more flexible queries of table information.",
    "crumbs": [
      "Module 5 - MIKE+Py",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>MIKE+ Database</span>"
    ]
  },
  {
    "objectID": "module5_mikepluspy/03_select.html",
    "href": "module5_mikepluspy/03_select.html",
    "title": "Selecting Data",
    "section": "",
    "text": "Selecting Data\nMIKE+Py allows you to programmatically read data from your MIKE+ model database. This is done by constructing and executing queries against specific tables. This section focuses on SELECT queries, which are used to retrieve data.\nQueries in MIKE+Py are typically initiated from a Table object, which you access through an opened Database object.\nFirst, let’s open our MIKE+ project database. We’ll use the db object for all subsequent examples in this section.\nThe primary way to retrieve data is by using the select() method on a table object (e.g., db.tables.msm_Node). This creates a SelectQuery object, which you can then refine and execute.",
    "crumbs": [
      "Module 5 - MIKE+Py",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Selecting Data</span>"
    ]
  },
  {
    "objectID": "module5_mikepluspy/03_select.html#selecting-data",
    "href": "module5_mikepluspy/03_select.html#selecting-data",
    "title": "Selecting Data",
    "section": "",
    "text": "All Columns\nTo select all columns from a table, you can call select() without arguments, followed by to_dataframe() to get the results as a Pandas DataFrame.\nLet’s retrieve all data from the msm_Catchment table:\n\ndf_all_catchments = db.tables.msm_Catchment.select().to_dataframe()\ndf_all_catchments.head()\n\n\n\n\n\n\n\n\nMUID\nEnabled\nGeomCentroidX\nGeomCentroidY\nArea\nGeomArea\nPersons\nHydrologicalModelNo\nLossDefinition\nModelAImpArea\n...\nSWMM_InitDef\nSWMM_Conduct\nSWMM_RunoffCN\nSWMM_CRegen\nSWMM_Tag\nDataSource\nAssetName\nElement_S\nNetTypeNo\nDescription\n\n\n\n\nG61F011_7292\nG61F011_7292\n1\n585625.618252\n6134811.993345\n4501.460376\n4501.456959\n0.0\n1\n1\n0.375906\n...\nNone\nNone\n0\nNone\nNone\nNone\nNone\nNone\n3\nD10F050\n\n\nG61F013_7293\nG61F013_7293\n1\n585673.073141\n6134756.306611\n2376.508964\n2376.511229\n0.0\n1\n1\n0.327839\n...\nNone\nNone\n0\nNone\nNone\nNone\nNone\nNone\n3\nD10F050\n\n\nG61F030_7294\nG61F030_7294\n1\n585564.527279\n6134874.864836\n4567.727442\n4567.727991\n0.0\n1\n1\n0.416389\n...\nNone\nNone\n0\nNone\nNone\nNone\nNone\nNone\n3\nD10F050\n\n\nG61F031_7295\nG61F031_7295\n1\n585533.822109\n6134832.92312\n3759.297083\n3759.301193\n0.0\n1\n1\n0.387001\n...\nNone\nNone\n0\nNone\nNone\nNone\nNone\nNone\n3\nD10F050\n\n\nG61F032_7296\nG61F032_7296\n1\n585499.225735\n6134786.067181\n5635.766493\n5635.764436\n0.0\n1\n1\n0.344482\n...\nNone\nNone\n0\nNone\nNone\nNone\nNone\nNone\n3\nD10F050\n\n\n\n\n5 rows × 109 columns\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nWhen retrieving all columns, a shortcut is simply calling to_dataframe() on the table itself, omitting select().\n\n\n\n\nSpecific Columns\nOften, you only need a subset of columns. You can specify these by passing a list of column names to the select() method.\nLet’s select only the Area, and ModelAImpArea for all catchments:\n\ndf_catchment_subset = db.tables.msm_Catchment.select(['Area', 'ModelAImpArea']).to_dataframe()\ndf_catchment_subset.head()\n\n\n\n\n\n\n\n\nArea\nModelAImpArea\n\n\n\n\nG61F011_7292\n4501.460376\n0.375906\n\n\nG61F013_7293\n2376.508964\n0.327839\n\n\nG61F030_7294\n4567.727442\n0.416389\n\n\nG61F031_7295\n3759.297083\n0.387001\n\n\nG61F032_7296\n5635.766493\n0.344482\n\n\n\n\n\n\n\nExpressions can be made more readable to help better understand complex queries. Additionally, specifying column names by object rather than string helps reduce typo errors.\n\ndf_catchment_subset = (\n    db.tables.msm_Catchment\n        .select([\n            db.tables.msm_Catchment.columns.Area,\n            db.tables.msm_Catchment.columns.ModelAImpArea,\n            db.tables.msm_Catchment.columns.ModelAConcTime,\n            db.tables.msm_Catchment.columns.ModelARFactor,\n        ])\n        .to_dataframe()\n)\ndf_catchment_subset.head()\n\n\n\n\n\n\n\n\nArea\nModelAImpArea\nModelAConcTime\nModelARFactor\n\n\n\n\nG61F011_7292\n4501.460376\n0.375906\n1200.0\n1.0\n\n\nG61F013_7293\n2376.508964\n0.327839\n1200.0\n1.0\n\n\nG61F030_7294\n4567.727442\n0.416389\n1200.0\n1.0\n\n\nG61F031_7295\n3759.297083\n0.387001\n1200.0\n1.0\n\n\nG61F032_7296\n5635.766493\n0.344482\n1200.0\n1.0\n\n\n\n\n\n\n\nAlthough the expression above is more verbose, it’s now much more readable, especially when you read it a year later.\n\n\n\n\n\n\nNote\n\n\n\nUse parentheses to break long statements across lines for readability. Alternatively, use backslashes.",
    "crumbs": [
      "Module 5 - MIKE+Py",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Selecting Data</span>"
    ]
  },
  {
    "objectID": "module5_mikepluspy/03_select.html#muids",
    "href": "module5_mikepluspy/03_select.html#muids",
    "title": "Selecting Data",
    "section": "MUIDs",
    "text": "MUIDs\nIf you only need the MUIDs (unique identifiers) for all records in a table, you can use the get_muids() method.\n\ncatchment_muids = db.tables.msm_Catchment.get_muids()\ncatchment_muids[:3]\n\n['G61F011_7292', 'G61F013_7293', 'G61F030_7294']",
    "crumbs": [
      "Module 5 - MIKE+Py",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Selecting Data</span>"
    ]
  },
  {
    "objectID": "module5_mikepluspy/03_select.html#filtering-rows",
    "href": "module5_mikepluspy/03_select.html#filtering-rows",
    "title": "Selecting Data",
    "section": "Filtering Rows",
    "text": "Filtering Rows\nMIKE+Py provides several ways to filter rows based on conditions.\n\nFiltering with by_muid()\nIf you know the MUID(s) of the specific record(s) you want, the by_muid() method is a convenient way to filter.\nRetrieve data for a catchment with a specific MUID, for example, ‘G61F011_7292’.\n\n(\n    db.tables.msm_Catchment\n        .select()\n        .by_muid('G61F011_7292')\n        .to_dataframe()\n        .head()\n)\n\n\n\n\n\n\n\n\nMUID\nEnabled\nGeomCentroidX\nGeomCentroidY\nArea\nGeomArea\nPersons\nHydrologicalModelNo\nLossDefinition\nModelAImpArea\n...\nSWMM_InitDef\nSWMM_Conduct\nSWMM_RunoffCN\nSWMM_CRegen\nSWMM_Tag\nDataSource\nAssetName\nElement_S\nNetTypeNo\nDescription\n\n\n\n\nG61F011_7292\nG61F011_7292\n1\n585625.618252\n6134811.993345\n4501.460376\n4501.456959\n0.0\n1\n1\n0.375906\n...\nNone\nNone\n0\nNone\nNone\nNone\nNone\nNone\n3\nD10F050\n\n\n\n\n1 rows × 109 columns\n\n\n\nYou can also provide a list of MUIDs.\n\n(\n    db.tables.msm_Catchment\n        .select()\n        .by_muid(['G61F011_7292', 'G61F013_7293'])\n        .to_dataframe()\n        .head()\n)\n\n\n\n\n\n\n\n\nMUID\nEnabled\nGeomCentroidX\nGeomCentroidY\nArea\nGeomArea\nPersons\nHydrologicalModelNo\nLossDefinition\nModelAImpArea\n...\nSWMM_InitDef\nSWMM_Conduct\nSWMM_RunoffCN\nSWMM_CRegen\nSWMM_Tag\nDataSource\nAssetName\nElement_S\nNetTypeNo\nDescription\n\n\n\n\nG61F011_7292\nG61F011_7292\n1\n585625.618252\n6134811.993345\n4501.460376\n4501.456959\n0.0\n1\n1\n0.375906\n...\nNone\nNone\n0\nNone\nNone\nNone\nNone\nNone\n3\nD10F050\n\n\nG61F013_7293\nG61F013_7293\n1\n585673.073141\n6134756.306611\n2376.508964\n2376.511229\n0.0\n1\n1\n0.327839\n...\nNone\nNone\n0\nNone\nNone\nNone\nNone\nNone\n3\nD10F050\n\n\n\n\n2 rows × 109 columns\n\n\n\n\n\nFiltering with where()\nFor more complex filtering based on column values, use the where() method. This method accepts an SQL-like condition string.\nSelect catchments where the Area is greater than 0.8 hectares.\n\n(\n    db.tables.msm_Catchment\n        .select(['Area'])\n        .where(\"Area &gt; 8000\")\n        .to_dataframe()\n)\n\n\n\n\n\n\n\n\nArea\n\n\n\n\nG61F080_7311\n8936.923187\n\n\nG61F163_7318\n8633.103482\n\n\nG61F430_7362\n8076.876771\n\n\nG62F090_7429\n9543.676343\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnits in MIKE+ GUI versus what’s in the database\n\n\n\nValues displayed in the MIKE+ GUI may have different units than what is stored in the database. The examples above show catchment area in ‘hectares’, whereas the database stores them in ‘m²’. Stored units depend on the ‘Unit system’ defined for the specific model.\n\n\nSelect pipes having node ‘G61F351’ as its start node.\n\n(\n    db.tables.msm_Link\n        .select(['FromNodeID'])\n        .where(\"FromNodeID = 'G61F351'\")\n        .to_dataframe()\n)\n\n\n\n\n\n\n\n\nFromNodeID\n\n\n\n\nG61F351_G61F350_l1\nG61F351\n\n\n\n\n\n\n\nNotice the following about string condition passed to where():\n\ncolumn names are not in quotes (e.g. FromNodeID)\nstring values are in single quotes (e.g. ‘G61F351’)\nnumeric values are not in quotes (e.g. 8000)\n\nWrongly formatted condition strings is a common source of errors. MIKE+Py provides a helper function mp.to_sql() that helps with this formatting by preventing mistakes like forgetting to close quote pairs, or forgetting them entirely.\n\n(\n    db.tables.msm_Link\n        .select(['FromNodeID'])\n        .where(f\"FromNodeID = {mp.to_sql(\"G61F351\")}\")\n        .to_dataframe()\n)\n\n\n\n\n\n\n\n\nFromNodeID\n\n\n\n\nG61F351_G61F350_l1\nG61F351\n\n\n\n\n\n\n\nYou can chain multiple where() calls, or combine conditions within a single where() string using AND or OR. Chaining where() calls implies an AND relationship.\nSelect catchments where Area &gt; 8000 AND ModelAImpArea (impervious area for Model A) is less than 0.5.\n\n(\n    db.tables.msm_Catchment\n        .select(['MUID', 'Area', 'ModelAImpArea'])\n        .where(f\"Area &gt; {mp.to_sql(8000)}\")\n        .where(f\"ModelAImpArea &lt; {mp.to_sql(0.5)}\")\n        .to_dataframe()\n)\n\n\n\n\n\n\n\n\nMUID\nArea\nModelAImpArea\n\n\n\n\nG61F080_7311\nG61F080_7311\n8936.923187\n0.115279\n\n\nG61F163_7318\nG61F163_7318\n8633.103482\n0.249552\n\n\nG61F430_7362\nG61F430_7362\n8076.876771\n0.36946\n\n\nG62F090_7429\nG62F090_7429\n9543.676343\n0.447018\n\n\n\n\n\n\n\nThis is equivalent to:\n\n(\n    db.tables.msm_Catchment\n        .select(['MUID', 'Area', 'ModelAImpArea'])\n        .where(f\"Area &gt; {mp.to_sql(8000)} AND ModelAImpArea &lt; {mp.to_sql(0.5)}\")\n        .to_dataframe()\n)\n\n\n\n\n\n\n\n\nMUID\nArea\nModelAImpArea\n\n\n\n\nG61F080_7311\nG61F080_7311\n8936.923187\n0.115279\n\n\nG61F163_7318\nG61F163_7318\n8633.103482\n0.249552\n\n\nG61F430_7362\nG61F430_7362\n8076.876771\n0.36946\n\n\nG62F090_7429\nG62F090_7429\n9543.676343\n0.447018",
    "crumbs": [
      "Module 5 - MIKE+Py",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Selecting Data</span>"
    ]
  },
  {
    "objectID": "module5_mikepluspy/03_select.html#practical-example",
    "href": "module5_mikepluspy/03_select.html#practical-example",
    "title": "Selecting Data",
    "section": "Practical Example",
    "text": "Practical Example\nLet’s say we defined a selection of catchments in MIKE+, for which we we want to retrieve their Area, ModelAImpArea, and ModelAConcTime.\nFirst let’s find the table that selections are stored in. Notice there’s no hover tooltips in MIKE+ GUI for selections. However, we find the table m_Selection anyways by searching MIKE+Py’s documentation for selection. Alternatively, we could have guessed it with help from IDE auto-completion, typing something like db.tables.selec.\nStart by displaying the whole table.\n\ndb.tables.m_Selection.to_dataframe()\n\n\n\n\n\n\n\n\nMUID\nSelectionID\nTableName\nItemMUID\n\n\n\n\nSel_1\nSel_1\nFlow_Meter_B_Catchments\nmsm_Catchment\nG61F180_7321\n\n\nSel_2\nSel_2\nFlow_Meter_B_Catchments\nmsm_Catchment\nG62F060_7424\n\n\nSel_3\nSel_3\nFlow_Meter_B_Catchments\nmsm_Catchment\nG62F070_7425\n\n\nSel_4\nSel_4\nFlow_Meter_B_Catchments\nmsm_Catchment\nG62F071_7426\n\n\nSel_5\nSel_5\nFlow_Meter_B_Catchments\nmsm_Catchment\nG62F072_7427\n\n\n...\n...\n...\n...\n...\n\n\nSel_108\nSel_108\nFlow_Meter_A_Catchments\nmsm_Catchment\nG62F014_7419\n\n\nSel_109\nSel_109\nFlow_Meter_A_Catchments\nmsm_Catchment\nG62F015_7420\n\n\nSel_110\nSel_110\nFlow_Meter_A_Catchments\nmsm_Catchment\nG62F020_7421\n\n\nSel_111\nSel_111\nFlow_Meter_A_Catchments\nmsm_Catchment\nG62F022_7422\n\n\nSel_112\nSel_112\nFlow_Meter_A_Catchments\nmsm_Catchment\nG62F023_7423\n\n\n\n\n112 rows × 4 columns\n\n\n\nWe notice the column SelectionID corresponds with our selection names. TableName and ItemMUID together seem to refer to the items of the selection. We want the catchment MUIDs for selection Flow_Meter_B_Catchments.\n\nmuids = (\n    db.tables.m_Selection\n        .select([\"ItemMUID\"])\n        .where(f\"{db.tables.m_Selection.columns.SelectionID} = {mp.to_sql(\"Flow_Meter_B_Catchments\")}\")\n        .where(f\"{db.tables.m_Selection.columns.TableName} = {mp.to_sql(db.tables.msm_Catchment.name)}\")\n        .to_dataframe()[\"ItemMUID\"]\n        .values.tolist()\n)\nmuids[:3]\n\n['G61F180_7321', 'G62F060_7424', 'G62F070_7425']\n\n\nNow that we have a list of MUIDs, we can use by_muid() to get our desired data.\n\n(\n    db.tables.msm_Catchment\n        .select(['Area', 'ModelAImpArea', 'ModelAConcTime'])\n        .by_muid(muids)\n        .to_dataframe()\n)\n\n\n\n\n\n\n\n\nArea\nModelAImpArea\nModelAConcTime\n\n\n\n\nG61F180_7321\n3426.042426\n0.471066\n2500.0\n\n\nG62F060_7424\n5248.974839\n0.372547\n2500.0\n\n\nG62F070_7425\n3483.333978\n0.493153\n2500.0\n\n\nG62F071_7426\n7680.506712\n0.422575\n2500.0\n\n\nG62F072_7427\n6110.634318\n0.403768\n2500.0\n\n\nG62F073_7428\n3291.502189\n0.337902\n2500.0\n\n\nG62F090_7429\n9543.676343\n0.447018\n2500.0\n\n\nG62F091_7430\n6482.144842\n0.350343\n2500.0\n\n\nG62F092_7431\n4877.124584\n0.356388\n2500.0\n\n\nG62F093_7432\n5069.445488\n0.339430\n2500.0\n\n\nG62F094_7433\n5118.905991\n0.358003\n2500.0\n\n\nG62F095_7434\n5029.988743\n0.383146\n2500.0\n\n\nG62F110_7435\n6385.861122\n0.398525\n2500.0\n\n\nG62F112_7436\n7932.460472\n0.354533\n2500.0\n\n\nG62F113_7437\n4526.396075\n0.361819\n2500.0\n\n\nG62F114_7438\n3481.524653\n0.406584\n2500.0\n\n\nG62F115_7439\n1952.681519\n0.489942\n2500.0\n\n\nG62F116_7440\n3796.497897\n0.475596\n2500.0\n\n\nG62F117_7441\n1745.517658\n0.387302\n2500.0\n\n\nG62F121_7442\n6943.703429\n0.327055\n2500.0\n\n\nG62F122_7443\n4712.980255\n0.395809\n2500.0\n\n\nG62F130_7444\n3543.567635\n0.399932\n2500.0\n\n\nG62F140_7445\n6021.019428\n0.359946\n2500.0\n\n\nG62F160_7446\n6060.401422\n0.358035\n2500.0\n\n\nG62F170_7447\n5339.049153\n0.362818\n2500.0\n\n\nG62F180_7448\n7958.281468\n0.351433\n2500.0\n\n\nG62F191_7449\n4628.034499\n0.420837\n2500.0\n\n\nG62F195_7450\n3289.125097\n0.544337\n2500.0\n\n\nG62F197_7451\n4284.428469\n0.467404\n2500.0\n\n\nG62F199_7452\n3187.640319\n0.429698\n2500.0\n\n\nG62F402_7453\n1495.125317\n0.487788\n2500.0\n\n\nG62F403_7454\n2068.891662\n0.605979\n2500.0\n\n\n\n\n\n\n\nIn the next section, you’ll discover how to modify data and that it uses the same filter mechanisms.",
    "crumbs": [
      "Module 5 - MIKE+Py",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Selecting Data</span>"
    ]
  },
  {
    "objectID": "module5_mikepluspy/04_modify.html",
    "href": "module5_mikepluspy/04_modify.html",
    "title": "Modifying Data",
    "section": "",
    "text": "Update Operations\nThe previous section focused on reading data from your MIKE+ model database. This section delves into modifying that data programmatically using MIKE+Py. You’ll learn how to perform UPDATE, INSERT, and DELETE operations, enabling you to automate changes to your model setup, parameters, and even create new model elements.\nFirst, let’s open our copied MIKE+ project database. We’ll use the db object for all subsequent examples in this section.\nThe update() method on a table object allows you to change existing data in your MIKE+ database. It creates an UpdateQuery object, which you then refine with conditions (using where() or by_muid()) and the new values. To apply the changes to the database, you must call execute() on the query object.\nThe execute() method for an update query returns a list of MUIDs for the rows that were updated.",
    "crumbs": [
      "Module 5 - MIKE+Py",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Modifying Data</span>"
    ]
  },
  {
    "objectID": "module5_mikepluspy/04_modify.html#update-operations",
    "href": "module5_mikepluspy/04_modify.html#update-operations",
    "title": "Modifying Data",
    "section": "",
    "text": "Updating Specific Rows\nLet’s update the diameter of two specific pipes in the msm_Link table.\nFirst, we can inspect their current diameters:\n\n(\n    db.tables.msm_Link\n    .select(['MUID', 'Diameter'])\n    .by_muid(['G60F380_G60F360_l1', 'G62F070_G62F060_l1'])\n    .to_dataframe()\n)\n\n\n\n\n\n\n\n\nMUID\nDiameter\n\n\n\n\nG60F380_G60F360_l1\nG60F380_G60F360_l1\n0.8\n\n\nG62F070_G62F060_l1\nG62F070_G62F060_l1\n0.55\n\n\n\n\n\n\n\nNow, update their diameters. For example, set pipe G60F380_G60F360_l1 and G62F070_G62F060_l1 to 0.6m. The argument passed to update() is a Python dictionary with keys of column names and values matching the desired updated value.\n\n(\n    db.tables.msm_Link\n    .update({\n        db.tables.msm_Link.columns.Diameter : 0.4\n    })\n    .by_muid(['G60F380_G60F360_l1', 'G62F070_G62F060_l1'])\n    .execute() # don't forget this, or nothing happens\n)\n\n['G60F380_G60F360_l1', 'G62F070_G62F060_l1']\n\n\nNotice the update() returned MUIDs of the updated rows. Verify the changes:\n\n(\n    db.tables.msm_Link\n    .select(['MUID', 'Diameter'])\n    .by_muid(['G60F380_G60F360_l1', 'G62F070_G62F060_l1'])\n    .to_dataframe()\n)\n\n\n\n\n\n\n\n\nMUID\nDiameter\n\n\n\n\nG60F380_G60F360_l1\nG60F380_G60F360_l1\n0.4\n\n\nG62F070_G62F060_l1\nG62F070_G62F060_l1\n0.4\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nUse the return value of update() to confirm the result is what you intended. A common error is to forget calling execute().\n\n\n\n\nUpdating All Rows in a Table\nYou can update all rows in a table by calling all() before execute(). Be extremely careful with this operation.\nLet’s update the Description for all nodes in the msm_Node table.\n\nupdated_muids = (\n    db.tables.msm_Node\n    .update({\n        'Description': 'Hello from MIKE+Py'\n    })\n    .all().execute()\n)\nlen(updated_muids)\n\n143\n\n\nYou can verify this change by selecting a few rows or opening the model in MIKE+.",
    "crumbs": [
      "Module 5 - MIKE+Py",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Modifying Data</span>"
    ]
  },
  {
    "objectID": "module5_mikepluspy/04_modify.html#insert-operations",
    "href": "module5_mikepluspy/04_modify.html#insert-operations",
    "title": "Modifying Data",
    "section": "Insert Operations",
    "text": "Insert Operations\nThe insert() method is used to add new rows to a table. You provide a dictionary where keys are column names and values are the data to be inserted. The same default values as MIKE+ are used if a column value is not specified.\nBy default, insert() executes immediately and returns the MUID of the newly inserted row. If you set execute=False, it returns an InsertQuery object, which you would then need to call execute() on.\nMIKE+Py will typically auto-generate an MUID if one is not provided in the values dictionary.\n\n\n\n\n\n\nNote\n\n\n\nCurrently, the insert() method via MIKE+Py is best suited for inserting single rows at a time. For bulk inserts, you need to loop through your data and call insert() for each row.\n\n\n\nInserting a New Simulation Setup\nLet’s insert a new simulation setup into the msm_Project table. We’ll define a new MUID and ScenarioName, and set some essential parameters.\n\ndb.tables.msm_Project.insert({\n    db.tables.msm_Project.columns.MUID              : 'My_Simulation',\n    db.tables.msm_Project.columns.Description       : 'Simulation setup created by MIKE+Py',\n    db.tables.msm_Project.columns.Enable_Catchment  : 1,\n    db.tables.msm_Project.columns.Enable_CS         : 1,\n    db.tables.msm_Project.columns.Enable_RR         : 1,\n    db.tables.msm_Project.columns.Enable_HD         : 1,\n    db.tables.msm_Project.columns.Enable_LTS        : 1,\n})\n\n'My_Simulation'\n\n\nNotice the MUID of the inserted row is returned.",
    "crumbs": [
      "Module 5 - MIKE+Py",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Modifying Data</span>"
    ]
  },
  {
    "objectID": "module5_mikepluspy/04_modify.html#delete-operations",
    "href": "module5_mikepluspy/04_modify.html#delete-operations",
    "title": "Modifying Data",
    "section": "Delete Operations",
    "text": "Delete Operations\nThe delete() method removes rows from a table. Similar to update(), it creates a DeleteQuery. You must specify which rows to delete using where() or by_muid(), or use all() to delete all rows (with extreme caution). Call execute() to perform the deletion.\nThe execute() method for a delete query returns a list of MUIDs for the rows that were deleted.\n\nDeleting a Specific Row\nLet’s delete the simulation setup we just created using its MUID.\n\n(\n    db.tables.msm_Project\n        .delete()\n        .by_muid('My_Simulation')\n        .execute()\n)\n\n['My_Simulation']\n\n\n\n\nDeleting Rows with a Filter\nYou can delete multiple rows that match a specific condition. For instance, to delete all pipes in msm_Link with a Diameter less than 0.2m:\n\ndeleted_pipes = (\n    db.tables.msm_Link\n        .delete()\n        .where(f\"{db.tables.msm_Link.columns.Diameter} &lt; {mp.to_sql(0.2)}\")\n        .execute()\n)\nlen(deleted_pipes)\n\n6\n\n\n\n\nDeleting All Rows in a Table\nTo delete all rows from a table, use all().execute(). This is a very destructive operation. For example, to delete all simulation setups from msm_Project (use with extreme caution):\n\ndb.tables.msm_Project.delete().all().execute()\n\n['rainfall']",
    "crumbs": [
      "Module 5 - MIKE+Py",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Modifying Data</span>"
    ]
  },
  {
    "objectID": "module5_mikepluspy/04_modify.html#practical-example",
    "href": "module5_mikepluspy/04_modify.html#practical-example",
    "title": "Modifying Data",
    "section": "Practical Example",
    "text": "Practical Example\nIn the previous section, we identified how to get MUIDs for catchments belonging to a specific selection. Let’s use that to modify the ‘Time of Concentration’ (ModelAConcTime) for all catchments in the “Flow_Meter_B_Catchments” selection, increasing it by 10%.\nFirst, let’s get a list of the catchment MUIDs again.\n\nselection_name = \"Flow_Meter_B_Catchments\"\ncatchment_muids_to_modify = (\n    db.tables.m_Selection\n        .select(['ItemMUID'])\n        .where(f\"{db.tables.m_Selection.columns.SelectionID} = {mp.to_sql(selection_name)}\")\n        .where(f\"{db.tables.m_Selection.columns.TableName} = {mp.to_sql(db.tables.msm_Catchment.name)}\")\n        .to_dataframe()['ItemMUID']\n        .tolist()\n)\ncatchment_muids_to_modify[:3] # Show first few MUIDs\n\n['G61F180_7321', 'G62F060_7424', 'G62F070_7425']\n\n\nNext, let’s review the initial times of concentration.\n\ndf_toc = (\n    db.tables.msm_Catchment\n    .select(['ModelAConcTime'])\n    .by_muid(catchment_muids_to_modify)\n    .to_dataframe()\n)\ndf_toc.describe()\n\n\n\n\n\n\n\n\nModelAConcTime\n\n\n\n\ncount\n32.0\n\n\nmean\n2500.0\n\n\nstd\n0.0\n\n\nmin\n2500.0\n\n\n25%\n2500.0\n\n\n50%\n2500.0\n\n\n75%\n2500.0\n\n\nmax\n2500.0\n\n\n\n\n\n\n\nNow let’s calculate new times of concentration.\n\ndf_toc = df_toc * 1.1\ndf_toc.describe()\n\n\n\n\n\n\n\n\nModelAConcTime\n\n\n\n\ncount\n32.0\n\n\nmean\n2750.0\n\n\nstd\n0.0\n\n\nmin\n2750.0\n\n\n25%\n2750.0\n\n\n50%\n2750.0\n\n\n75%\n2750.0\n\n\nmax\n2750.0\n\n\n\n\n\n\n\nFinally, we’ll update each catchment with its new time of concentration.\n\nupdated_count = 0\nfor muid, row in df_toc.iterrows():\n    updated_count += len(\n        db.tables.msm_Catchment\n            .update({'ModelAConcTime': row['ModelAConcTime']})\n            .by_muid(muid)\n            .execute()\n    )\n\nprint(f\"Updated Time of Concentration for {updated_count} catchments.\")\n\nUpdated Time of Concentration for 32 catchments.\n\n\nVerify the changes.\n\n(\n    db.tables.msm_Catchment\n        .select(['ModelAConcTime'])\n        .by_muid(catchment_muids_to_modify)\n        .to_dataframe()\n        .describe()\n)\n\n\n\n\n\n\n\n\nModelAConcTime\n\n\n\n\ncount\n32.0\n\n\nmean\n2750.0\n\n\nstd\n0.0\n\n\nmin\n2750.0\n\n\n25%\n2750.0\n\n\n50%\n2750.0\n\n\n75%\n2750.0\n\n\nmax\n2750.0\n\n\n\n\n\n\n\nThis practical example demonstrates a common workflow: selecting data, performing calculations or logic in Python (often with Pandas), writing the modified data back to the MIKE+ database, and verifying everything went okay.",
    "crumbs": [
      "Module 5 - MIKE+Py",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Modifying Data</span>"
    ]
  },
  {
    "objectID": "module5_mikepluspy/05_scenarios.html",
    "href": "module5_mikepluspy/05_scenarios.html",
    "title": "Scenarios",
    "section": "",
    "text": "Concept Overview\nMIKE+Py allows you to programmatically manage scenarios and alternatives within your MIKE+ model database, enabling automation of scenario-based analyses.\nMIKE+ uses a system of scenarios and alternatives to manage different model setups and variations.\nFor a detailed conceptual understanding of scenarios and alternatives, please refer to the MIKE+ documentation. This section focuses on how to interact with them using MIKE+Py.",
    "crumbs": [
      "Module 5 - MIKE+Py",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Scenarios</span>"
    ]
  },
  {
    "objectID": "module5_mikepluspy/05_scenarios.html#concept-overview",
    "href": "module5_mikepluspy/05_scenarios.html#concept-overview",
    "title": "Scenarios",
    "section": "",
    "text": "Scenarios\n\nRepresent a complete model configuration for a specific simulation case (e.g., “Current Conditions”, “Future Development with Pipe Upgrade A”). Scenarios can inherit from a parent scenario. Only one scenario may be active at a time.\n\nAlternative Groups\n\nOrganize different types of model data that can vary between scenarios (e.g., “CS Network data”). Each group contains one or more alternatives, which can inherit from parent alternatives. Only one alternative may be active at a time within an alternative group.\n\nAlternatives\n\nRepresent a specific version of the data within an alternative group (e.g., for “CS Network data”, you might have a “Base Network” alternative and a “Upgraded Pipes” alternative).",
    "crumbs": [
      "Module 5 - MIKE+Py",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Scenarios</span>"
    ]
  },
  {
    "objectID": "module5_mikepluspy/05_scenarios.html#alternative-groups",
    "href": "module5_mikepluspy/05_scenarios.html#alternative-groups",
    "title": "Scenarios",
    "section": "Alternative Groups",
    "text": "Alternative Groups\nList the names of all alternative groups. These are the main categories visible from the MIKE+ Scenarios editor.\n\ndb.alternative_groups.group_names()\n\n['CS Network data',\n 'River network data',\n 'Loads and boundaries data',\n 'Catchments and hydrology data',\n 'Transport data',\n 'Control rules data',\n 'Long Term Statistics data',\n 'Profiles and curves',\n '2D overland',\n '2D boundaries data']\n\n\nAccess a specific alternative group by its name:\n\nnetwork_group = db.alternative_groups[\"CS Network data\"]\nnetwork_group\n\n&lt;AlternativeGroup CS Network data&gt;\n\n\nEach group has a base alternative:\n\nnetwork_group.base\n\nAlternative(CS Network data) &lt;Base Alternative&gt;\n\n\nAnd an active alternative (which is part of the active scenario):\n\nnetwork_group.active\n\nAlternative(CS Network data) &lt;Base Alternative&gt;\n\n\nList the tables associated with an alternative group:\n\nnetwork_group.tables\n\n['msm_Node',\n 'msm_Link',\n 'msm_Pump',\n 'msm_Weir',\n 'msm_Orifice',\n 'msm_Valve',\n 'msm_CurbInlet',\n 'msm_OnGrade',\n 'msm_OnGradeD']",
    "crumbs": [
      "Module 5 - MIKE+Py",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Scenarios</span>"
    ]
  },
  {
    "objectID": "module5_mikepluspy/05_scenarios.html#alternatives",
    "href": "module5_mikepluspy/05_scenarios.html#alternatives",
    "title": "Scenarios",
    "section": "Alternatives",
    "text": "Alternatives\nSee all alternatives for the group.\n\nlist(network_group)\n\n[Alternative(CS Network data) &lt;Base Alternative&gt;]\n\n\nAccess an alternative within a group by its name:\n\nnetwork_group.by_name(\"Base Alternative\")\n\nAlternative(CS Network data) &lt;Base Alternative&gt;\n\n\nCreate a new alternative. It will be a child of its parent (defaults to the group’s base alternative if not specified):\n\nnew_roughness_alt = network_group.create(\"New Roughness Values\", parent=network_group.base)\nnew_roughness_alt\n\nAlternative(CS Network data) &lt;New Roughness Values&gt;\n\n\nAlternatives have several properties. For example, change an alternative’s comment:\n\nnew_roughness_alt.comment = \"Alternative for testing higher roughness.\"\nnew_roughness_alt.comment\n\n'Alternative for testing higher roughness.'",
    "crumbs": [
      "Module 5 - MIKE+Py",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Scenarios</span>"
    ]
  },
  {
    "objectID": "module5_mikepluspy/05_scenarios.html#scenarios",
    "href": "module5_mikepluspy/05_scenarios.html#scenarios",
    "title": "Scenarios",
    "section": "Scenarios",
    "text": "Scenarios\nScenarios combine various alternatives to define a complete model state. Access all scenarios in the database:\n\ndb.scenarios\n\nScenarioCollection &lt;1&gt;\n\n\nGet the currently active scenario:\n\ndb.scenarios.active\n\nScenario &lt;Base&gt;\n\n\nGet the base scenario:\n\ndb.scenarios.base\n\nScenario &lt;Base&gt;\n\n\nAccess a scenario by its name (returns the first match):\n\ndb.scenarios.by_name(\"Base\")\n\nScenario &lt;Base&gt;\n\n\nList all scenarios:\n\nlist(db.scenarios)\n\n[Scenario &lt;Base&gt;]\n\n\nCreate new scenarios with a name and its parent scenario (defaults to base scenario if not provided)\n\nnew_scenario = db.scenarios.create(\"My Scenario\", parent=db.scenarios.base)\nlist(db.scenarios)\n\n[Scenario &lt;Base&gt;, Scenario &lt;My Scenario&gt;]\n\n\nSimilar to Alternatives, Scenarios have several properties. For example:\n\nnew_scenario.comment = \"This is a test scenario\"\n\nList the alternatives used by a scenario:\n\nnew_scenario.alternatives\n\n[Alternative(CS Network data) &lt;Base Alternative&gt;,\n Alternative(River network data) &lt;Base Alternative&gt;,\n Alternative(Loads and boundaries data) &lt;Base Alternative&gt;,\n Alternative(Catchments and hydrology data) &lt;Base Alternative&gt;,\n Alternative(Transport data) &lt;Base Alternative&gt;,\n Alternative(Control rules data) &lt;Base Alternative&gt;,\n Alternative(Long Term Statistics data) &lt;Base Alternative&gt;,\n Alternative(Profiles and curves) &lt;Base Alternative&gt;,\n Alternative(2D overland) &lt;Base Alternative&gt;,\n Alternative(2D boundaries data) &lt;Base Alternative&gt;]\n\n\nCheck if a scenario is active\n\nnew_scenario.is_active\n\nFalse\n\n\nActivate a scenario. This makes it the current context for data modifications and simulations.\n\nnew_scenario.activate()\nnew_scenario.is_active\n\nTrue\n\n\nSet a specific alternative for a scenario. This will replace any existing alternative from the same group.\nLet’s use the new_roughness_alt created earlier and assign it to new_scenario.\n\nnew_scenario.set_alternative(new_roughness_alt)\nlist(new_scenario.alternatives)\n\n[Alternative(CS Network data) &lt;New Roughness Values&gt;,\n Alternative(River network data) &lt;Base Alternative&gt;,\n Alternative(Loads and boundaries data) &lt;Base Alternative&gt;,\n Alternative(Catchments and hydrology data) &lt;Base Alternative&gt;,\n Alternative(Transport data) &lt;Base Alternative&gt;,\n Alternative(Control rules data) &lt;Base Alternative&gt;,\n Alternative(Long Term Statistics data) &lt;Base Alternative&gt;,\n Alternative(Profiles and curves) &lt;Base Alternative&gt;,\n Alternative(2D overland) &lt;Base Alternative&gt;,\n Alternative(2D boundaries data) &lt;Base Alternative&gt;]\n\n\nDelete a scenario. You cannot delete the base scenario.\n\ndb.scenarios.delete(new_scenario)\nlist(db.scenarios)\n\n[Scenario &lt;Base&gt;]",
    "crumbs": [
      "Module 5 - MIKE+Py",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Scenarios</span>"
    ]
  },
  {
    "objectID": "module5_mikepluspy/05_scenarios.html#practical-example",
    "href": "module5_mikepluspy/05_scenarios.html#practical-example",
    "title": "Scenarios",
    "section": "Practical Example",
    "text": "Practical Example\nLet’s continue with the catchment examples of the previous section. We would like to test two different scenarios:\n\nTime of concentration = 2000s for all catchments draining to Flow Meter B\nTime of concentration = 3000s for all catchments draining to Flow Meter B\n\n\n\n\n\n\n\nNote\n\n\n\nRecall that the initial time of concentration for these catchments was 2500 seconds.\n\n\nFirst, get a list of the catchment MUIDs again.\n\nselection_name = \"Flow_Meter_B_Catchments\"\ncatchment_muids_to_modify = (\n    db.tables.m_Selection\n        .select(['ItemMUID'])\n        .where(f\"{db.tables.m_Selection.columns.SelectionID} = {mp.to_sql(selection_name)}\")\n        .where(f\"{db.tables.m_Selection.columns.TableName} = {mp.to_sql(db.tables.msm_Catchment.name)}\")\n        .to_dataframe()['ItemMUID']\n        .tolist()\n)\ncatchment_muids_to_modify[:3] # Show first few MUIDs\n\n['G61F180_7321', 'G62F060_7424', 'G62F070_7425']\n\n\nOur catchments are in table msm_Catchment, which is part of the Catchments and hydrology data Alternative Group. Let’s create two new alternatives.\n\nalternative_2000 = db.alternative_groups[\"Catchments and hydrology data\"].create(\"Time of Concentration = 2000s\")\nalternative_3000 = db.alternative_groups[\"Catchments and hydrology data\"].create(\"Time of Concentration = 3000s\")\nlist(db.alternative_groups[\"Catchments and hydrology data\"])\n\n[Alternative(Catchments and hydrology data) &lt;Base Alternative&gt;,\n Alternative(Catchments and hydrology data) &lt;Time of Concentration = 2000s&gt;,\n Alternative(Catchments and hydrology data) &lt;Time of Concentration = 3000s&gt;]\n\n\nLet’s create two new scenarios, and apply the alternatives to them.\n\nscenario_2000 = db.scenarios.create(\"Time of Concentration = 2000s\")\nscenario_2000.set_alternative(alternative_2000)\n\nscenario_3000 = db.scenarios.create(\"Time of Concentration = 3000s\")\nscenario_3000.set_alternative(alternative_3000)\n\nlist(db.scenarios)\n\n[Scenario &lt;Base&gt;,\n Scenario &lt;Time of Concentration = 2000s&gt;,\n Scenario &lt;Time of Concentration = 3000s&gt;]\n\n\nNow let’s activate each scenario and make changes to the new alternative.\n\nscenario_2000.activate()\nupdated = (\n    db.tables.msm_Catchment\n        .update({\n            db.tables.msm_Catchment.columns.ModelAConcTime : 2000\n        })\n        .by_muid(catchment_muids_to_modify)\n        .execute()\n)\nlen(updated)\n\n32\n\n\n\nscenario_3000.activate()\nupdated = (\n    db.tables.msm_Catchment\n        .update({\n            db.tables.msm_Catchment.columns.ModelAConcTime : 3000\n        })\n        .by_muid(catchment_muids_to_modify)\n        .execute()\n)\nlen(updated)\n\n32\n\n\nNow let’s verify by activating each scenario and checking time of concentration values.\n\nimport pandas as pd\n\n# Check base scenario\ndb.scenarios.base.activate()\ndf_base = (\n    db.tables.msm_Catchment\n        .select([db.tables.msm_Catchment.columns.ModelAConcTime])\n        .by_muid(catchment_muids_to_modify)\n        .to_dataframe()\n)\ndf_base.columns = [\"Base Tc\"]\n\n# Check Tc = 2000\ndb.scenarios.by_name(\"Time of Concentration = 2000s\").activate()\ndf_2000 = (\n    db.tables.msm_Catchment\n        .select([db.tables.msm_Catchment.columns.ModelAConcTime])\n        .by_muid(catchment_muids_to_modify)\n        .to_dataframe()\n)\ndf_2000.columns = [\"Tc = 2000s\"]\n\n# Check Tc = 3000\ndb.scenarios.by_name(\"Time of Concentration = 3000s\").activate()\ndf_3000 = (\n    db.tables.msm_Catchment\n        .select([db.tables.msm_Catchment.columns.ModelAConcTime])\n        .by_muid(catchment_muids_to_modify)\n        .to_dataframe()\n)\ndf_3000.columns = [\"Tc = 3000s\"]\n\n# Concatenate into common DataFrame for comparison\ndf_compare = pd.concat([df_base, df_2000, df_3000], axis=1)\ndf_compare.describe()\n\n\n\n\n\n\n\n\nBase Tc\nTc = 2000s\nTc = 3000s\n\n\n\n\ncount\n32.0\n32.0\n32.0\n\n\nmean\n2500.0\n2000.0\n3000.0\n\n\nstd\n0.0\n0.0\n0.0\n\n\nmin\n2500.0\n2000.0\n3000.0\n\n\n25%\n2500.0\n2000.0\n3000.0\n\n\n50%\n2500.0\n2000.0\n3000.0\n\n\n75%\n2500.0\n2000.0\n3000.0\n\n\nmax\n2500.0\n2000.0\n3000.0\n\n\n\n\n\n\n\n This section demonstrated how to view, create, and manage scenarios and alternatives. In the next section, you’ll learn how to run MIKE+ simulations.",
    "crumbs": [
      "Module 5 - MIKE+Py",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Scenarios</span>"
    ]
  },
  {
    "objectID": "module5_mikepluspy/06_simulations.html",
    "href": "module5_mikepluspy/06_simulations.html",
    "title": "Simulations",
    "section": "",
    "text": "Running Simulations\nMIKE+Py enables you to configure and execute MIKE+ simulations programmatically from Python scripts, which is particularly useful for automating workflows like scenario analysis or calibration.",
    "crumbs": [
      "Module 5 - MIKE+Py",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Simulations</span>"
    ]
  },
  {
    "objectID": "module5_mikepluspy/06_simulations.html#running-simulations",
    "href": "module5_mikepluspy/06_simulations.html#running-simulations",
    "title": "Simulations",
    "section": "",
    "text": "By Active Simulation\nYou can run the simulation setup that is currently marked as “active” in the MIKE+ project.\nFirst, you can identify the active simulation setup’s MUID:\n\nactive_sim_muid = db.active_simulation\nactive_sim_muid\n\n'rainfall'\n\n\nTo run the active simulation, call db.run() without specifying a simulation_muid. It will return a list of Path objects pointing to the generated result files.\n\nresult_files_active = db.run()\nfor rf in result_files_active:\n    print(rf.name)\n\n\n\nrainfallBaseDefault_Network_HD.res1d\nrainfallBaseDefault_Surface_runoff.res1d\n\n\n\n\nBy Simulation MUID\nMore commonly, you’ll want to run a specific simulation setup, regardless of which one is currently active. You can do this by providing the simulation_muid argument to db.run().\nYou can find available simulation setup MUIDs by inspecting a project table, like msm_Project:\n\ndb.tables.msm_Project.select([\"ScenarioName\", \"Description\"]).to_dataframe()\n\n\n\n\n\n\n\n\nScenarioName\nDescription\n\n\n\n\nrainfall\nBase\nNone\n\n\n\n\n\n\n\nTo run the simulation setup named ‘rainfall’:\n\nresult_files_rainfall = db.run(simulation_muid=\"rainfall\")\nfor rf in result_files_rainfall:\n    print(rf.name)\n\n\n\nrainfallBaseDefault_Network_HD.res1d\nrainfallBaseDefault_Surface_runoff.res1d",
    "crumbs": [
      "Module 5 - MIKE+Py",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Simulations</span>"
    ]
  },
  {
    "objectID": "module5_mikepluspy/06_simulations.html#practical-example",
    "href": "module5_mikepluspy/06_simulations.html#practical-example",
    "title": "Simulations",
    "section": "Practical Example",
    "text": "Practical Example\nA common use case is to run the same simulation configuration but for different scenarios. In this example, we’ll run the scenarios created in the previous section.\nRecall that two scenarios were created in addition to the base scenario:\n\n\nCode for generating scenarios (based on previous section)\nselection_name = \"Flow_Meter_B_Catchments\"\n\n# Catchment MUIDs draining to Flow Meter B\ncatchment_muids_to_modify = (\n    db.tables.m_Selection\n        .select(['ItemMUID'])\n        .where(f\"{db.tables.m_Selection.columns.SelectionID} = {mp.to_sql(selection_name)}\")\n        .where(f\"{db.tables.m_Selection.columns.TableName} = {mp.to_sql(db.tables.msm_Catchment.name)}\")\n        .to_dataframe()['ItemMUID']\n        .tolist()\n)\n\n\nalternative_2000 = db.alternative_groups[\"Catchments and hydrology data\"].create(\"Time of Concentration = 2000s\")\nscenario_2000 = db.scenarios.create(\"Time of Concentration = 2000s\")\nscenario_2000.set_alternative(alternative_2000)\n\nalternative_3000 = db.alternative_groups[\"Catchments and hydrology data\"].create(\"Time of Concentration = 3000s\")\nscenario_3000 = db.scenarios.create(\"Time of Concentration = 3000s\")\nscenario_3000.set_alternative(alternative_3000)\n\nscenario_2000.activate()\n(\ndb.tables.msm_Catchment\n    .update({\n        db.tables.msm_Catchment.columns.ModelAConcTime : 2000\n    })\n    .by_muid(catchment_muids_to_modify)\n    .execute()\n)\n\nscenario_3000.activate()\n(\ndb.tables.msm_Catchment\n    .update({\n        db.tables.msm_Catchment.columns.ModelAConcTime : 3000\n    })\n    .by_muid(catchment_muids_to_modify)\n    .execute()\n)\n\nlist(db.scenarios)\n\n\n[Scenario &lt;Base&gt;,\n Scenario &lt;Time of Concentration = 2000s&gt;,\n Scenario &lt;Time of Concentration = 3000s&gt;]\n\n\nLet’s inspect our model simulation setups.\n\ndf_sim_setups = db.tables.msm_Project.select([\"ScenarioName\", \"Description\"]).to_dataframe()\ndf_sim_setups\n\n\n\n\n\n\n\n\nScenarioName\nDescription\n\n\n\n\nrainfall\nBase\nNone\n\n\n\n\n\n\n\nWe will use the simulation setup with MUID ‘rainfall’, iteratively running all scenarios by modifying the ScenarioName.\n\nsimulation_setup_muid = \"rainfall\"\nscenarios_to_run = [\n    db.scenarios.base,\n    db.scenarios.by_name(\"Time of Concentration = 2000s\"),\n    db.scenarios.by_name(\"Time of Concentration = 3000s\"),\n]\nscenario_results = {}\n\nfor scenario in scenarios_to_run:\n\n    # Update the simulation setup to use the current scenario\n    (\n        db.tables.msm_Project\n            .update({\n                db.tables.msm_Project.columns.ScenarioName: scenario.name\n            })\n            .by_muid(simulation_setup_muid)\n            .execute()\n    )\n    \n    # Run simulation\n    result_paths = db.run(simulation_muid=simulation_setup_muid)\n\n    # Store result paths\n    scenario_results[scenario.name] = [p.name for p in result_paths]\n\n# Print all files generated\nfor scenario_name, files in scenario_results.items():\n    print(f\"Scenario '{scenario_name}'\")\n    for file in files:\n        print(f\"\\t- {file}\")\n\n\n\nScenario 'Base'\n    - rainfallBaseDefault_Network_HD.res1d\n    - rainfallBaseDefault_Surface_runoff.res1d\nScenario 'Time of Concentration = 2000s'\n    - rainfallTime of Concentration = 2000sDefault_Network_HD.res1d\n    - rainfallTime of Concentration = 2000sDefault_Surface_runoff.res1d\nScenario 'Time of Concentration = 3000s'\n    - rainfallTime of Concentration = 3000sDefault_Network_HD.res1d\n    - rainfallTime of Concentration = 3000sDefault_Surface_runoff.res1d",
    "crumbs": [
      "Module 5 - MIKE+Py",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Simulations</span>"
    ]
  },
  {
    "objectID": "module5_mikepluspy/homework.html",
    "href": "module5_mikepluspy/homework.html",
    "title": "Homework",
    "section": "",
    "text": "Setup\n\nDownload the module5_model.zip file.\nCreate a new project folder for this homework.\nExtract the contents of module5_model.zip into a data subfolder within your project folder. Your project structure should look something like below.\n\nYour_Project_Folder/\n├── data/\n│   ├── Dyrup_uncalibrated.mupp\n│   ├── Dyrup_uncalibrated.sqlite\n│   └── ... (other files from the zip)\n└── Module5_Homework.ipynb  (or your script)\n\n\n\n\n\n\nTip\n\n\n\nThis is the exact same model that was used in Module 4 Homework, and for the content of Module 5.\n\n\nExercise 1\n\nCreate a Python script or Jupyter Notebook.\nImport the mikeplus package as mp.\nOpen the Dyrup_uncalibrated.sqlite database from your data subfolder using a context manager (with mp.open(...) as db:).\nInside the context manager:\n\nPrint the database version.\nPrint the name of the active model configuration.\nPrint the name of the active scenario.\n\nRetrieve all MUIDs from the msm_Catchment table and print the first 5 MUIDs.\nRetrieve the Area, ModelAConcTime, and ModelARFactor for all catchments and display the head of the resulting DataFrame.\n\nExercise 2\n\n\n\n\n\n\nNote\n\n\n\nFor this exercise, make a copy of Dyrup_uncalibrated.sqlite (e.g., Dyrup_uncalibrated_ex2_copy.sqlite) and work with this copy.\n\n\n\nOpen your copied database.\nSelect the catchments with MUIDs ‘G61F080_7311’ and ‘G61F375_7353’. Display their current ModelAConcTime.\nUpdate the ModelAConcTime for these two catchments to 1800 seconds.\nVerify the change by selecting and displaying the ModelAConcTime for these two catchments again.\nClose the database.\n\nExercise 3\n\n\n\n\n\n\nNote\n\n\n\nContinue working with a fresh copy of the original database or the copy from Exercise 2.\n\n\n\nOpen your copied database.\nIdentify the alternative group that contains catchment data (Hint: its name likely involves “Catchments” or “hydrology”). Store this AlternativeGroup object.\nCreate a new alternative named “High_ConcTime_AllCatchments” within this group, making it a child of the group’s base alternative.\nCreate a new scenario named “Scenario_High_Tc”, making it a child of the base scenario.\nAssign the “High_ConcTime_AllCatchments” alternative to the “Scenario_High_Tc” scenario.\nActivate the “Scenario_High_Tc” scenario and confirm it was activated.\nWithin this active scenario (“Scenario_High_Tc” with “High_ConcTime_AllCatchments” active for catchments):\n\nUpdate the ModelAConcTime for all catchments in the msm_Catchment table to 3600 seconds.\n\nVerify the change:\n\nWhile “Scenario_High_Tc” is active, select and display the ModelAConcTime for a few catchments (e.g., the first 3 MUIDs from msm_Catchment). They should show 3600.\nActivate the base scenario.\nSelect and display the ModelAConcTime for the same catchments. They should show their original values (e.g., 1200.0 if unchanged from the original database).\n\nClose the database.\n\nExercise 4\n\n\n\n\n\n\nNote\n\n\n\nContinue working with the database copy you modified in Exercise 3 (the one with “Scenario_High_Tc”).\n\n\n\nOpen the database copy from Exercise 3.\nRetrieve the simulation setup with MUID ‘rainfall’ from the msm_Project table.\nUpdate this ‘rainfall’ simulation setup to use your “Scenario_High_Tc” scenario by setting its ScenarioName column.\nVerify that the ScenarioName for the ‘rainfall’ simulation setup in msm_Project is now “Scenario_High_Tc”.\nRun the ‘rainfall’ simulation.\nPrint the list of result files generated.\nClose the database.\n\nExercise 5\n\n\n\n\n\n\nNote\n\n\n\nStart with a fresh copy of the original Dyrup_uncalibrated.sqlite database (e.g., Dyrup_uncalibrated_ex5_copy.sqlite). This exercise will create many scenarios and run multiple simulations, which can take some time.\n\n\nThe goal is to investigate the impact of ModelAConcTime and ModelARFactor for all catchments on the simulation results.\n\nDefine two Python lists:\n\nconc_times_sec = [1200, 1500, 1800, 2100, 2400] (these are 20, 25, 30, 35, 40 minutes in seconds)\nr_factors = [0.6, 0.7, 0.8, 0.9, 1.0]\n\nOpen your copied database.\nGet the “Catchments and hydrology data” alternative group object.\nLoop through each combination of conc_time from conc_times_sec and r_factor from r_factors. For each of the 25 combinations:\n\nCreate a unique alternative name, e.g., Tc{tc}_Rf{rf}.\nCreate a new alternative in the “Catchments and hydrology data” group using the unique name, parented to the group’s base alternative.\nCreate a unique scenario name, e.g., Tc{tc}_Rf{rf}.\nCreate a new scenario using the unique name, parented to the database’s base scenario.\nAssign the newly created alternative to the newly created scenario.\nActivate the new scenario.\nIn the active scenario, update ModelAConcTime to the current tc value and ModelARFactor to the current rf value for all catchments in the msm_Catchment table.\nUpdate the simulation setup with MUID ‘rainfall’ in the msm_Project table to use the current new_scenario.name for its ScenarioName column.\nRun the ‘rainfall’ simulation.\nPrint the current scenario name and the list of result files generated for this scenario. (Optionally, just print the name of the .res1d file containing network HD results).\n\nAfter the loop finishes, close the database.\nOpen the model in MIKE+. Can you see your scenarios? Try activating a few and checking catchment parameters.\n\n\nThis set of exercises will guide you through the core functionalities of MIKE+Py, preparing you for more complex automated workflows. Good luck!",
    "crumbs": [
      "Module 5 - MIKE+Py",
      "Homework"
    ]
  },
  {
    "objectID": "module6_putting_everything_together/final_project.html",
    "href": "module6_putting_everything_together/final_project.html",
    "title": "Final Project",
    "section": "",
    "text": "Project Specifications\nWelcome to your final project! This project is designed to bring together everything you’ve learned in this course, focusing on automating common MIKE+ modelling tasks using Python. You’ll prepare data, assess an existing model, and then perform a calibration exercise.\nProject Goal: To improve the performance of a sample MIKE+ model by calibrating catchment parameters using MIKE+Py for automation and ModelSkill for assessment.\nDataset:\nYou’ll be working with the Dyrup_uncalibrated model and associated rainfall and flow meter CSV data.\nEnsure these files are in a data subfolder within your project directory. The Dyrup_uncalibrated.sqlite and .mupp files should be in the root of your project folder, or adjust paths in the code accordingly.\nFamiliarize yourself with the MIKE+ model before starting the project. It involves a storm sewer system using Model A with two flow meters: Flow Meter A (the outlet) and Flow Meter B. Flow Meter B drains into Flow Meter A. The model defines selections: one for areas draining to Flow Meter B and another for areas draining to Flow Meter A, excluding catchments from Flow Meter B.\nProject Tracks:\nTwo tracks: Parts 1-3 are mandatory. Part 4 is an optional advanced challenge.\nLet’s begin!",
    "crumbs": [
      "Module 6 - Putting Everything Together",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Final Project</span>"
    ]
  },
  {
    "objectID": "module6_putting_everything_together/final_project.html#project-specifications",
    "href": "module6_putting_everything_together/final_project.html#project-specifications",
    "title": "Final Project",
    "section": "",
    "text": "Download the project data: DyrupFinalProject_uncalibrated.zip\n\n\nYour_Project_Folder/\n├── data/\n│   ├── rain_events.csv\n│   └── station_flow_meters.csv\n├── LTS/\n│   └── rainfallBase.MJL\n├── Dyrup_uncalibrated.sqlite\n├── Dyrup_uncalibrated.mupp\n└── final_project.ipynb (or your script)\n\n\n\nCatchments for Flow Meter B\n\n\n\n\n\nAdditional catchments for Flow Meter A\n\n\n\n\n\n\n Standard Track (Mandatory):\n\nPart 1: Prepare rainfall and flow data.\nPart 2: Assess the skill of the existing, uncalibrated model.\nPart 3: Calibrate the model focusing on parameters affecting Flow Meter B.\n\n Advanced Challenge (Optional):\n\nPart 4: Further calibrate the model, building upon Part 3, by focusing on parameters affecting Flow Meter A.\n\n\n\n\n\n\n\n\n\nInstall Python Packages\n\n\n\n\n\nInstall these required Python packages. Import necessary modules in your notebook.\nuv pip install ipykernel pandas mikeio modelskill mikeplus mikeio1d matplotlib plotly nbformat",
    "crumbs": [
      "Module 6 - Putting Everything Together",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Final Project</span>"
    ]
  },
  {
    "objectID": "module6_putting_everything_together/final_project.html#part-1-prepare-data",
    "href": "module6_putting_everything_together/final_project.html#part-1-prepare-data",
    "title": "Final Project",
    "section": " Part 1: Prepare Data",
    "text": "Part 1: Prepare Data\nObjective: Convert rainfall and flow meter CSV data to dfs0 files.\n\n\n\n\n\n\nRelevant Resources\n\n\n\n\n\nRelevant Previous Exercises:\n\nModule 2 Homework: CSV reading, Pandas processing, writing dfs0.\n\n\n\n\nTasks:\n\nProcess rainfall CSV to dfs0.\n\n\n\n\n\n\nStarter Code: Task 1.1 - Process Rainfall Data\n\n\n\n\n\nRemember to add necessary import statements at the beginning of your script/notebook. You may split this code into multiple cells.\n# --- Part 1: Prepare Data ---\n# Add imports (e.g., pandas, mikeio).\n\n# Task 1.1: Rainfall CSV to dfs0\n\n# 1. Load \"data/rain_events.csv\". Timestamps are in the first column (use as index, parse dates). Rainfall is µm/s.\n# df_rain = ...\n\n# 2. Define ItemInfo for the rainfall data.\n#    - Name: \"Rainfall Intensity\"\n#    - EUMType: Use mikeio.EUMType for Rainfall_Intensity.\n#    - EUMUnit: Use mikeio.EUMUnit for mu_m_per_sec.\n#    - data_value_type: \"MeanStepBackward\" (intensity over previous step).\n# rainfall_item_info = ...\n\n# 3. Convert df_rain to mikeio.Dataset using mikeio.from_pandas() and rainfall_item_info.\n# ds_rain = ...\n\n# 4. Save Dataset to \"data/rain_events.dfs0\" using .to_dfs().\n# ...\n\n\n\n\nLoad rain_events.csv, ensuring dates are parsed correctly.\nConvert the rainfall DataFrame to a MIKE IO Dataset.\n\nDefine appropriate mikeio.ItemInfo (EUM type, unit, data value type, name).\n\nSave the Dataset to data/rain_events.dfs0.\n\nProcess flow meter CSV to dfs0.\n\n\n\n\n\n\nStarter Code: Task 1.2 - Process Flow Meter Data\n\n\n\n\n\nRemember to add necessary import statements. You may split this code into multiple cells.\n# Task 1.2: Flow meter CSV to dfs0\n\n# 1. Load \"data/station_flow_meters.csv\". Timestamps in first column (use as index, parse dates). Discharge is m^3/s.\n# df_flow_meters = ...\n\n# 2. Define ItemInfo for Flow Meter A.\n#    - Name: \"Flow Meter A\" (or your chosen column name).\n#    - EUMType: Use mikeio.EUMType for Discharge.\n#    - EUMUnit: Use mikeio.EUMUnit for meter_pow_3_per_sec.\n#    - data_value_type: \"Instantaneous\".\n# flow_meter_a_info = ...\n\n# 3. Define ItemInfo for Flow Meter B (similarly).\n# flow_meter_b_info = ... # Your code here\n\n# 4. Convert flow meter DataFrame to mikeio.Dataset using a list of ItemInfo for both meters.\n# ds_flow_meters = ... # Use mikeio.from_pandas()\n\n# 5. Save Dataset to \"data/station_flow_meters.dfs0\".\n# ...\n\n\n\n\nLoad station_flow_meters.csv, ensuring dates are parsed correctly.\nOptionally, rename columns for clarity.\nConvert the flow meter DataFrame to a MIKE IO Dataset.\n\nDefine mikeio.ItemInfo for each flow meter (EUM type, unit, data value type, name).\n\nSave the Dataset to data/station_flow_meters.dfs0.\nHandle potential NaN values if present (though not expected for this dataset’s relevant period).\n\n\n\n\n\n\n\n\n Checkpoint\n\n\n\nVerify successful conversion of rainfall and flow data to dfs0.\n\nrain_events.dfs0 created and contains one item with correct EUM type (Rainfall Intensity), unit (\\(\\mu m/s\\)), and data value type (MeanStepBackward).\nstation_flow_meters.dfs0 created and contains two items, each with correct EUM type (Discharge), unit (\\(m^3/s\\)), and data value type (Instantaneous).",
    "crumbs": [
      "Module 6 - Putting Everything Together",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Final Project</span>"
    ]
  },
  {
    "objectID": "module6_putting_everything_together/final_project.html#part-2-assess-skill-of-existing-model",
    "href": "module6_putting_everything_together/final_project.html#part-2-assess-skill-of-existing-model",
    "title": "Final Project",
    "section": " Part 2: Assess Skill of Existing Model",
    "text": "Part 2: Assess Skill of Existing Model\nObjective: Run uncalibrated model, extract results, assess baseline skill with ModelSkill.\n\n\n\n\n\n\nRelevant Resources\n\n\n\n\n\nRelevant Previous Exercises:\n\nModule 5 Homework: Running a simulation.\nModule 3 Homework: Reading .res1d files, extracting data.\nModule 4 Homework: Preparing data, matching, assessing skill with ModelSkill.\n\n\n\n\nTasks:\n\nRun initial model.\n\n\n\n\n\n\nStarter Code: Task 2.1 - Run Initial Model\n\n\n\n\n\nRemember to add necessary import statements. You may split this code into multiple cells.\n# --- Part 2: Assess Skill of Existing Model ---\n# Add imports (e.g., mikeplus).\n\n# Task 2.1: Run initial model\n# Path to \"Dyrup_uncalibrated.sqlite\".\n# initial_db_path = ... \n\n# 1. Open database with context manager.\n# with ... as db:\n\n# 2. Run simulation (MUID \"rainfall\"). db.run() returns list of Path objects.\n# initial_result_files = ...\n\n\n\n\nOpen Dyrup_uncalibrated.sqlite with mikeplus.\nRun the ‘rainfall’ simulation.\nStore the paths to the generated result files.\n\nExtract model results for ModelSkill.\n\n\n\n\n\n\nStarter Code: Task 2.2 - Extract Model Results\n\n\n\n\n\nRemember to add necessary import statements. You may split this code into multiple cells.\n# Task 2.2: Extract model results\n# Add imports (e.g., mikeio1d, mikeio, pandas).\n\n# 1. Find network HD result file (e.g., \"...Network_HD.res1d\") from initial_result_files.\n# hd_res1d_path_initial = ... \n\n# MUIDs for reaches: Flow Meter A (\"G60F380_G60F360_l1\"), B (\"G62F070_G62F060_l1\").\nreach_fm_a = \"G60F380_G60F360_l1\"\nreach_fm_b = \"G62F070_G62F060_l1\"\n\n# 2. Open .res1d file with mikeio1d.open().\n# res_initial = ...\n\n# 3. Extract 'Discharge' for reach_fm_a. Save to \"result_subsets/Model_FM_A_Initial.dfs0\".\n# res_initial.reaches[...].\n\n# 4. Extract 'Discharge' for reach_fm_b. Save to \"result_subsets/Model_FM_B_Initial.dfs0\".\n# res_initial.reaches[...].\n\n\n\n\nIdentify the network HD result file (e.g., ...Network_HD.res1d).\nOpen this file with mikeio1d.\nExtract ‘Discharge’ for reaches G60F380_G60F360_l1 (Flow Meter A) and G62F070_G62F060_l1 (Flow Meter B).\nSave extracted discharges to dfs0 files (e.g. result_subsets/Model_FM_A_Initial.dfs0)\n\nAssess skill with ModelSkill.\n\n\n\n\n\n\nStarter Code: Task 2.3 - Perform Skill Assessment\n\n\n\n\n\nRemember to add necessary import statements. You may split this code into multiple cells.\n# Task 2.3: Assess skill\n# Add imports (e.g., modelskill, mikeio, pandas).\n\n# Custom course metric defined for you (minimize for best performance).\n# Use it the same way as other metrics, but DO NOT put its name in quotations.\nfrom modelskill.metrics import metric, kge, pr\n\n@metric(best=\"-\") # Lower is better\ndef course_metric(obs, model):\n    alpha = 0.5 # Weighting factor\n    kge_val = kge(obs, model) \n    pr_val = pr(obs, model)\n    kge_component = alpha * (1 - kge_val)\n    pr_component = (1 - alpha) * abs(1 - pr_val)\n    return kge_component + pr_component\n\n# 1. Load observed flow data (\"data/station_flow_meters.dfs0\") to MIKE IO Dataset.\n# ds_obs = ...\n\n# 2. Load initial model results (e.g., \"data/Model_FM_A_Initial.dfs0\") to MIKE IO Datasets.\n# ds_mod_a = ...\n# ds_mod_b = ...\n\n# 3. Create PointObservation for Flow Meter A from ds_obs. Use item name from Part 1. name=\"FM_A\".\n# obs_fm_a = ...\n\n# 4. Create PointObservation for Flow Meter B similarly. name=\"FM_B\".\n# obs_fm_b = ...\n\n# 5. Create PointModelResult for initial model output at FM A. name=\"MIKE+ Initial\".\n# mod_fm_a_initial = ...\n\n# 6. Create PointModelResult for initial model output at FM B. name=\"MIKE+ Initial\".\n# mod_fm_b_initial = ...\n\n# 7. Match obs_fm_a with mod_fm_a_initial using ms.match(max_model_gap=200).\n# cmp_a_initial = ...\n\n# 8. Match obs_fm_b with mod_fm_b_initial similarly.\n# cmp_b_initial = ...\n\n# 9. Create ComparerCollection from [cmp_a_initial, cmp_b_initial].\n# cc_initial = ...\n\n# 10. Plot cc_initial.plot.temporal_coverage().\n# ...\n\n# 11. For FM B (cmp_b_initial): plot scatter and timeseries(backend='plotly').\n# ...\n\n# 12. Skill table for cc_initial. Metrics: ['bias', 'rmse', 'nse', 'kge', 'pr', course_metric].\n# metrics_to_calc = ['bias', 'rmse', 'nse', 'kge', 'pr', course_metric]\n# skill_initial = ...\n\n# 13. Mean skill table for cc_initial with same metrics.\n# mean_skill_initial = ...\n\n# 14. Review metrics: How is initial model performance?\n\n\n\n\nLoad observed flow data (data/station_flow_meters.dfs0).\nDefine and use the custom course_metric (referenced in code template).\nCreate ComparerCollection matching observations (Flow Meter A, B) with model results. Use max_model_gap=200.\nPlot temporal coverage.\nFor Flow Meter B, generate scatter plot. Plot the time series using plotly as the backend, then explore.\nCalculate and display skill tables (individual and mean) with ‘bias’, ‘rmse’, ‘nse’, ‘kge’, ‘pr’, and course_metric.\nReflect on the model’s existing skill, paying particular attention to kge, pr, and the course metric.\n\n\n\n\n\n\n\n\n Checkpoint\n\n\n\nVerify initial model run and baseline skill assessment.\n\nInitial model results for FM A & B reaches saved to result_subsets/*.dfs0.\nComparerCollection created comparing initial model results with observed flow.\nBaseline skill scores (esp. course_metric) for FM A and FM B.",
    "crumbs": [
      "Module 6 - Putting Everything Together",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Final Project</span>"
    ]
  },
  {
    "objectID": "module6_putting_everything_together/final_project.html#part-3-calibrate-model",
    "href": "module6_putting_everything_together/final_project.html#part-3-calibrate-model",
    "title": "Final Project",
    "section": " Part 3: Calibrate Model",
    "text": "Part 3: Calibrate Model\nObjective: Calibrate ModelAConcTime & ModelARFactor for Flow Meter B catchments via grid search (MIKE+Py, ModelSkill).\n\n\n\n\n\n\nWhat is a Grid Search?\n\n\n\nA grid search runs the model for every combination of chosen parameter values. The best combination is based on a chosen metric (e.g., course_metric). See Wikipedia: Grid search.\n\n\n\n\n\n\n\n\nRelevant Resources\n\n\n\n\n\nRelevant Previous Exercises:\n\nModule 5 Homework: Selecting data, creating scenarios/alternatives, looping, modifying parameters, running simulations (especially Exercise 5).\nModule 4 Homework: As in Part 2 for ModelSkill.\n\n\n\n\nTasks:\n\nSetup calibration for FM B.\n\n\n\n\n\n\nStarter Code: Task 3.1 - Setup for Calibration (FM B)\n\n\n\n\n\nRemember to add necessary import statements. You may split this code into multiple cells.\n# --- Part 3: Calibrate Model at Flow Meter B ---\n# Add imports (e.g., numpy, mikeplus).\n\n# Task 3.1: Set up for calibration\n\n# 1. Open database. Query \"m_Selection\" for \"ItemMUID\" where \"SelectionID\" is \"Flow_Meter_B_Catchments\" AND \"TableName\" is \"msm_Catchment\". Store MUIDs in catchments_b_muids list.\n# with ... as db:\n    # selection_df = (db.tables.m_Selection\n                    # .select(...) # Select \"ItemMUID\"\n                    # .where(...) # Filter by SelectionID\n                    # .where(...) # Filter by TableName\n                    # .to_dataframe())\n    # catchments_b_muids = selection_df[\"ItemMUID\"].tolist()\n\n# 2. Review ModelAConcTime & ModelARFactor for catchments_b_muids. Query \"msm_Catchment\", filter by MUIDs, describe().\n# with ... as db: # Or reuse open db if still in context\n\n# 3. Define parameter ranges for Tc (Time of Conc.) and R_factor (Reduction Factor) using np.linspace() (e.g., 3 values each).\n# tc_vals_b = ...\n# r_vals_b = ...\n\n\n\n\nOpen Dyrup_uncalibrated.sqlite.\nRetrieve MUIDs for catchments in selection “Flow_Meter_B_Catchments”.\nReview existing Time of Concentration (Tc) and Reduction Factor (R_factor) for these catchments.\nDefine ranges for Tc and R_factor for these catchments (e.g., 3 values for each parameter to create a 3x3 grid).\n\n\nGrid parameters apply to all catchments in the “Flow_Meter_B_Catchments” group.\n\n\n\n\n\n\n\nLong Runtimes\n\n\n\nRunning many MIKE+ simulations can be time-consuming. It’s suggested to keep the grid search low initially (e.g. 3x3 = 9 trials). Once your workflow is defined, and you want to refine the calibration, then consider creating more trials.\n\n\nSingle calibration trial (FM B).\n\n\n\n\n\n\nStarter Code: Task 3.2 - Perform a Single Calibration Trial (FM B)\n\n\n\n\n\nAdd necessary imports.\n# Task 3.2: Single trial (FM B)\n\n# 1. Pick one Tc and one R_factor from defined ranges.\n# tc_trial = tc_vals_b[0]  # Example: first Tc value\n# r_factor_trial = r_vals_b[0] # Example: first R_factor value\n\n# 2. Define unique name for alternative & scenario (use f-string with param values).\n# name = f\"Trial 0 (tc={tc:.0f}, r={r_factor:.3f})\"\n\n# 3. Create alternative & scenario. Activate scenario. Verify in MIKE+.\n# with ... as db:\n#    alt_group = db.alternative_groups[...]\n#    alternative = alt_group.create(...)\n#    scenario = db.scenarios.create(...)\n#    scenario.set_alternative(...)\n#    scenario.activate()\n\n# 4. Update ModelAConcTime & ModelARFactor for catchments_b_muids. Execute & verify.\n# updated = (\n#   db.tables.msm_Catchment\n#     .update({...}) # Define columns and new values\n#     .by_muid(...)  # Specify MUIDs to update\n#     .execute()\n# )\n# len(updated)\n\n# 5. Update 'rainfall' sim setup in msm_Project to use new scenario.\n# (db.tables.msm_Project\n#     .update({...}) # Define column and new value\n#     .by_muid(...)  # Specify simulation MUID\n#     .execute())\n\n# 6. Run the 'rainfall' simulation and store the list of result file Paths.\n# result_files = db.run(...)\n\n# 7. Open .res1d with mikeio1d.\n# res_single_trial = ...\n\n# 8. Extract Discharge for FM B to dfs0.\n# ...\n\n# 9. Create PointModelResult. Name it like scenario/alternative (trial_name).\n# mod_result_single_trial = ms.PointModelResult(...)\n\n# 10. Match with obs_fm_b.\n# comparer_single_trial = ms.match(...)\n\n# 11. Plot scatter & timeseries (as in Part 2).\n# ...\n\n# 12. Calculate skill (include course_metric).\n# skill_single_trial = comparer_single_trial.skill(...)\n\n# 13. Review metrics: How does this trial compare with the initial model?\n\n\n\n\nThis task guides you to run one iteration of the calibration process. This will help you understand the steps involved before automating them in a loop for the entire grid search.\n\n\nWork on the database Dyrup_uncalibrated.sqlite.\nPerform all steps for one parameter combination: create scenario/alternative, update parameters, run simulation, extract results, calculate course_metric.\nVerify that the process works and compare the output metric with the initial model’s metric for Flow Meter B.\n\nAutomated grid search (FM B).\n\n\n\n\n\n\nStarter Code: Task 3.3 - Automated Grid Search (FM B)\n\n\n\n\n\nRemember to add necessary import statements.\n# Task 3.3: Automated grid search (FM B)\n# import itertools\n\n# Store results: trial_name -&gt; hd_res1d_path\ntrial_b_results = {}   \n\n# 1. Open database ONCE for the loop (context manager).\n# with ... as db:\n\n    # 2. Get alternative group for \"Catchments and hydrology data\".\n    # alt_group = db.alternative_groups[...]\n\n    # 3. Loop all Tc & R_factor combinations (itertools.product).\n    # for i, (tc, r_factor) in enumerate(itertools.product(tc_vals_b, r_vals_b), start=1):\n        # print(f\"Starting Trial {i}: Tc={tc_current:.0f}, R_factor={r_current:.2f}\")\n\n        # 4. Define unique alt/scenario name.\n        # trial_name = f\"Trial {i} (tc={tc:.0f}, r={r_factor:.3f})\"\n\n        # 5. Create & activate new alt/scenario.\n        # ...\n\n        # 6. Update catchment Tc & R_factor for catchments_b_muids.\n        # ...\n\n        # 7. Update msm_Project, run simulation, get HD result file path.\n        # ...\n\n        # 8. Store HD result path in trial_b_results.\n        # trial_b_results[trial_name] = ...\n\n\n\n\nUsing the database Dyrup_uncalibrated.sqlite, loop through all combinations of Tc and R_factor.\nInside the loop, include all the steps to create a scenario/alternative, update parameters for the current Tc and R_factor, and run the simulation.\nSave the result of the loop in a dictionary, with keys being the scenario name and values being the path to the res1d result file.\nOpen MIKE+, activate a few scenarios, and check the parameters to ensure your code worked as intended.\n\n\n\n\n\n\n\nCaution\n\n\n\nTest your code carefully before running it in the full loop. Use cells to test chunks of code. If you accidentally destroy the model database, it’s easiest to start over with a fresh copy of the database. Luckily you have a reproducible workflow that’s as easy as running all cells :).\n\n\nAnalyze calibration results (FM B).\n\n\n\n\n\n\nStarter Code: Task 3.4 - Analyze Calibration Results (FM B)\n\n\n\n\n\nRemember to add necessary import statements. You may split this code into multiple cells.\n# Task 3.4: Analyze FM B calibration results\n\n# 1. Loop over `trial_b_results` from the previous part. Create a list of comparer objects at Flow Meter B for each trial.\n# comparers = []\n# for scenario_name, res1d_path in trial_b_results.items():\n#     res = mikeio1d.open(res1d_path)\n#     ... some more code to get result data into modelskill\n#     cmp = ms.match(...)\n#     comparers.append(cmp)\n\n# 2. Add the comparer object from MIKE+ Initial at Flow Meter B\n# comparers.append(...)\n\n# 3. Create a ComparerCollection of all the trials and the initial model.\n# cc_fm_b = ms.ComparerCollection(...)\n\n# 4. Save the ComparerCollection so that you can reload it in the future.\n# cc_fm_b.save(\"flow_meter_b_calibration.msk\") \n# cc_fm_b = cc_fm_b.load(\"flow_meter_b_calibration.msk\") # this is how you would load it\n\n# 5. Skill table for cc_fm_b (include course_metric).\n# skill_fm_b = cc_fm_b.skill(...)\n\n# 6. Sort skill_fm_b by course_metric to find best/worst trials. Display styled table.\n# skill_fm_b\n\n# 7. Plot best and worst trials vs. MIKE+ Initial (e.g., scatter, timeseries for FM B).\n# cc_fm_b[0].sel(model=[\"MIKE+ Initial\", ...]).plot.\n\n# 8. Review metrics: How do these trials compare?\n\n\n\n\nCreate a ComparerCollection for Flow Meter B:\n\nInclude the “MIKE+ Initial” model results (from Part 2).\nInclude all trials from the previous part..\n\n(Optional: save/load .msk file).\nCalculate the skill table for Flow Meter B using course_metric and other standard metrics.\nSort the skill table to find the best and worst trials, according to course_metric.\nMake plot comparisons of the best trial versus MIKE+ Initial to support the calibration efforts.\n\n\n\n\n\n\n\n\n Checkpoint\n\n\n\nVerify automated calibration for FM B and result analysis.\n\nDyrup_uncalibrated.sqlite contains all scenarios/alternatives for FM B grid search.\nAll grid search sim result files generated (or best trial’s .res1d path is known).\nBest ModelAConcTime & ModelARFactor for FM B catchments identified via course_metric.\nPerformance improvement at FM B quantified (using course_metric) and visualized.",
    "crumbs": [
      "Module 6 - Putting Everything Together",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Final Project</span>"
    ]
  },
  {
    "objectID": "module6_putting_everything_together/final_project.html#part-4-more-calibration-optional",
    "href": "module6_putting_everything_together/final_project.html#part-4-more-calibration-optional",
    "title": "Final Project",
    "section": " Part 4: More Calibration (optional)",
    "text": "Part 4: More Calibration (optional)\nObjective: Further calibrate for Flow Meter A catchments. Use Part 3’s best scenario as parent.\nTasks:\n\nThere’s no code templates or checkpoints for this part - you’re on your own :)\nRepeat Part 3 process for FM A catchments.\nCreate functions to reuse Part 3 code (recommended).\nCreate ComparerCollection for FM A & B. Calculate overall mean course_metric.",
    "crumbs": [
      "Module 6 - Putting Everything Together",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Final Project</span>"
    ]
  }
]